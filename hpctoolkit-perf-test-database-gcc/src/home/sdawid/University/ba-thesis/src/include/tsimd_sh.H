// ===========================================================================
//
// tsimd_sh.H --
// Single header version of the T-SIMD library.
//
// This file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// ===========================================================================
// LICENSE
// ===========================================================================
/*
License agreement (version 2)
=================

between the licensor

    Ralf Möller
    Computer Engineering
    Faculty of Technology
    Bielefeld University
    www.ti.uni-bielefeld.de

and the licensee (the user of the software and the databases)

regarding the use of the following software

   - the low-level C++ template SIMD library
   - the SIMD implementation of the MinWarping and the 2D-Warping methods 
     for local visual homing

in the following referred to as "the software", 

and on the use of the

   - panoramic image databases

in the following referred to as "the databases".

(1) The licensor grants the licensee the non-exclusive,
    non-transferable, non-licensable right to use and modify the
    software and the databases. The software and the databases are
    licensed free of charge.

(2) The licensee agrees not to transfer or to disclose the software
    and the databases in original or modified form to other
    individuals or to other institutions.

(3) The software and the databases will only be used for the
    licensee's own scientific study, scientific research, or academic
    teaching. Use for commercial or business purposes is not
    permitted. Use for any purposes with military background is not
    permitted.

(4) The licensee agrees not to use the software and the databases in
    applications where the life and health of humans and the property
    of humans or institutions is potentially endangered.

(5) The software is provided entirely in the form of source code. It
    is the sole responsibility of the licensee to check the
    correctness of all parts of the software that are used by the
    licensee. The licensee has to be able to understand the
    functionality of all parts of the software that the licensee
    uses. The licensee accepts that the software was not tested in
    depth and agrees to perform in-depth tests and to correct errors
    for all parts that are used by the licensee.

(6) The software contains code fragments taken from other software
    which is licensed by "The MIT license"
    (https://opensource.org/licenses/MIT) and code fragments from
    software help sites on the internet (e.g. stackoverflow.com). The
    software was inspired by and contains code fragments from Agner
    Fog's "Vector Class Library"
    (http://www.agner.org/optimize/#vectorclass). Agner Fog declared
    that he sees no violation of his copyright by the software in its
    present form.

(7) Because the software and the databases are provided free of
    charge, there is no warranty for the software and the databases
    (to the extent permitted by applicable law). The software
    including the databases is provided "as is" without warranty of
    any kind, without even the implied warranty of merchantability or
    fitness for a particular purpose. The entire risk of using the
    software and the databases lies with the licensee.

(8) In no event (unless required by applicable law) will the licensor
    or any contributor be liable for any claim, any sort of damage or
    any other liability arising from the use of the software and the
    databases or arising from the inability to use the software and
    the databases.

(9) The licensor is not obligated to provide any support for the
    licensee regarding the use of the software and the databases.

(10) This license agreement comes into force without signatures as
     soon as the licensee obtains the software and/or the
     databases. If the licensee disagrees with any of the clauses of
     this license, the licensee is not permitted to obtain or keep the
     software and the image databases.

(11) Should any provision of this license agreement be or become
     invalid, this shall not affect the validity of the remaining
     provisions. Any invalid provision shall be replaced by a valid
     provision which corresponds to the meaning and purpose of the
     invalid provision.

(12) This license agreement (files LICENSE, LICENSE.md and LICENSE.doc) has to
     accompany the files of the software and the files of the
     databases. The header text of the source files referring to the
     license agreement has to remain unchanged.

(13) German law applies and the place of jurisdiction is Bielefeld.
*/
// ===========================================================================
// end of LICENSE
// ===========================================================================

// ===========================================================================
//
// tsimd.H --
// Main include file for using the T-SIMD library.
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

/**
 * @file tsimd.H
 * @brief Main include file for using the T-SIMD library.
 */

#pragma once
#ifndef T_SIMD_H_
#define T_SIMD_H_

// base-level templates/functions
// ===========================================================================
//
// SIMDVecBase.H --
// base-level classes and functions
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Jonas Keller, Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): introduced wrapper layer that wraps the internal
// architecture-specific implementations

// 09. Mar 23 (Jonas Keller): added doxygen documentation

#ifndef SIMD_VEC_BASE_H_
#define SIMD_VEC_BASE_H_

// ===========================================================================
//
// SIMDDefs.H --
// encapsulates compiler- and architecture-specific definitions and constructs
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SIMD_DEFS_H_
#define SIMD_DEFS_H_

#include <cstring>
#include <iostream>

// TODO: implement and test for other compilers
// TODO: static inline on primary function templates useful?
// TODO: (static not allowed on function template specializations)
// TODO: MS world: inline __forceinline?

// ---------------------------------------------------------------------------
// preprocessor symbols
// ---------------------------------------------------------------------------

// NOTE: these symbols should be defined before including any T-SIMD header file
//       (e.g. by passing them to the compiler via the command line) otherwise
//       they might not have the desired effect

// SIMD_ALIGN_CHK:
// if defined, alignment checks of pointers are added in aligned load
// and store functions (if necessary on given architecture)

// MAX_SIMD_WIDTH:
// if defined, NATIVE_SIMD_WIDTH will be at most to this value
// must be at least 16, as smaller vector widths are not supported

// ---------------------------------------------------------------------------
// architecture-specific definitions
// ---------------------------------------------------------------------------

#if defined(__i386__) || defined(__x86_64__)
#define SIMDVEC_INTEL_ENABLE 1

#ifdef __AVX512F__
// this can be used to check whether 64-byte support is available
#define _SIMD_VEC_64_AVAIL_
#ifdef __AVX512BW__
// this can be used to check whether full 64-byte support is available
#define _SIMD_VEC_64_FULL_AVAIL_
#endif
#endif

#ifdef __AVX__
// this can be used to check whether 32-byte support is available
#define _SIMD_VEC_32_AVAIL_
#ifdef __AVX2__
// this can be used to check whether full 32-byte support is available
#define _SIMD_VEC_32_FULL_AVAIL_
#endif
#endif

#ifdef __SSE2__
#define _SIMD_VEC_16_AVAIL_
#endif

// determine NATIVE_SIMD_WIDTH
#ifdef _SIMD_VEC_64_AVAIL_
#define NATIVE_SIMD_WIDTH 64
#elif defined(_SIMD_VEC_32_AVAIL_)
#define NATIVE_SIMD_WIDTH 32
#else
#define NATIVE_SIMD_WIDTH 16
#endif

#endif // defined(__i386__) || defined(__x86_64__)

// 31. Mar 22 (rm): gcc on ARM doesn't define __arm__
// #if defined(__arm__) && (defined(__ARM_NEON__) || defined(__ARM_NEON))
// 20. May 23 (Jonas Keller): apparently, some gcc versions in ARM *only* define
// __arm__, so added checks for more possible defines to cover as many compilers
// as possible
// #if defined(__ARM_NEON__) || defined(__ARM_NEON)
#if defined(__arm__) || defined(__aarch64__) || defined(__ARM_NEON__) ||       \
  defined(__ARM_NEON) || defined(_M_ARM) || defined(_M_ARM64)
#define SIMDVEC_NEON_ENABLE 1

#define _SIMD_VEC_16_AVAIL_

// determine NATIVE_SIMD_WIDTH
#define NATIVE_SIMD_WIDTH 16

#endif // arm defines

// 20. May 23 (Jonas Keller):
// added error message if no SIMD support was detected
#ifndef _SIMD_VEC_16_AVAIL_
#error "no SIMD support detected"
#endif

// determine double support
#if defined(SIMDVEC_INTEL_ENABLE) || defined(__aarch64__)
#define SIMD_64BIT_TYPES 1
#endif

// set NATIVE_SIMD_WIDTH to be at most MAX_SIMD_WIDTH
#ifdef MAX_SIMD_WIDTH
#if NATIVE_SIMD_WIDTH > MAX_SIMD_WIDTH
#undef NATIVE_SIMD_WIDTH
#if MAX_SIMD_WIDTH >= 64
#define NATIVE_SIMD_WIDTH 64
#elif MAX_SIMD_WIDTH >= 32
#define NATIVE_SIMD_WIDTH 32
#elif MAX_SIMD_WIDTH >= 16
#define NATIVE_SIMD_WIDTH 16
#else
#error "MAX_SIMD_WIDTH must be at least 16"
#endif
#endif
#endif

// ---------------------------------------------------------------------------
// g++, clang++ and icc
// ---------------------------------------------------------------------------

// thanks to Wolfram Schenck for icc tests
// 30. Aug 22 (Jonas Keller): clang++ on windows does not define __GNUC__
// #ifdef __GNUC__
#if defined(__GNUC__) || defined(__clang__) || defined(__INTEL_COMPILER)
#define SIMD_DEFS_DEFINED
#define SIMD_INLINE              inline __attribute__((always_inline))
#define SIMD_ATTR_ALIGNED(ALIGN) __attribute__((aligned(ALIGN)))
#define SIMD_ATTR_PACKED_STRUCT  struct __attribute__((__packed__))
#define SIMD_FULL_MEMBARRIER     __sync_synchronize()
#endif

// TODO: MSVC?

// ---------------------------------------------------------------------------
// compiler-independent stuff
// ---------------------------------------------------------------------------

#ifndef SIMD_DEFS_DEFINED
#error "attribute definitions missing for this compiler"
#endif

// macro to set the default value SIMD_WIDTH template parameter to
// NATIVE_WIDTH if NATIVE_WIDTH is defined
#ifdef NATIVE_SIMD_WIDTH
#define SIMD_WIDTH_DEFAULT_NATIVE SIMD_WIDTH = NATIVE_SIMD_WIDTH
#else
#define SIMD_WIDTH_DEFAULT_NATIVE SIMD_WIDTH
#endif

namespace simd {
namespace internal {
template <typename Tout, typename Tin>
SIMD_INLINE Tout bit_cast(Tin in)
{
  static_assert(std::is_trivially_copyable<Tout>::value,
                "Tout must be trivially copyable");
  static_assert(std::is_trivially_copyable<Tin>::value,
                "Tin must be trivially copyable");
  Tout out;
  // set out to zero for padding in case sizeof(in) < sizeof(out)
  std::memset(&out, 0, sizeof(out));
  std::memcpy(&out, &in, std::min(sizeof(in), sizeof(out)));
  return out;
}
} // namespace internal
} // namespace simd

// 29. Mar 23 (Jonas Keller): added SIMD_ENABLE_IF macro

// macros to make enable_if SFINAE more convenient and readable
#define SIMD_ENABLE_IF(EXPR) typename = typename std::enable_if<(EXPR)>::type

// 26. Sep 23 (Jonas Keller): added SIMD_IF_CONSTEXPR macro

// Macro for if's that can be evaluated at compile time.
// Defined as "if constexpr" in C++17, otherwise as a regular "if".
// If "constexpr if" is not available the compiler should still optimize the
// regular "if" away since the condition is known at compile time.
#ifdef __cpp_if_constexpr
#define SIMD_IF_CONSTEXPR if constexpr
#else
#define SIMD_IF_CONSTEXPR if
#endif

// 08. Apr 23 (Jonas Keller): added SIMD_CHECK_ALIGNMENT macro

// macro to check alignment of a pointer
// does nothing if SIMD_ALIGN_CHK is not defined
// does not use assert() because assert() may be disabled (e.g. in release
// builds)

#ifdef SIMD_ALIGN_CHK
#define SIMD_CHECK_ALIGNMENT(PTR, ALIGN)                                       \
  if (reinterpret_cast<uintptr_t>(PTR) % (ALIGN) != 0) {                       \
    ::std::cerr << "SIMD_CHECK_ALIGNMENT: " << __FILE__ << ":" << __LINE__     \
                << ": " << __func__ << ": " << #PTR << " = " << (PTR)          \
                << " is not aligned to " << (ALIGN) << " bytes"                \
                << ::std::endl;                                                \
    ::std::abort();                                                            \
  }
#else
#define SIMD_CHECK_ALIGNMENT(PTR, ALIGN) ((void) 0)
#endif

#endif

// ===========================================================================
//
// SIMDTypes.H --
// some basic types used for SIMD programming
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 02. Mar 23 (Jonas Keller): added doxygen documentation
// 13. May 23 (Jonas Keller): added Double support

#ifndef SIMD_TYPES_H_
#define SIMD_TYPES_H_

#include <cstdint>
#include <cstdio>
#include <cstring>
#include <limits>
#include <type_traits>

namespace simd {

// ===========================================================================
// data types
// ===========================================================================

/**
 * @addtogroup group_element_types
 * @{
 */
using Byte       = uint8_t;  ///< Unsigned 8-bit integer
using SignedByte = int8_t;   ///< Signed 8-bit integer
using Word       = uint16_t; ///< Unsigned 16-bit integer
using Short      = int16_t;  ///< Signed 16-bit integer
using Int        = int32_t;  ///< Signed 32-bit integer
using Long       = int64_t;  ///< Signed 64-bit integer
using Float      = float;  ///< Single-precision floating point number (32-bit)
using Double     = double; ///< Double-precision floating point number (64-bit)
/** @} */

// exclude from doxygen (until endcond)
/// @cond

// 30. Mar 23 (Jonas Keller): replaced the magic numbers in these macros with
// std::numeric_limits<T>::max() and std::numeric_limits<T>::lowest()
// and also added SIMDFLOAT_TRUE using an immediately invoked lambda and
// std::memcpy

// 23. Nov 23 (Jonas Keller): use simd::internal::bit_cast instead of using
// memcpy directly in SIMDFLOAT_TRUE and SIMDDOUBLE_TRUE

#define SIMDBYTE_TRUE       (::simd::Byte(~0))
#define SIMDSIGNEDBYTE_TRUE (::simd::SignedByte(~0))
#define SIMDWORD_TRUE       (::simd::Word(~0))
#define SIMDSHORT_TRUE      (::simd::Short(~0))
#define SIMDINT_TRUE        (::simd::Int(~0))
#define SIMDLONG_TRUE       (::simd::Long(~0))
#define SIMDFLOAT_TRUE      (::simd::internal::bit_cast<::simd::Float>(SIMDINT_TRUE))
#define SIMDDOUBLE_TRUE                                                        \
  (::simd::internal::bit_cast<::simd::Double>(SIMDLONG_TRUE))

#define SIMDBYTE_MIN       (::std::numeric_limits<Byte>::lowest())
#define SIMDSIGNEDBYTE_MIN (::std::numeric_limits<SignedByte>::lowest())
#define SIMDWORD_MIN       (::std::numeric_limits<Word>::lowest())
#define SIMDSHORT_MIN      (::std::numeric_limits<Short>::lowest())
#define SIMDINT_MIN        (::std::numeric_limits<Int>::lowest())
#define SIMDLONG_MIN       (::std::numeric_limits<Long>::lowest())
#define SIMDFLOAT_MIN      (::std::numeric_limits<Float>::lowest())
#define SIMDDOUBLE_MIN     (::std::numeric_limits<Double>::lowest())

#define SIMDBYTE_MAX       (::std::numeric_limits<Byte>::max())
#define SIMDSIGNEDBYTE_MAX (::std::numeric_limits<SignedByte>::max())
#define SIMDWORD_MAX       (::std::numeric_limits<Word>::max())
#define SIMDSHORT_MAX      (::std::numeric_limits<Short>::max())
#define SIMDINT_MAX        (::std::numeric_limits<Int>::max())
#define SIMDLONG_MAX       (::std::numeric_limits<Long>::max())
#define SIMDFLOAT_MAX      (::std::numeric_limits<Float>::max())
#define SIMDDOUBLE_MAX     (::std::numeric_limits<Double>::max())

// 0x7fffff80
#define MAX_POS_FLOAT_CONVERTIBLE_TO_INT32 2147483520.0f
// 0x7fffff8000000000
#define MAX_POS_FLOAT_CONVERTIBLE_TO_INT64 9223371487098961920.0f
// 0x7fffffff
#define MAX_POS_DOUBLE_CONVERTIBLE_TO_INT32 2147483647.0
// 0x7ffffffffffffc00
#define MAX_POS_DOUBLE_CONVERTIBLE_TO_INT64 9223372036854774784.0
/// @endcond

// 28. Feb 23 (Jonas Keller): added SortSlope enum

/**
 * @ingroup group_types
 * @brief Used to indicate the direction of a sort function.
 */
enum class SortSlope { ASCENDING = 0, DESCENDING = 1 };

// ===========================================================================
// TypeInfo
// ===========================================================================

// NOTE: min() and max() are functions since "floating-point literals are not
// allowed in constant expressions according to -pedantic

// defaultFormat() returns a full format specifier with % and space, can be
// used for tests

/**
 * @addtogroup group_element_types
 * @{
 */

// 27. Jan 23 (Jonas Keller): added wrapper class for TypeInfo for doxygen
// documentation
// 30. Mar 23 (Jonas Keller): deprecated isSigned, isInteger, isFloatingPoint,
// min() and max() in TypeInfo in favor of the corresponding functions in
// the std library and used those functions in the implementation of
// TypeInfo

namespace internal {
namespace types {
template <typename T>
struct TypeInfo;

template <>
struct TypeInfo<Byte>
{
  static constexpr SIMD_INLINE const char *name() { return "Byte"; }
  static constexpr SIMD_INLINE const char *format() { return "u"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%u "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%02x "; }
  static constexpr SIMD_INLINE Byte trueval() { return SIMDBYTE_TRUE; }
  using NextLargerType = Word;
  using UnsignedType   = Byte;
  using SignedType     = SignedByte;
};

template <>
struct TypeInfo<SignedByte>
{
  static constexpr SIMD_INLINE const char *name() { return "SignedByte"; }
  static constexpr SIMD_INLINE const char *format() { return "d"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%d "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%02x "; }
  static constexpr SIMD_INLINE SignedByte trueval()
  {
    return SIMDSIGNEDBYTE_TRUE;
  }
  using NextLargerType = Short;
  using UnsignedType   = Byte;
  using SignedType     = SignedByte;
};

template <>
struct TypeInfo<Word>
{
  static constexpr SIMD_INLINE const char *name() { return "Word"; }
  static constexpr SIMD_INLINE const char *format() { return "u"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%u "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%04x "; }
  static constexpr SIMD_INLINE Word trueval() { return SIMDWORD_TRUE; }
  using NextLargerType = Int; // no larger unsigned type, use Int
  using UnsignedType   = Word;
  using SignedType     = Short;
};

template <>
struct TypeInfo<Short>
{
  static constexpr SIMD_INLINE const char *name() { return "Short"; }
  static constexpr SIMD_INLINE const char *format() { return "d"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%d "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%04x "; }
  static constexpr SIMD_INLINE Short trueval() { return SIMDSHORT_TRUE; }
  using NextLargerType = Int;
  using UnsignedType   = Word;
  using SignedType     = Short;
};

template <>
struct TypeInfo<Int>
{
  static constexpr SIMD_INLINE const char *name() { return "Int"; }
  static constexpr SIMD_INLINE const char *format() { return "d"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%d "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%08x "; }
  static constexpr SIMD_INLINE Int trueval() { return SIMDINT_TRUE; }
  using NextLargerType = Long;
  using UnsignedType   = uint32_t; // not a SIMD type
  using SignedType     = Int;
};

template <>
struct TypeInfo<Long>
{
  static constexpr SIMD_INLINE const char *name() { return "Long"; }
  static constexpr SIMD_INLINE const char *format() { return "ld"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%ld "; }
  static constexpr SIMD_INLINE const char *hexFormat() { return "%016lx "; }
  static constexpr SIMD_INLINE Long trueval() { return SIMDLONG_TRUE; }
  using NextLargerType = Long;     // no larger integer type than Long
  using UnsignedType   = uint64_t; // not a SIMD type
  using SignedType     = Long;
};

template <>
struct TypeInfo<Float>
{
  static constexpr SIMD_INLINE const char *name() { return "Float"; }
  static constexpr SIMD_INLINE const char *format() { return ".9g"; }
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%.9g "; }
  static SIMD_INLINE Float trueval() { return SIMDFLOAT_TRUE; }
  using NextLargerType = Double;
  using UnsignedType   = Float; // no unsigned float type
  using SignedType     = Float;
};

template <>
struct TypeInfo<Double>
{
  static constexpr SIMD_INLINE const char *name() { return "Double"; };
  static constexpr SIMD_INLINE const char *format() { return ".17g"; };
  static constexpr SIMD_INLINE const char *defaultFormat() { return "%.17g "; };
  static SIMD_INLINE Double trueval() { return SIMDDOUBLE_TRUE; }
  using NextLargerType = Double; // no larger double type than Double
  using UnsignedType   = Double; // no unsigned double type
  using SignedType     = Double;
};

} // namespace types
} // namespace internal

/**
 * @brief Type information for SIMD types
 */
template <typename T>
struct TypeInfo
{
  /// @brief Returns the name of the type (e.g. "Int" for Int)
  static constexpr SIMD_INLINE const char *name()
  {
    return internal::types::TypeInfo<T>::name();
  };
  /// @brief Returns the format string for printf (e.g. "d" for Int)
  static constexpr SIMD_INLINE const char *format()
  {
    return internal::types::TypeInfo<T>::format();
  };
  /// @brief Returns the default format string for printf (e.g. "%d " for
  /// Int)
  static constexpr SIMD_INLINE const char *defaultFormat()
  {
    return internal::types::TypeInfo<T>::defaultFormat();
  };
  /// @brief Returns the hex format string for printf (e.g. "%08x " for Int)
  static constexpr SIMD_INLINE const char *hexFormat()
  {
    return internal::types::TypeInfo<T>::hexFormat();
  }
  /**
   * @brief Whether the type is signed
   * @deprecated Use std::is_signed<T>::value or (if C++17 is available)
   * std::is_signed_v<T> instead
   */
  static constexpr bool isSigned = std::is_signed<T>::value;
  /**
   * @brief Whether the type is an integer
   * @deprecated Use std::is_integral<T>::value or (if C++17 is available)
   * std::is_integral_v<T> instead
   */
  static constexpr bool isInteger = std::is_integral<T>::value;
  /**
   * @brief Whether the type is a floating point type
   * @deprecated Use std::is_floating_point<T>::value or (if C++17 is
   * available) std::is_floating_point_v<T> instead
   */
  static constexpr bool isFloatingPoint = std::is_floating_point<T>::value;
  /**
   * @brief Returns the minimum value of the type
   * @deprecated Use std::numeric_limits<T>::lowest() instead
   */
  static constexpr SIMD_INLINE T min()
  {
    return std::numeric_limits<T>::lowest();
  }
  /**
   * @brief Returns the maximum value of the type
   * @deprecated Use std::numeric_limits<T>::max() instead
   */
  static constexpr SIMD_INLINE T max() { return std::numeric_limits<T>::max(); }
  /// @brief Returns a value where all bits are 1
  static constexpr SIMD_INLINE T trueval()
  {
    return internal::types::TypeInfo<T>::trueval();
  }
  /// @brief The next larger type (e.g. Word for Byte), or the same
  /// type if there is no larger type
  using NextLargerType = typename internal::types::TypeInfo<T>::NextLargerType;
  /// @brief The unsigned type (e.g. Byte for SignedByte), or the same
  /// type if there is no unsigned type
  using UnsignedType = typename internal::types::TypeInfo<T>::UnsignedType;
  /// @brief The signed type (e.g. SignedByte for Byte), or the same
  /// type if there is no signed type
  using SignedType = typename internal::types::TypeInfo<T>::SignedType;
};

// ===========================================================================
// formatting
// ===========================================================================

// note that for T=Float, TypeInfo<T>::format() returns "g",
// for which precision encodes the number of significant digits,
// not the number of fractional digits
/**
 * @brief Class for generating format strings for printf for SIMD types
 *
 * @tparam T type for which to generate a format string
 */
template <typename T>
struct Format
{
  char format[256]; ///< The generated format string

  /**
   * @brief Constructor
   *
   * @param fieldWidth field width (e.g. 4 for "%4d")
   * @param precision precision (e.g. 2 for "%.2f")
   */
  Format(int fieldWidth = -1, int precision = -1)
  {
    char fieldWidthStr[16], precisionStr[16];
    if (fieldWidth >= 0)
      sprintf(fieldWidthStr, "%d", fieldWidth);
    else
      strcpy(fieldWidthStr, "");
    if (precision >= 0)
      sprintf(precisionStr, ".%d", precision);
    else
      strcpy(precisionStr, "");
    if (TypeInfo<T>::isInteger)
      // integer format, precision is ignored
      sprintf(format, "%%%s%s", fieldWidthStr, TypeInfo<T>::format());
    else
      // float format, precision is used
      sprintf(format, "%%%s%s%s", fieldWidthStr, precisionStr,
              TypeInfo<T>::format());
  }
};

// a crude way to format SIMD* types as decimal number, can be used in
// fprintf (use %s format specification)
/**
 * @brief Class for formatting SIMD types as decimal numbers
 *
 * @tparam T type to format
 */
template <typename T>
struct Decimal
{
  char str[256]; ///< The formatted string

  /**
   * @brief Constructor
   *
   * @param value value to format
   * @param fieldWidth field width (e.g. 4 for "%4d")
   * @param precision precision (e.g. 2 for "%.2f")
   */
  Decimal(T value, int fieldWidth = -1, int precision = -1)
  {
    sprintf(str, Format<T>(fieldWidth, precision).format, value);
  }
};

/** @} */

// 07. Oct 23 (Jonas Keller): added dont_deduce

namespace internal {
template <typename T>
struct dont_deduce
{
  using type = T;
};
} // namespace internal

/**
 * @brief Helper type to prevent template argument deduction.
 *
 * This type can be used to prevent template argument deduction for a given
 * type.
 *
 * @tparam T The type to prevent deduction for.
 */
template <typename T>
using dont_deduce = typename internal::dont_deduce<T>::type;

// 22. Jan 23 (Jonas Keller): moved tag dispatching classes into internal
// namespace

namespace internal {
// ===========================================================================
// tag dispatching
// ===========================================================================

// int2type trick from
// Andrei Alexandrescu: Modern C++ Design (Addison Wesley)

// 08. Apr 23 (Jonas Keller): removed IsIntSize; it is no longer used
// 24. Nov 23 (Jonas Keller): removed some more unused tag dispatching classes

template <size_t N>
struct Integer
{};

template <size_t N>
struct Part
{};

template <size_t N>
struct Elements
{};

template <size_t N>
struct Bytes
{};

template <bool AT_LOWER_LIMIT, size_t LOWER_LIMIT_INCLUSIVE,
          size_t UPPER_LIMIT_EXCLUSIVE>
struct Range
{};

template <size_t IMM, size_t SIZE>
struct SizeRange
  : public Range<(IMM & (SIZE - 1)) == 0,    // is IMM a multiple of SIZE?
                 IMM & ~(SIZE - 1),          // previous multiple of SIZE
                 (IMM & ~(SIZE - 1)) + SIZE> // next multiple of SIZE
{};

template <size_t N>
struct Compression
{};

template <typename T>
struct OutputType
{};
} // namespace internal

} // namespace simd

#endif

// ===========================================================================
//
// SIMDVec.H --
// generic template for Vec
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 13. Feb 23 (Jonas Keller): removed "SIMD"-prefix from most types
// (versions with "SIMD"-prefix are still available for backward
// compatibility in SIMDBackwardCompat.H)

// 09. Mar 23 (Jonas Keller): added doxygen documentation

#ifndef SIMD_VEC_H_
#define SIMD_VEC_H_

#include <algorithm>
#include <cstddef>
#include <type_traits>

/**
 * @brief Namespace for T-SIMD.
 */
namespace simd {

// TODO: - absdiff also for unsigned types -> SSE lecture vecintrin66
// TODO: - bitwise shift: what about float?
// TODO: - loadr / storer
// TODO: - element-wise rotation in a vector using alignre
// TODO: - functions for rsqrt, rcp Newton *steps*?
// TODO: - add to names rcp, rsqrt something with "estimate"?
// TODO: - NEON has a "set1" with immediate arguments (vmovq_n), so it would
// TODO:   be nice to have a set1const function with template argument, but
// TODO:   this only works for integers since float template parameter are not
// TODO:   allowed by the standard

// ===========================================================================
// generic template for Vec and Mask
// ===========================================================================

// 30. Sep 19 (rm): Mask support contributed by Markus Vieth

// specialized for type of elements and number of bytes in the SIMD vector
/**
 * @ingroup group_types
 * @brief SIMD vector class, holds multiple elements of the same type.
 *
 * @tparam T type of the vector elements
 * @tparam SIMD_WIDTH number of bytes in the SIMD vector
 */
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
class Vec
#ifdef DOXYGEN
{
public:
  /**
   * @brief Number of elements in the vector.
   */
  static constexpr size_t elements = SIMD_WIDTH / sizeof(T);

  /**
   * @brief Number of elements in the vector. Alias for @ref elements.
   */
  static constexpr size_t elems = elements;

  /**
   * @brief Number of bytes in the vector.
   */
  static constexpr size_t bytes = SIMD_WIDTH;

  // 05. Sep 23 (Jonas Keller): added allocator
  /**
   * @brief Allocator to be used with std::vector.
   *
   * This allocator is to be used when creating a std::vector of Vec in the
   * following way:
   *
   * @code
   * std::vector<Vec<T, SIMD_WIDTH>, typename Vec<T, SIMD_WIDTH>::allocator> v;
   * @endcode
   */
  using allocator = simd_aligned_allocator<Vec<T, SIMD_WIDTH>, SIMD_WIDTH>;
}
#endif
;

/**
 * @ingroup group_types
 * @brief SIMD mask class consisting of as many bits as the corresponding
 *        Vec has elements.
 *
 * @tparam T type of the elements of the corresponding Vec
 * @tparam SIMD_WIDTH number of bytes in the corresponding Vec
 */
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
class Mask
#ifdef DOXYGEN
{
public:
  /**
   * @brief Constructs a Mask from a Vec.
   *
   * The Mask bits are set to 1 if the corresponding Vec element has its
   * most significant bit set, otherwise the Mask bit is set to 0.
   */
  explicit SIMD_INLINE Mask<T, SIMD_WIDTH>(const Vec<T, SIMD_WIDTH> &x);

  /**
   * @brief Constructs a Mask from an integer.
   *
   * The lowest bits of the integer are used to set the Mask bits.
   */
  explicit SIMD_INLINE Mask<T, SIMD_WIDTH>(const uint64_t x);

  /**
   * @brief Converts the Mask to a Vec.
   *
   * The bits of the Vec elements are set to all 1s if the corresponding
   * Mask bit is set, otherwise the Vec element bits are set to 0.
   *
   * @return converted Vec from the Mask
   */
  explicit SIMD_INLINE operator Vec<T, SIMD_WIDTH>() const { return mask; };

  /**
   * @brief Converts the Mask to an integer.
   *
   * The lowest bits of the integer are set to the Mask bits.
   *
   * @return converted integer from the Mask
   */
  explicit SIMD_INLINE operator uint64_t() const;

  /**
   * @brief Returns the Mask bit at the given index.
   *
   * @param i index of the Mask bit
   * @return whether the Mask bit at the given index is set
   */
  SIMD_INLINE bool operator[](const size_t i) const;

  /**
   * @brief Compares the Mask with another Mask.
   *
   * @param other Mask to compare with
   * @return whether the Mask is equal to the other Mask
   */
  SIMD_INLINE bool operator==(const Mask<T, SIMD_WIDTH> &other) const;
}
#endif
;

// ===========================================================================
// helper functions for templates converting from Tin to Tout
// ===========================================================================

// numInVecs() and numOutVecs() assume that either
// - a single vector is extended into multiple vectors or
// - multiple vectors are packed into a single vector
//
// numSIMDVecsElements encodes the number of elements in *all* input / all
// output vectors
//
// removed: numSIMDVecElements encodes the number of elements in each vector
// (or use Vec::elements instead)
//
// removed: numInputSIMDVecElements/numOutputSIMDVecElements encode
// the number of elements in *each* input / output vector

/**
 * @ingroup group_type_conversion
 * @brief Number of input vectors for functions that potentially change the
 * size of the elements but not the number of elements.
 *
 * @tparam Tout output type
 * @tparam Tin input type
 * @return number of input vectors
 */
template <typename Tout, typename Tin>
static constexpr SIMD_INLINE size_t numInVecs()
{
  return (sizeof(Tout) < sizeof(Tin)) ? (sizeof(Tin) / sizeof(Tout)) : 1;
}

/**
 * @ingroup group_type_conversion
 * @brief Number of output vectors for functions that potentially change the
 * size of the elements but not the number of elements.
 *
 * @tparam Tout output type
 * @tparam Tin input type
 * @return number of output vectors
 */
template <typename Tout, typename Tin>
static constexpr SIMD_INLINE size_t numOutVecs()
{
  return (sizeof(Tout) > sizeof(Tin)) ? (sizeof(Tout) / sizeof(Tin)) : 1;
}

/**
 * @ingroup group_type_conversion
 * @brief Number of elements in all input vectors for functions that potentially
 * change the size of the elements but not the number of elements.
 *
 * @tparam Tout output type
 * @tparam Tin input type
 * @return number of elements in all input vectors
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static constexpr SIMD_INLINE size_t numSIMDVecsElements()
{
  return (sizeof(Tout) > sizeof(Tin)) ? Vec<Tin, SIMD_WIDTH>::elems :
                                        Vec<Tout, SIMD_WIDTH>::elems;
}

// 13. May 23 (Jonas Keller): added BigEnoughFloat

namespace internal {
namespace vec {
// std::max and std::min are not constexpr in C++11, so we need to provide our
// own, since we need them in a constexpr context
template <typename T>
constexpr const T &max(const T &a, const T &b)
{
  return (a < b) ? b : a;
}

template <typename T>
constexpr const T &min(const T &a, const T &b)
{
  return (a < b) ? a : b;
}
} // namespace vec
} // namespace internal

/**
 * @ingroup group_fops
 * @brief Smallest floating point type that is at least as big as the
 * input and output types.
 *
 * @tparam Tout output type
 * @tparam Tin input type
 */
template <typename Tout, typename Tin>
using BigEnoughFloat =
#ifdef SIMD_64BIT_TYPES
  typename std::conditional<internal::vec::max(sizeof(Tout), sizeof(Tin)) <=
                              sizeof(Float),
                            Float, Double>::type;
#else
  Float;
#endif

// 22. Jan 23 (Jonas Keller): removed primary template functions, as they are
// not needed anymore due to a wrapper layer

} // namespace simd

#endif

// ===========================================================================
//
// SIMDVecBaseImplIntel16.H --
// encapsulation for SSE Intel vector extensions
// inspired by Agner Fog's C++ Vector Class Library
// http://www.agner.org/optimize/#vectorclass
// (VCL License: GNU General Public License Version 3,
//  http://www.gnu.org/licenses/gpl-3.0.en.html)
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace
// 13. May 23 (Jonas Keller): added Double support

#ifndef SIMD_VEC_BASE_IMPL_INTEL_16_H_
#define SIMD_VEC_BASE_IMPL_INTEL_16_H_

// ===========================================================================
//
// SIMDAlloc.H --
// allocation code
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 30. Aug 22 (Jonas Keller): added simd_aligned_malloc and simd_aligned_free

// 02. Mar 23 (Jonas Keller): added doxygen documentation

/**
 * @file SIMDAlloc.H
 * @brief Aligned memory allocation and deallocation.
 *
 * This file contains functions for aligned memory allocation and deallocation.
 *
 * This file is standalone, i.e. it can also be used independently of T-SIMD.
 *
 * @author Ralf Möller
 * @author Jonas Keller
 */

#ifndef SIMDALLOC_H_
#define SIMDALLOC_H_

#include <cstddef>
#include <utility>

#ifdef _WIN32
#include <malloc.h>
#else
#include <cstdlib>
#endif

/**
 * @ingroup group_aligned_alloc
 * @brief Aligned memory allocation.
 *
 * This function allocates a block of memory of size bytes, aligned to the
 * specified alignment.
 *
 * The allocated memory must be freed with simd_aligned_free().
 *
 * @param alignment alignment of the memory block in bytes
 * @param size size of the memory block in bytes
 * @return pointer to the allocated memory block
 */
inline void *simd_aligned_malloc(size_t alignment, size_t size)
{
#ifdef _WIN32
  return _aligned_malloc(size, alignment);
#else
  void *ptr = nullptr;
  if (posix_memalign(&ptr, alignment, size) != 0) { return nullptr; }
  return ptr;
#endif
}

/**
 * @ingroup group_aligned_alloc
 * @brief Aligned memory deallocation.
 *
 * This function frees a block of memory that was allocated with
 * simd_aligned_malloc().
 *
 * @param ptr pointer to the memory block to be freed
 */
inline void simd_aligned_free(void *ptr)
{
#ifdef _WIN32
  _aligned_free(ptr);
#else
  free(ptr);
#endif
}

// 05. Sep 23 (Jonas Keller): added simd_aligned_allocator

/**
 * @ingroup group_aligned_alloc
 * @brief Aligned allocator.
 *
 * This class is an allocator that allocates aligned memory blocks.
 *
 * This class is meant to be used with std::vector for types that require
 * aligned memory blocks.
 *
 * @tparam T type of the elements in the memory block
 * @tparam ALIGN alignment of the memory block in bytes
 */
template <typename T, size_t ALIGN>
class simd_aligned_allocator
{
  // exclude from doxygen (until endcond)
  /// @cond
public:
  using value_type      = T;
  using pointer         = T *;
  using const_pointer   = const T *;
  using reference       = T &;
  using const_reference = const T &;
  using size_type       = std::size_t;
  using difference_type = std::ptrdiff_t;

  template <typename U>
  struct rebind
  {
    using other = simd_aligned_allocator<U, ALIGN>;
  };

  simd_aligned_allocator() noexcept {}
  simd_aligned_allocator(const simd_aligned_allocator &) noexcept {}
  template <typename U>
  simd_aligned_allocator(const simd_aligned_allocator<U, ALIGN> &) noexcept
  {}
  ~simd_aligned_allocator() noexcept {}

  pointer address(reference x) const noexcept { return std::addressof(x); }
  const_pointer address(const_reference x) const noexcept
  {
    return std::addressof(x);
  }

  pointer allocate(size_type n, const void * = 0)
  {
    return static_cast<pointer>(simd_aligned_malloc(ALIGN, n * sizeof(T)));
  }
  void deallocate(pointer p, size_type) { simd_aligned_free(p); }

  size_type max_size() const noexcept
  {
    return (size_type(-1) - size_type(ALIGN)) / sizeof(T);
  }
  template <typename U, typename... Args>
  void construct(U *p, Args &&...args)
  {
    ::new (static_cast<void *>(p)) U(std::forward<Args>(args)...);
  }
  void destroy(pointer p) { p->~T(); }

  bool operator==(const simd_aligned_allocator &) const { return true; }
  bool operator!=(const simd_aligned_allocator &) const { return false; }
  /// @endcond
};

#endif // SIMDALLOC_H_

// ===========================================================================
//
// SIMDIntrinsIntel.H --
// includes include files for vector intrinsics on Intel CPUs
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SIMD_INTRINS_INTEL_H_
#define SIMD_INTRINS_INTEL_H_

#ifdef SIMDVEC_INTEL_ENABLE

// 30. Aug 22 (Jonas Keller):
// gcc warns that the value returned by the _mm*_undefined_* intrinsics is
// used uninitialized, which is exactly what these intrinsics are for,
// so disabling the warning when compiling with gcc
#pragma GCC diagnostic push
#if defined(__GNUC__) && !defined(__llvm__) && !defined(__INTEL_COMPILER)
#pragma GCC diagnostic ignored "-Wmaybe-uninitialized"
#pragma GCC diagnostic ignored "-Wuninitialized"
#endif
#include <x86intrin.h>
#pragma GCC diagnostic pop

// ---------------------------------------------------------------------------
// some definitions are missing for -O0 in some versions of gcc (e.g. 5.4)
// ---------------------------------------------------------------------------

// 15. Nov 22 (Jonas Keller): moved this to here from SIMDVecBaseImplIntel64.H

// bug seems to be fixed in avx512bwintrin.h in gcc 5.5.0

#if defined(__GNUC__) && !defined(__clang__) && !defined(__INTEL_COMPILER) &&  \
  (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) < 50500 &&   \
  !defined(__OPTIMIZE__) && defined(__AVX512BW__)

// _mm512_pack[u]s_epi32 doesn't need a define (no int arguments),
// but is not available without optimization (error in include file)

extern __inline __m512i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_packs_epi32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packssdw512_mask(
    (__v16si) __A, (__v16si) __B, (__v32hi) _mm512_setzero_hi(),
    (__mmask32) -1);
}

extern __inline __m512i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_packus_epi32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_packusdw512_mask(
    (__v16si) __A, (__v16si) __B, (__v32hi) _mm512_setzero_hi(),
    (__mmask32) -1);
}

#endif

// ---------------------------------------------------------------------------
// masked abs for float and double missing in gcc below version 7
// ---------------------------------------------------------------------------

// 24. Nov 23 (Jonas Keller):
// added this fix for missing masked abs for float and double in gcc < 7

// _mm512_abs_ps, _mm512_mask_abs_ps, _mm512_abs_pd and _mm512_mask_abs_pd are
// missing in gcc below version 7
// see https://gcc.gnu.org/pipermail/gcc-patches/2017-April/472183.html

// implementation from
// https://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avx512fintrin.h

#if defined(__GNUC__) && !defined(__clang__) && !defined(__INTEL_COMPILER) &&  \
  (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) < 70000 &&   \
  defined(__AVX512F__)

extern __inline __m512
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_abs_ps(__m512 __A)
{
  return (__m512) _mm512_and_epi32((__m512i) __A,
                                   _mm512_set1_epi32(0x7fffffff));
}

extern __inline __m512
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_mask_abs_ps(__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) _mm512_mask_and_epi32((__m512i) __W, __U, (__m512i) __A,
                                        _mm512_set1_epi32(0x7fffffff));
}

extern __inline __m512d
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_abs_pd(__m512d __A)
{
  return (__m512d) _mm512_and_epi64((__m512i) __A,
                                    _mm512_set1_epi64(0x7fffffffffffffffLL));
}

extern __inline __m512d
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_mask_abs_pd(__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) _mm512_mask_and_epi64(
    (__m512i) __W, __U, (__m512i) __A, _mm512_set1_epi64(0x7fffffffffffffffLL));
}

#endif

// ---------------------------------------------------------------------------
// _mm256_set_m128 and friends are missing in gcc below version 8
// ---------------------------------------------------------------------------

// 14. May 23 (Jonas Keller):
// added this fix for missing _mm256_set_m128 and friends in gcc < 8

// implementation from
// https://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avxintrin.h

#if defined(__GNUC__) && !defined(__clang__) && !defined(__INTEL_COMPILER) &&  \
  (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) < 80000 &&   \
  defined(__AVX__)

extern __inline __m256
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_set_m128(__m128 __H, __m128 __L)
{
  return _mm256_insertf128_ps(_mm256_castps128_ps256(__L), __H, 1);
}

extern __inline __m256d
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_set_m128d(__m128d __H, __m128d __L)
{
  return _mm256_insertf128_pd(_mm256_castpd128_pd256(__L), __H, 1);
}

extern __inline __m256i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_set_m128i(__m128i __H, __m128i __L)
{
  return _mm256_insertf128_si256(_mm256_castsi128_si256(__L), __H, 1);
}

extern __inline __m256
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_setr_m128(__m128 __L, __m128 __H)
{
  return _mm256_set_m128(__H, __L);
}

extern __inline __m256d
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_setr_m128d(__m128d __L, __m128d __H)
{
  return _mm256_set_m128d(__H, __L);
}

extern __inline __m256i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm256_setr_m128i(__m128i __L, __m128i __H)
{
  return _mm256_set_m128i(__H, __L);
}

#endif

// ---------------------------------------------------------------------------
// _mm512_set_epi8 and _mm512_set_epi16 missing in gcc below version 9
// ---------------------------------------------------------------------------

// 29. Sep 23 (Jonas Keller):
// added this fix for missing _mm512_set_epi8 and _mm512_set_epi16 in gcc < 9

// implementation from
// https://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avx512fintrin.h

#if defined(__GNUC__) && !defined(__clang__) && !defined(__INTEL_COMPILER) &&  \
  (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) < 90000 &&   \
  defined(__AVX512F__)

extern __inline __m512i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_set_epi8(char __q63, char __q62, char __q61, char __q60, char __q59,
                  char __q58, char __q57, char __q56, char __q55, char __q54,
                  char __q53, char __q52, char __q51, char __q50, char __q49,
                  char __q48, char __q47, char __q46, char __q45, char __q44,
                  char __q43, char __q42, char __q41, char __q40, char __q39,
                  char __q38, char __q37, char __q36, char __q35, char __q34,
                  char __q33, char __q32, char __q31, char __q30, char __q29,
                  char __q28, char __q27, char __q26, char __q25, char __q24,
                  char __q23, char __q22, char __q21, char __q20, char __q19,
                  char __q18, char __q17, char __q16, char __q15, char __q14,
                  char __q13, char __q12, char __q11, char __q10, char __q09,
                  char __q08, char __q07, char __q06, char __q05, char __q04,
                  char __q03, char __q02, char __q01, char __q00)
{
  return __extension__(__m512i)(__v64qi) {
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07, __q08, __q09, __q10,
    __q11, __q12, __q13, __q14, __q15, __q16, __q17, __q18, __q19, __q20, __q21,
    __q22, __q23, __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31, __q32,
    __q33, __q34, __q35, __q36, __q37, __q38, __q39, __q40, __q41, __q42, __q43,
    __q44, __q45, __q46, __q47, __q48, __q49, __q50, __q51, __q52, __q53, __q54,
    __q55, __q56, __q57, __q58, __q59, __q60, __q61, __q62, __q63};
}

extern __inline __m512i
  __attribute__((__gnu_inline__, __always_inline__, __artificial__))
  _mm512_set_epi16(short __q31, short __q30, short __q29, short __q28,
                   short __q27, short __q26, short __q25, short __q24,
                   short __q23, short __q22, short __q21, short __q20,
                   short __q19, short __q18, short __q17, short __q16,
                   short __q15, short __q14, short __q13, short __q12,
                   short __q11, short __q10, short __q09, short __q08,
                   short __q07, short __q06, short __q05, short __q04,
                   short __q03, short __q02, short __q01, short __q00)
{
  return __extension__(__m512i)(__v32hi) {
    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07, __q08, __q09, __q10,
    __q11, __q12, __q13, __q14, __q15, __q16, __q17, __q18, __q19, __q20, __q21,
    __q22, __q23, __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31};
}

#endif

#endif // SIMDVEC_INTEL_ENABLE

#endif // SIMD_INTRINS_INTEL_H_

// ===========================================================================
//
// SIMDSSSE3Compat.H --
// compatibility code for CPUs without SSE3 or SSSE3
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SSSE3_COMPAT_H_
#define SSSE3_COMPAT_H_

#include <cmath>
#include <cstddef>
#include <cstdint>

// SSE3/SSSE3 emulation for very old CPUs (very inefficient sequential code!)

// replacement for SSE3 instructions
// _mm_hadd_ps
// _mm_hsub_ps
//
// replacement for SSSE3 instructions
// _mm_abs_epi8
// _mm_abs_epi16
// _mm_abs_epi32
// _mm_alignr_epi8
// _mm_hadd_epi16
// _mm_hadd_epi32
// _mm_hadds_epi16
// _mm_hsub_epi16
// _mm_hsub_epi32
// _mm_hsubs_epi16
// _mm_shuffle_epi8
// _mm_sign_epi16

#ifdef __SSE2__

namespace simd {

// ===========================================================================
// SSE3 replacements
// ===========================================================================

#ifndef __SSE3__

// #warning "SSE3 intrinsics are replaced by slow sequential implementations"

static inline __m128 _mm_hadd_ps(__m128 a, __m128 b)
{
  float atmp[4] SIMD_ATTR_ALIGNED(16);
  float btmp[4] SIMD_ATTR_ALIGNED(16);
  float tmp[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_ps(atmp, a);
  _mm_store_ps(btmp, b);
  tmp[0] = atmp[1] + atmp[0];
  tmp[1] = atmp[3] + atmp[2];
  tmp[2] = btmp[1] + btmp[0];
  tmp[3] = btmp[3] + btmp[2];
  return _mm_load_ps(tmp);
}

static inline __m128 _mm_hsub_ps(__m128 a, __m128 b)
{
  float atmp[4] SIMD_ATTR_ALIGNED(16);
  float btmp[4] SIMD_ATTR_ALIGNED(16);
  float tmp[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_ps(atmp, a);
  _mm_store_ps(btmp, b);
  tmp[0] = atmp[0] - atmp[1];
  tmp[1] = atmp[2] - atmp[3];
  tmp[2] = btmp[0] - btmp[1];
  tmp[3] = btmp[2] - btmp[3];
  return _mm_load_ps(tmp);
}

#endif

// ===========================================================================
// SSE3 replacements
// ===========================================================================

#ifndef __SSSE3__

// #warning "SSSE3 intrinsics are replaced by slow sequential implementations"

static inline __m128i _mm_abs_epi8(__m128i a)
{
  int8_t tmp[16] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) tmp, a);
  for (size_t i = 0; i < 16; i++) tmp[i] = std::abs(tmp[i]);
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_abs_epi16(__m128i a)
{
  int16_t tmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) tmp, a);
  for (size_t i = 0; i < 8; i++) tmp[i] = std::abs(tmp[i]);
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_abs_epi32(__m128i a)
{
  int32_t tmp[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) tmp, a);
  for (size_t i = 0; i < 4; i++) tmp[i] = std::abs(tmp[i]);
  return _mm_load_si128((__m128i *) tmp);
}

// 24. Jul 18 (rm): strange, definitions from tmmintrin.h are
// included even if ssse3 is not available;
// gcc: for undefined __OPTIMIZE__, the macro _mm_alignr_epi8
// clashes with the definition below;
// clang: _mm_alignr_epi8 always defined as macro, regardless of
// __OPTIMIZE__; also clashes with the definition below
// solution: undefine the macro if it is defined
#ifdef _mm_alignr_epi8
#undef _mm_alignr_epi8
#endif

// 23. Sep 15 (rm): fixed several bugs
static inline __m128i _mm_alignr_epi8(__m128i a, __m128i b, int n)
{
  int8_t abtmp[32] SIMD_ATTR_ALIGNED(16);
  int8_t rtmp[16] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) abtmp, b);
  _mm_store_si128((__m128i *) (abtmp + 16), a);
  for (size_t i = 0; i < 16; i++) {
    const size_t j = i + n;
    if (j < 32)
      rtmp[i] = abtmp[j];
    else
      rtmp[i] = 0;
  }
  return _mm_load_si128((__m128i *) rtmp);
}

static inline __m128i _mm_hadd_epi16(__m128i a, __m128i b)
{
  int16_t atmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t btmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t tmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = atmp[1] + atmp[0];
  tmp[1] = atmp[3] + atmp[2];
  tmp[2] = atmp[5] + atmp[4];
  tmp[3] = atmp[7] + atmp[6];
  tmp[4] = btmp[1] + btmp[0];
  tmp[5] = btmp[3] + btmp[2];
  tmp[6] = btmp[5] + btmp[4];
  tmp[7] = btmp[7] + btmp[6];
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_hsub_epi16(__m128i a, __m128i b)
{
  int16_t atmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t btmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t tmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = atmp[0] - atmp[1];
  tmp[1] = atmp[2] - atmp[3];
  tmp[2] = atmp[4] - atmp[5];
  tmp[3] = atmp[6] - atmp[7];
  tmp[4] = btmp[0] - btmp[1];
  tmp[5] = btmp[2] - btmp[3];
  tmp[6] = btmp[4] - btmp[5];
  tmp[7] = btmp[6] - btmp[7];
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_hadd_epi32(__m128i a, __m128i b)
{
  int32_t atmp[4] SIMD_ATTR_ALIGNED(16);
  int32_t btmp[4] SIMD_ATTR_ALIGNED(16);
  int32_t tmp[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = atmp[1] + atmp[0];
  tmp[1] = atmp[3] + atmp[2];
  tmp[2] = btmp[1] + btmp[0];
  tmp[3] = btmp[3] + btmp[2];
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_hsub_epi32(__m128i a, __m128i b)
{
  int32_t atmp[4] SIMD_ATTR_ALIGNED(16);
  int32_t btmp[4] SIMD_ATTR_ALIGNED(16);
  int32_t tmp[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = atmp[0] - atmp[1];
  tmp[1] = atmp[2] - atmp[3];
  tmp[2] = btmp[0] - btmp[1];
  tmp[3] = btmp[2] - btmp[3];
  return _mm_load_si128((__m128i *) tmp);
}

static inline int16_t adds16(int16_t a, int16_t b)
{
  int32_t s = int32_t(a) + int32_t(b);
  return (s < -0x8000) ? -0x8000 : (s > 0x7fff) ? 0x7fff : s;
}

static inline __m128i _mm_hadds_epi16(__m128i a, __m128i b)
{
  int16_t atmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t btmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t tmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = adds16(atmp[1], atmp[0]);
  tmp[1] = adds16(atmp[3], atmp[2]);
  tmp[2] = adds16(atmp[5], atmp[4]);
  tmp[3] = adds16(atmp[7], atmp[6]);
  tmp[4] = adds16(btmp[1], btmp[0]);
  tmp[5] = adds16(btmp[3], btmp[2]);
  tmp[6] = adds16(btmp[5], btmp[4]);
  tmp[7] = adds16(btmp[7], btmp[6]);
  return _mm_load_si128((__m128i *) tmp);
}

static inline int16_t subs16(int16_t a, int16_t b)
{
  int32_t s = int32_t(a) - int32_t(b);
  return (s < -0x8000) ? -0x8000 : (s > 0x7fff) ? 0x7fff : s;
}

// 12. Aug 16 (rm): fixed bug: adds16->subs16
static inline __m128i _mm_hsubs_epi16(__m128i a, __m128i b)
{
  int16_t atmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t btmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t tmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  tmp[0] = subs16(atmp[0], atmp[1]);
  tmp[1] = subs16(atmp[2], atmp[3]);
  tmp[2] = subs16(atmp[4], atmp[5]);
  tmp[3] = subs16(atmp[6], atmp[7]);
  tmp[4] = subs16(btmp[0], btmp[1]);
  tmp[5] = subs16(btmp[2], btmp[3]);
  tmp[6] = subs16(btmp[4], btmp[5]);
  tmp[7] = subs16(btmp[6], btmp[7]);
  return _mm_load_si128((__m128i *) tmp);
}

static inline __m128i _mm_shuffle_epi8(__m128i a, __m128i mask)
{
  uint8_t atmp[16] SIMD_ATTR_ALIGNED(16);
  uint8_t masktmp[16] SIMD_ATTR_ALIGNED(16);
  uint8_t rtmp[16] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) masktmp, mask);
  for (size_t i = 0; i < 16; i++)
    rtmp[i] = (masktmp[i] & 0x80) ? 0 : atmp[masktmp[i] & 0x0f];
  return _mm_load_si128((__m128i *) rtmp);
}

//  1. Oct 14 (rm): added
static inline __m128i _mm_sign_epi16(__m128i a, __m128i b)
{
  int16_t atmp[8] SIMD_ATTR_ALIGNED(16);
  int16_t btmp[8] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) atmp, a);
  _mm_store_si128((__m128i *) btmp, b);
  for (size_t i = 0; i < 8; i++)
    if (btmp[i] < 0) atmp[i] = -atmp[i];
  return _mm_load_si128((__m128i *) atmp);
}

#endif

} // namespace simd

#endif // __SSE2__

#endif // SSSE3_COMPAT_H_

#include <cmath>
#include <cstddef>
#include <cstdint>
#include <limits>
#include <type_traits>

#if defined(SIMDVEC_INTEL_ENABLE) && defined(_SIMD_VEC_16_AVAIL_) &&           \
  !defined(SIMDVEC_SANDBOX)

namespace simd {

// ===========================================================================
// NOTES:
//
// - setting zero inside the function is not inefficient, see:
//   http://stackoverflow.com/questions/26807285/...
//   ...are-static-static-local-sse-avx-variables-blocking-a-xmm-ymm-register
//
// - for some data types (Int, Float) there are no saturated versions
//   of add/sub instructions; in this case we use the unsaturated version;
//   the user is responsible to avoid overflows
//
// - we could improve performance by using 128-bit instructions from
//   AVX512-VL (e.g. permute instructions); at the moment the idea is that
//   typically the widest vector width is used, so if AVX512 is available,
//   SSE would only rarely be used
//
// ===========================================================================

// ===========================================================================
// Vec integer instantiation for SSE
// ===========================================================================

// partial specialization for SIMD_WIDTH = 16
template <typename T>
class Vec<T, 16>
{
  __m128i xmm = _mm_setzero_si128();

public:
  using Type                       = T;
  static constexpr size_t elements = 16 / sizeof(T);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 16;

  Vec() = default;
  Vec(const __m128i &x) { xmm = x; }
  Vec &operator=(const __m128i &x)
  {
    xmm = x;
    return *this;
  }
  operator __m128i() const { return xmm; }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<T, bytes>, bytes>;
};

// ===========================================================================
// Vec float specialization for SSE
// ===========================================================================

template <>
class Vec<Float, 16>
{
  __m128 xmm = _mm_setzero_ps();

public:
  using Type                       = Float;
  static constexpr size_t elements = 16 / sizeof(Float);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 16;

  Vec() = default;
  Vec(const __m128 &x) { xmm = x; }
  Vec &operator=(const __m128 &x)
  {
    xmm = x;
    return *this;
  }
  operator __m128() const { return xmm; }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<Float, bytes>, bytes>;
};

// ===========================================================================
// Vec double specialization for SSE
// ===========================================================================

template <>
class Vec<Double, 16>
{
  __m128d xmm;

public:
  using Type                       = Double;
  static constexpr size_t elements = 16 / sizeof(Double);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 16;
  Vec()                            = default;
  Vec(const __m128d &x) { xmm = x; }
  Vec &operator=(const __m128d &x)
  {
    xmm = x;
    return *this;
  }
  operator __m128d() const { return xmm; }
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  using allocator = simd_aligned_allocator<Vec<Double, bytes>, bytes>;
};

namespace internal {
namespace base {
// ===========================================================================
// Vec function template specialization or overloading for SSE
// ===========================================================================

// ---------------------------------------------------------------------------
// reinterpretation casts
// ---------------------------------------------------------------------------

// 08. Apr 23 (Jonas Keller): used enable_if for cleaner implementation

// between all integer types
template <typename Tdst, typename Tsrc,
          SIMD_ENABLE_IF((!std::is_same<Tdst, Tsrc>::value &&
                          std::is_integral<Tdst>::value &&
                          std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Tdst, 16> reinterpret(const Vec<Tsrc, 16> &vec,
                                             OutputType<Tdst>)
{
  // 26. Nov 22 (Jonas Keller): reinterpret_cast is technically undefined
  // behavior, so just rewrapping the vector register in a new Vec instead
  // return reinterpret_cast<const Vec<Tdst,16>&>(vec);
  return Vec<Tdst, 16>(__m128i(vec));
}

// from float to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 16> reinterpret(const Vec<Float, 16> &vec,
                                             OutputType<Tdst>)
{
  return _mm_castps_si128(vec);
}

// from any integer type to float
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Float, 16> reinterpret(const Vec<Tsrc, 16> &vec,
                                              OutputType<Float>)
{
  return _mm_castsi128_ps(vec);
}

// from double to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 16> reinterpret(const Vec<Double, 16> &vec,
                                             OutputType<Tdst>)
{
  return _mm_castpd_si128(vec);
}

// from any integer type to double
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Double, 16> reinterpret(const Vec<Tsrc, 16> &vec,
                                               OutputType<Double>)
{
  return _mm_castsi128_pd(vec);
}

// from float to double
static SIMD_INLINE Vec<Double, 16> reinterpret(const Vec<Float, 16> &vec,
                                               OutputType<Double>)
{
  return _mm_castps_pd(vec);
}

// from double to float
static SIMD_INLINE Vec<Float, 16> reinterpret(const Vec<Double, 16> &vec,
                                              OutputType<Float>)
{
  return _mm_castpd_ps(vec);
}

// between identical types
template <typename T>
static SIMD_INLINE Vec<T, 16> reinterpret(const Vec<T, 16> &vec, OutputType<T>)
{
  return vec;
}

// ---------------------------------------------------------------------------
// convert (without changes in the number of of elements)
// ---------------------------------------------------------------------------

// conversion with saturation; we wanted to have a fast solution that
// doesn't trigger the overflow which results in a negative two's
// complement result ("invalid int32": 0x80000000); therefore we clamp
// the positive values at the maximal positive float which is
// convertible to int32 without overflow (0x7fffffbf = 2147483520);
// negative values cannot overflow (they are clamped to invalid int
// which is the most negative int32)
static SIMD_INLINE Vec<Int, 16> cvts(const Vec<Float, 16> &a, OutputType<Int>)
{
  // TODO: analyze much more complex solution for cvts at
  // TODO: http://stackoverflow.com/questions/9157373/
  // TODO: most-efficient-way-to-convert-vector-of-float-to-vector-of-uint32
  // NOTE: float->int: rounding is affected by MXCSR rounding control bits!
  __m128 clip = _mm_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT32);
  return _mm_cvtps_epi32(_mm_min_ps(clip, a));
}

// saturation is not necessary in this case
static SIMD_INLINE Vec<Float, 16> cvts(const Vec<Int, 16> &a, OutputType<Float>)
{
  return _mm_cvtepi32_ps(a);
}

static SIMD_INLINE Vec<Long, 16> cvts(const Vec<Double, 16> &a,
                                      OutputType<Long>)
{
  // _mm_cvtpd_epi64 is only available with AVX512
  // workaround from https://stackoverflow.com/a/41148578 only works for
  // values in range [-2^52, 2^52]
  // using serial workaround instead
  // TODO: serial workaround is slow, find parallel workaround
  const auto clip = _mm_set1_pd(MAX_POS_DOUBLE_CONVERTIBLE_TO_INT64);
  Double tmpD[2] SIMD_ATTR_ALIGNED(16);
  _mm_store_pd(tmpD, _mm_min_pd(clip, a));
  Long tmpL[2] SIMD_ATTR_ALIGNED(16);
  tmpL[0] = Long(std::rint(tmpD[0]));
  tmpL[1] = Long(std::rint(tmpD[1]));
  return _mm_load_si128((__m128i *) tmpL);
}

static SIMD_INLINE Vec<Double, 16> cvts(const Vec<Long, 16> &a,
                                        OutputType<Double>)
{
  // workaround from https://stackoverflow.com/a/41148578 (modified)
  __m128i xH = _mm_srai_epi32(a, 16);
  xH         = _mm_and_si128(xH, _mm_set1_epi64x(0xffffffff00000000));
  xH         = _mm_add_epi64(
    xH, _mm_castpd_si128(_mm_set1_pd(442721857769029238784.))); //  3*2^67
#ifdef __SSE4_1__
  __m128i xL = _mm_blend_epi16(
    a, _mm_castpd_si128(_mm_set1_pd(0x0010000000000000)), 0x88); //  2^52
#else
  __m128i xL =
    _mm_or_si128(_mm_and_si128(a, _mm_set1_epi64x(0x0000ffffffffffff)),
                 _mm_castpd_si128(_mm_set1_pd(0x0010000000000000))); //  2^52
#endif
  __m128d f = _mm_sub_pd(_mm_castsi128_pd(xH),
                         _mm_set1_pd(442726361368656609280.)); //  3*2^67 + 2^52
  return _mm_add_pd(f, _mm_castsi128_pd(xL));
}

// ---------------------------------------------------------------------------
// setzero
// ---------------------------------------------------------------------------

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 16> setzero(OutputType<T>, Integer<16>)
{
  return _mm_setzero_si128();
}

static SIMD_INLINE Vec<Float, 16> setzero(OutputType<Float>, Integer<16>)
{
  return _mm_setzero_ps();
}

static SIMD_INLINE Vec<Double, 16> setzero(OutputType<Double>, Integer<16>)
{
  return _mm_setzero_pd();
}

// ---------------------------------------------------------------------------
// set1
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> set1(Byte a, Integer<16>)
{
  return _mm_set1_epi8(a);
}

static SIMD_INLINE Vec<SignedByte, 16> set1(SignedByte a, Integer<16>)
{
  return _mm_set1_epi8(a);
}

static SIMD_INLINE Vec<Word, 16> set1(Word a, Integer<16>)
{
  return _mm_set1_epi16(a);
}

static SIMD_INLINE Vec<Short, 16> set1(Short a, Integer<16>)
{
  return _mm_set1_epi16(a);
}

static SIMD_INLINE Vec<Int, 16> set1(Int a, Integer<16>)
{
  return _mm_set1_epi32(a);
}

static SIMD_INLINE Vec<Long, 16> set1(Long a, Integer<16>)
{
  return _mm_set1_epi64x(a);
}

static SIMD_INLINE Vec<Float, 16> set1(Float a, Integer<16>)
{
  return _mm_set1_ps(a);
}

static SIMD_INLINE Vec<Double, 16> set1(Double a, Integer<16>)
{
  return _mm_set1_pd(a);
}

// ---------------------------------------------------------------------------
// load
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> load(const T *const p, Integer<16>)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  return _mm_load_si128((__m128i *) p);
}

static SIMD_INLINE Vec<Float, 16> load(const Float *const p, Integer<16>)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  return _mm_load_ps(p);
}

static SIMD_INLINE Vec<Double, 16> load(const Double *const p, Integer<16>)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  return _mm_load_pd(p);
}

// ---------------------------------------------------------------------------
// loadu
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> loadu(const T *const p, Integer<16>)
{
  return _mm_loadu_si128((__m128i *) p);
}

static SIMD_INLINE Vec<Float, 16> loadu(const Float *const p, Integer<16>)
{
  return _mm_loadu_ps(p);
}

static SIMD_INLINE Vec<Double, 16> loadu(const Double *const p, Integer<16>)
{
  return _mm_loadu_pd(p);
}

// ---------------------------------------------------------------------------
// store
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void store(T *const p, const Vec<T, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_store_si128((__m128i *) p, a);
}

// float version
static SIMD_INLINE void store(Float *const p, const Vec<Float, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_store_ps(p, a);
}

// double version
static SIMD_INLINE void store(Double *const p, const Vec<Double, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_store_pd(p, a);
}

// ---------------------------------------------------------------------------
// storeu
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void storeu(T *const p, const Vec<T, 16> &a)
{
  _mm_storeu_si128((__m128i *) p, a);
}

// float version
static SIMD_INLINE void storeu(Float *const p, const Vec<Float, 16> &a)
{
  _mm_storeu_ps(p, a);
}

// double version
static SIMD_INLINE void storeu(Double *const p, const Vec<Double, 16> &a)
{
  _mm_storeu_pd(p, a);
}

// ---------------------------------------------------------------------------
// stream_store
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void stream_store(T *const p, const Vec<T, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_stream_si128((__m128i *) p, a);
}

// float version
static SIMD_INLINE void stream_store(Float *const p, const Vec<Float, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_stream_ps(p, a);
}

// double version
static SIMD_INLINE void stream_store(Double *const p, const Vec<Double, 16> &a)
{
  // SSE load and store instructions need alignment to 16 byte
  // (lower 4 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 16);
  _mm_stream_pd(p, a);
}

// ---------------------------------------------------------------------------
// fences (defined only here and not in SIMDVec32.H)
// ---------------------------------------------------------------------------

static SIMD_INLINE void lfence()
{
  _mm_lfence();
}

static SIMD_INLINE void sfence()
{
  _mm_sfence();
}

static SIMD_INLINE void mfence()
{
  _mm_mfence();
}

// ---------------------------------------------------------------------------
// extract: with template parameter for immediate argument
// ---------------------------------------------------------------------------

template <size_t INDEX>
static SIMD_INLINE Byte extract(const Vec<Byte, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si32(a);
  } else SIMD_IF_CONSTEXPR (INDEX < 16) {
#ifdef __SSE4_1__
    return _mm_extract_epi8(a, INDEX);
#else
    SIMD_IF_CONSTEXPR ((INDEX & 0x1) == 0) {
      return _mm_extract_epi16(a, INDEX / 2) & 0xff;
    } else {
      return _mm_extract_epi16(_mm_srli_epi16(a, 8), INDEX / 2);
    }
#endif
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE SignedByte extract(const Vec<SignedByte, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si32(a);
  } else SIMD_IF_CONSTEXPR (INDEX < 16) {
#ifdef __SSE4_1__
    return _mm_extract_epi8(a, INDEX);
#else
    SIMD_IF_CONSTEXPR ((INDEX & 0x1) == 0) {
      return _mm_extract_epi16(a, INDEX / 2) & 0xff;
    } else {
      return _mm_extract_epi16(_mm_srli_epi16(a, 8), INDEX / 2);
    }
#endif
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE Word extract(const Vec<Word, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si32(a);
  } else SIMD_IF_CONSTEXPR (INDEX < 8) {
    return _mm_extract_epi16(a, INDEX);
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE Short extract(const Vec<Short, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si32(a);
  } else SIMD_IF_CONSTEXPR (INDEX < 8) {
    return _mm_extract_epi16(a, INDEX);
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE Int extract(const Vec<Int, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si32(a);
  } else SIMD_IF_CONSTEXPR (INDEX < 4) {
#ifdef __SSE4_1__
    return _mm_extract_epi32(a, INDEX);
#else
    return _mm_cvtsi128_si32(_mm_srli_si128(a, INDEX * 4));
#endif
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE Long extract(const Vec<Long, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return _mm_cvtsi128_si64(a);
  } else SIMD_IF_CONSTEXPR (INDEX == 1) {
    return _mm_cvtsi128_si64(_mm_srli_si128(a, 8));
  } else {
    return 0;
  }
}

template <size_t INDEX>
static SIMD_INLINE Float extract(const Vec<Float, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return ::simd::internal::bit_cast<Float>(
      _mm_cvtsi128_si32(_mm_castps_si128(a)));
  } else SIMD_IF_CONSTEXPR (INDEX < 4) {
#ifdef __SSE4_2__
    const int intRes = _mm_extract_ps(a, INDEX);
#else
    const int intRes =
      _mm_cvtsi128_si32(_mm_srli_si128(_mm_castps_si128(a), INDEX * 4));
#endif
    return ::simd::internal::bit_cast<Float>(intRes);
  } else {
    return 0.0f;
  }
}

template <size_t INDEX>
static SIMD_INLINE Double extract(const Vec<Double, 16> &a)
{
  SIMD_IF_CONSTEXPR (INDEX == 0) {
    return ::simd::internal::bit_cast<Double>(
      _mm_cvtsi128_si64(_mm_castpd_si128(a)));
  } else SIMD_IF_CONSTEXPR (INDEX == 1) {
    return ::simd::internal::bit_cast<Double>(
      _mm_cvtsi128_si64(_mm_srli_si128(_mm_castpd_si128(a), 8)));
  } else {
    return 0.0;
  }
}

// ---------------------------------------------------------------------------
// ifelse
// ---------------------------------------------------------------------------

// elements of cond must be all 1's or all 0's (blendv just tests top
// bit in each byte, but work-around needs this)

template <typename T>
static SIMD_INLINE Vec<T, 16> ifelse(const Vec<T, 16> &cond,
                                     const Vec<T, 16> &trueVal,
                                     const Vec<T, 16> &falseVal)
{
#ifdef __SSE4_1__
  return _mm_blendv_epi8(falseVal, trueVal, cond);
#else
  return _mm_or_si128(_mm_and_si128(cond, trueVal),
                      _mm_andnot_si128(cond, falseVal));
#endif
}

static SIMD_INLINE Vec<Float, 16> ifelse(const Vec<Float, 16> &cond,
                                         const Vec<Float, 16> &trueVal,
                                         const Vec<Float, 16> &falseVal)
{
#ifdef __SSE4_1__
  return _mm_blendv_ps(falseVal, trueVal, cond);
#else
  return _mm_or_ps(_mm_and_ps(cond, trueVal), _mm_andnot_ps(cond, falseVal));
#endif
}

static SIMD_INLINE Vec<Double, 16> ifelse(const Vec<Double, 16> &cond,
                                          const Vec<Double, 16> &trueVal,
                                          const Vec<Double, 16> &falseVal)
{
#ifdef __SSE4_1__
  return _mm_blendv_pd(falseVal, trueVal, cond);
#else
  return _mm_or_pd(_mm_and_pd(cond, trueVal), _mm_andnot_pd(cond, falseVal));
#endif
}

// ---------------------------------------------------------------------------
// add
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> add(const Vec<Byte, 16> &a,
                                     const Vec<Byte, 16> &b)
{
  return _mm_add_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> add(const Vec<SignedByte, 16> &a,
                                           const Vec<SignedByte, 16> &b)
{
  return _mm_add_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> add(const Vec<Word, 16> &a,
                                     const Vec<Word, 16> &b)
{
  return _mm_add_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> add(const Vec<Short, 16> &a,
                                      const Vec<Short, 16> &b)
{
  return _mm_add_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> add(const Vec<Int, 16> &a,
                                    const Vec<Int, 16> &b)
{
  return _mm_add_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 16> add(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  return _mm_add_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 16> add(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_add_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> add(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// adds
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> adds(const Vec<Byte, 16> &a,
                                      const Vec<Byte, 16> &b)
{
  return _mm_adds_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> adds(const Vec<SignedByte, 16> &a,
                                            const Vec<SignedByte, 16> &b)
{
  return _mm_adds_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> adds(const Vec<Word, 16> &a,
                                      const Vec<Word, 16> &b)
{
  return _mm_adds_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 16> adds(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b)
{
  return _mm_adds_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> adds(const Vec<Int, 16> &a,
                                     const Vec<Int, 16> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm_adds_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  const __m128i sum             = _mm_add_epi32(a, b);
  const __m128i opsHaveDiffSign = _mm_xor_si128(a, b);
  const __m128i sumHasDiffSign  = _mm_xor_si128(a, sum);
  // indicates when an overflow has occurred
  const __m128i overflow =
    _mm_srai_epi32(_mm_andnot_si128(opsHaveDiffSign, sumHasDiffSign), 31);
  // saturated sum for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  const __m128i saturatedSum =
    _mm_xor_si128(_mm_srai_epi32(a, 31), _mm_set1_epi32(0x7FFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return ifelse(Vec<Int, 16>(overflow), Vec<Int, 16>(saturatedSum),
                Vec<Int, 16>(sum));
}

static SIMD_INLINE Vec<Long, 16> adds(const Vec<Long, 16> &a,
                                      const Vec<Long, 16> &b)
{
  // _mm_adds_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  __m128i sum             = _mm_add_epi64(a, b);
  __m128i opsHaveDiffSign = _mm_xor_si128(a, b);
  __m128i sumHasDiffSign  = _mm_xor_si128(a, sum);
  // indicates when an overflow has occurred
  __m128i overflow32 =
    _mm_srai_epi32(_mm_andnot_si128(opsHaveDiffSign, sumHasDiffSign), 31);
  __m128i overflow    = _mm_shuffle_epi32(overflow32, _MM_SHUFFLE(3, 3, 1, 1));
  __m128i signMaskA32 = _mm_srai_epi32(a, 31);
  __m128i signMaskA   = _mm_shuffle_epi32(signMaskA32, _MM_SHUFFLE(3, 3, 1, 1));
  // saturated sum for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0,
  // 0x8000000000000000=min negative long, when sign of a (and thus b as well)
  // is 1)
  __m128i saturatedSum =
    _mm_xor_si128(signMaskA, _mm_set1_epi64x(0x7FFFFFFFFFFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return ifelse(Vec<Long, 16>(overflow), Vec<Long, 16>(saturatedSum),
                Vec<Long, 16>(sum));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 16> adds(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b)
{
  return _mm_add_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 16> adds(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return _mm_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// sub
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> sub(const Vec<Byte, 16> &a,
                                     const Vec<Byte, 16> &b)
{
  return _mm_sub_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> sub(const Vec<SignedByte, 16> &a,
                                           const Vec<SignedByte, 16> &b)
{
  return _mm_sub_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> sub(const Vec<Word, 16> &a,
                                     const Vec<Word, 16> &b)
{
  return _mm_sub_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> sub(const Vec<Short, 16> &a,
                                      const Vec<Short, 16> &b)
{
  return _mm_sub_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> sub(const Vec<Int, 16> &a,
                                    const Vec<Int, 16> &b)
{
  return _mm_sub_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 16> sub(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  return _mm_sub_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 16> sub(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_sub_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> sub(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// subs
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> subs(const Vec<Byte, 16> &a,
                                      const Vec<Byte, 16> &b)
{
  return _mm_subs_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> subs(const Vec<SignedByte, 16> &a,
                                            const Vec<SignedByte, 16> &b)
{
  return _mm_subs_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> subs(const Vec<Word, 16> &a,
                                      const Vec<Word, 16> &b)
{
  return _mm_subs_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 16> subs(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b)
{
  return _mm_subs_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> subs(const Vec<Int, 16> &a,
                                     const Vec<Int, 16> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm_subs_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  const __m128i diff            = _mm_sub_epi32(a, b);
  const __m128i opsHaveDiffSign = _mm_xor_si128(a, b);
  const __m128i diffHasDiffSign = _mm_xor_si128(a, diff);
  // indicates when an overflow has occurred
  const __m128i overflow =
    _mm_srai_epi32(_mm_and_si128(opsHaveDiffSign, diffHasDiffSign), 31);
  // saturated diff for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  const __m128i saturatedDiff =
    _mm_xor_si128(_mm_srai_epi32(a, 31), _mm_set1_epi32(0x7FFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return ifelse(Vec<Int, 16>(overflow), Vec<Int, 16>(saturatedDiff),
                Vec<Int, 16>(diff));
}

static SIMD_INLINE Vec<Long, 16> subs(const Vec<Long, 16> &a,
                                      const Vec<Long, 16> &b)
{
  // _mm_subs_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  __m128i diff            = _mm_sub_epi64(a, b);
  __m128i opsHaveDiffSign = _mm_xor_si128(a, b);
  __m128i diffHasDiffSign = _mm_xor_si128(a, diff);
  // indicates when an overflow has occurred
  __m128i overflow32 =
    _mm_srai_epi32(_mm_and_si128(opsHaveDiffSign, diffHasDiffSign), 63);
  __m128i overflow    = _mm_shuffle_epi32(overflow32, _MM_SHUFFLE(3, 3, 1, 1));
  __m128i signMaskA32 = _mm_srai_epi32(a, 63);
  __m128i signMaskA   = _mm_shuffle_epi32(signMaskA32, _MM_SHUFFLE(3, 3, 1, 1));
  // saturated diff for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0,
  // 0x8000000000000000=min negative long, when sign of a (and thus b as well)
  // is 1)
  __m128i saturatedDiff =
    _mm_xor_si128(signMaskA, _mm_set1_epi64x(0x7FFFFFFFFFFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return ifelse(Vec<Long, 16>(overflow), Vec<Long, 16>(saturatedDiff),
                Vec<Long, 16>(diff));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 16> subs(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b)
{
  return _mm_sub_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 16> subs(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return _mm_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// neg (negate = two's complement or unary minus), only signed types
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<SignedByte, 16> neg(const Vec<SignedByte, 16> &a)
{
  return _mm_sub_epi8(_mm_setzero_si128(), a);
}

static SIMD_INLINE Vec<Short, 16> neg(const Vec<Short, 16> &a)
{
  return _mm_sub_epi16(_mm_setzero_si128(), a);
}

static SIMD_INLINE Vec<Int, 16> neg(const Vec<Int, 16> &a)
{
  return _mm_sub_epi32(_mm_setzero_si128(), a);
}

static SIMD_INLINE Vec<Long, 16> neg(const Vec<Long, 16> &a)
{
  return _mm_sub_epi64(_mm_setzero_si128(), a);
}

static SIMD_INLINE Vec<Float, 16> neg(const Vec<Float, 16> &a)
{
  return _mm_sub_ps(_mm_setzero_ps(), a);
}

static SIMD_INLINE Vec<Double, 16> neg(const Vec<Double, 16> &a)
{
  return _mm_xor_pd(a, _mm_set1_pd(-0.0));
}

// ---------------------------------------------------------------------------
// min
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> min(const Vec<Byte, 16> &a,
                                     const Vec<Byte, 16> &b)
{
  return _mm_min_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> min(const Vec<SignedByte, 16> &a,
                                           const Vec<SignedByte, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_min_epi8(a, b);
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i signbit = _mm_set1_epi32(0x80808080);
  const __m128i a1      = _mm_xor_si128(a, signbit); // add 0x80
  const __m128i b1      = _mm_xor_si128(b, signbit); // add 0x80
  const __m128i m1      = _mm_min_epu8(a1, b1);      // unsigned min
  return _mm_xor_si128(m1, signbit);                 // sub 0x80
#endif
}

static SIMD_INLINE Vec<Word, 16> min(const Vec<Word, 16> &a,
                                     const Vec<Word, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_min_epu16(a, b);
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i signbit = _mm_set1_epi32(0x80008000);
  const __m128i a1      = _mm_xor_si128(a, signbit); // add 0x8000
  const __m128i b1      = _mm_xor_si128(b, signbit); // add 0x8000
  const __m128i m1      = _mm_min_epi16(a1, b1);     // signed min
  return _mm_xor_si128(m1, signbit);                 // sub 0x8000
#endif
}

static SIMD_INLINE Vec<Short, 16> min(const Vec<Short, 16> &a,
                                      const Vec<Short, 16> &b)
{
  return _mm_min_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> min(const Vec<Int, 16> &a,
                                    const Vec<Int, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_min_epi32(a, b);
#else
  // from Agner Fog's VCL vectori128.h (modified)
  const __m128i gt = _mm_cmpgt_epi32(a, b);
  return _mm_or_si128(_mm_and_si128(gt, b), _mm_andnot_si128(gt, a));
#endif
}

// there is an unsigned version of min for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 16> min(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // _mm_min_epi64 does not exist (not even in SSE4.1)

  // compute a > b into gt
#ifdef __SSE4_2__
  const __m128i gt = _mm_cmpgt_epi64(a, b);
#else
  // from Hacker's Delight, 2-12 Comparison Predicates: (swapped lt)
  const __m128i diff = _mm_sub_epi64(b, a);
#if 1 // TODO: check which is faster
  const __m128i res  = _mm_xor_si128(
    diff, _mm_and_si128(_mm_xor_si128(b, a), _mm_xor_si128(diff, b)));
#else
  const __m128i res = _mm_or_si128(_mm_andnot_si128(a, b),
                                   _mm_andnot_si128(_mm_xor_si128(b, a), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  const __m128i gt       = _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif

  // blend a and b according to gt
#ifdef __SSE4_1__
  return _mm_blendv_epi8(a, b, gt);
#else
  return _mm_or_si128(_mm_and_si128(gt, b), _mm_andnot_si128(gt, a));
#endif
}

static SIMD_INLINE Vec<Float, 16> min(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_min_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> min(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_min_pd(a, b);
}

// ---------------------------------------------------------------------------
// max
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> max(const Vec<Byte, 16> &a,
                                     const Vec<Byte, 16> &b)
{
  return _mm_max_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> max(const Vec<SignedByte, 16> &a,
                                           const Vec<SignedByte, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_max_epi8(a, b);
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i signbit = _mm_set1_epi32(0x80808080);
  const __m128i a1      = _mm_xor_si128(a, signbit); // add 0x80
  const __m128i b1      = _mm_xor_si128(b, signbit); // add 0x80
  const __m128i m1      = _mm_max_epu8(a1, b1);      // unsigned max
  return _mm_xor_si128(m1, signbit);                 // sub 0x80
#endif
}

static SIMD_INLINE Vec<Word, 16> max(const Vec<Word, 16> &a,
                                     const Vec<Word, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_max_epu16(a, b);
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i signbit = _mm_set1_epi32(0x80008000);
  const __m128i a1      = _mm_xor_si128(a, signbit); // add 0x8000
  const __m128i b1      = _mm_xor_si128(b, signbit); // add 0x8000
  const __m128i m1      = _mm_max_epi16(a1, b1);     // signed max
  return _mm_xor_si128(m1, signbit);                 // sub 0x8000
#endif
}

static SIMD_INLINE Vec<Short, 16> max(const Vec<Short, 16> &a,
                                      const Vec<Short, 16> &b)
{
  return _mm_max_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> max(const Vec<Int, 16> &a,
                                    const Vec<Int, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_max_epi32(a, b);
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i gt = _mm_cmpgt_epi32(a, b);
  return _mm_or_si128(_mm_and_si128(gt, a), _mm_andnot_si128(gt, b));
#endif
}

// there is an unsigned version of max for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 16> max(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // _mm_max_epi64 does not exist (not even in SSE4.1)

  // compute a > b into gt
#ifdef __SSE4_2__
  const __m128i gt = _mm_cmpgt_epi64(a, b);
#else
  // from Hacker's Delight, 2-12 Comparison Predicates: (swapped lt)
  const __m128i diff = _mm_sub_epi64(b, a);
#if 1 // TODO: check which is faster
  const __m128i res  = _mm_xor_si128(
    diff, _mm_and_si128(_mm_xor_si128(b, a), _mm_xor_si128(diff, b)));
#else
  const __m128i res = _mm_or_si128(_mm_andnot_si128(a, b),
                                   _mm_andnot_si128(_mm_xor_si128(b, a), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  const __m128i gt       = _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif

  // blend a and b according to gt
#ifdef __SSE4_1__
  return _mm_blendv_epi8(b, a, gt);
#else
  return _mm_or_si128(_mm_and_si128(gt, a), _mm_andnot_si128(gt, b));
#endif
}

static SIMD_INLINE Vec<Float, 16> max(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_max_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> max(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_max_pd(a, b);
}

// ---------------------------------------------------------------------------
// mul, div
// ---------------------------------------------------------------------------

// TODO: add mul/div versions for int types? or make special versions of mul
// TODO: and div where the result is scaled?

static SIMD_INLINE Vec<Float, 16> mul(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_mul_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> mul(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_mul_pd(a, b);
}

static SIMD_INLINE Vec<Float, 16> div(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return _mm_div_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> div(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return _mm_div_pd(a, b);
}

// ---------------------------------------------------------------------------
// ceil, floor, round, truncate
// ---------------------------------------------------------------------------

// 25. Mar 23 (Jonas Keller): added versions for integer types

// TODO: import complex workaround for non-SSE4.1 from Agner Fog's VCL?

// NOTE: behavior for workarounds differs for results of -0.0f and +0.0f

// work-arounds for round, truncate, floor, and ceil all check whether
// rounding is necessary (or whether float is an integer anyhow), this also
// prevents range excess when converting numbers to integer

// workarounds for floor and ceil:
// https://en.wikipedia.org/wiki/Floor_and_ceiling_functions
//
// floor, ceil:
//                 floor(x), x >= 0
// truncate(x) = {
//                 ceil(x), x < 0
//
// floor(x) = ceil(x)  - (x in Z ? 0 : 1)
// ceil(x)  = floor(x) + (x in Z ? 0 : 1)

// versions for integer types do nothing:

template <typename T>
static SIMD_INLINE Vec<T, 16> ceil(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> floor(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> round(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> truncate(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

static SIMD_INLINE Vec<Float, 16> ceil(const Vec<Float, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_ceil_ps(a);
#else
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  const __m128 limit = _mm_set1_ps(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  const __m128 absA =
    _mm_and_ps(a, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));
  const __m128 noRndReq = _mm_cmpge_ps(absA, limit);
  // bool mask: true if a is negative
  const __m128 isNeg =
    _mm_castsi128_ps(_mm_srai_epi32(_mm_castps_si128(a), 31));
  // truncated result (for |a| < limit)
  __m128 aTrunc = _mm_cvtepi32_ps(_mm_cvttps_epi32(a));
  // check if a is an integer
  const __m128 isNotInt = _mm_cmpneq_ps(a, aTrunc);
  // constant 1.0
  const __m128 one = _mm_set1_ps(1.0f);
  // mask which is 1.0f for non-negative non-integer values, 0.0f otherwise
  const __m128 oneMask = _mm_and_ps(_mm_andnot_ps(isNeg, isNotInt), one);
  // if non-negative, trunc computes floor, to turn it into ceil we
  // add 1 if aTrunc is non-integer
  aTrunc = _mm_add_ps(aTrunc, oneMask);
  // select result (a or aTrunc)
  return ifelse(noRndReq, a, aTrunc);
#endif
}

static SIMD_INLINE Vec<Double, 16> ceil(const Vec<Double, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_ceil_pd(a);
#else
  // There is no _mm_cvtepi64_pd in SSE*, which makes the workaround
  // used for the float version not possible here.
  // Another workaround would probably be complicated and slow, so we just
  // ceil serially.
  // TODO: is there a better, vectorized workaround?
  Double inArr[2] SIMD_ATTR_ALIGNED(16);
  _mm_store_pd(inArr, a);
  Double outArr[2] SIMD_ATTR_ALIGNED(16);
  outArr[0] = std::ceil(inArr[0]);
  outArr[1] = std::ceil(inArr[1]);
  return _mm_load_pd(outArr);
#endif
}

static SIMD_INLINE Vec<Float, 16> floor(const Vec<Float, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_floor_ps(a);
#else
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  const __m128 limit = _mm_set1_ps(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  const __m128 absA =
    _mm_and_ps(a, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));
  const __m128 noRndReq = _mm_cmpge_ps(absA, limit);
  // bool mask: true if a is negative
  const __m128 isNeg =
    _mm_castsi128_ps(_mm_srai_epi32(_mm_castps_si128(a), 31));
  // truncated result (for |a| < limit)
  __m128 aTrunc = _mm_cvtepi32_ps(_mm_cvttps_epi32(a));
  // check if a is an integer
  const __m128 isNotInt = _mm_cmpneq_ps(a, aTrunc);
  // constant 1.0
  const __m128 one = _mm_set1_ps(1.0f);
  // mask which is 1.0f for negative non-integer values, 0.0f otherwise
  const __m128 oneMask = _mm_and_ps(_mm_and_ps(isNeg, isNotInt), one);
  // if negative, trunc computes ceil, to turn it into floor we sub
  // 1 if aTrunc is non-integer
  aTrunc = _mm_sub_ps(aTrunc, oneMask);
  // select result (a or aTrunc)
  return ifelse(noRndReq, a, aTrunc);
#endif
}

static SIMD_INLINE Vec<Double, 16> floor(const Vec<Double, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_floor_pd(a);
#else
  // There is no _mm_cvtepi64_pd in SSE*, which makes the workaround
  // used for the float version not possible here.
  // Another workaround would probably be complicated and slow, so we just
  // floor serially.
  // TODO: is there a better, vectorized workaround?
  Double inArr[2] SIMD_ATTR_ALIGNED(16);
  _mm_store_pd(inArr, a);
  Double outArr[2] SIMD_ATTR_ALIGNED(16);
  outArr[0] = std::floor(inArr[0]);
  outArr[1] = std::floor(inArr[1]);
  return _mm_load_pd(outArr);
#endif
}

static SIMD_INLINE Vec<Float, 16> round(const Vec<Float, 16> &a)
{
#ifdef __SSE4_1__
  // old: use _MM_SET_ROUNDING_MODE to adjust rounding direction
  // return _mm_round_ps(a, _MM_FROUND_CUR_DIRECTION);
  // new  4. Aug 16 (rm): round to nearest, and suppress exceptions
  return _mm_round_ps(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
#else
  // NOTE: only works if rounding mode is default (rnd. to nearest (even))
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  const __m128 limit = _mm_set1_ps(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  const __m128 absA =
    _mm_and_ps(a, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));
  const __m128 noRndReq = _mm_cmpge_ps(absA, limit);
  // rounded result (here rounded according to current rounding mode)
  // (for |a| < limit)
  const __m128 aRnd = _mm_cvtepi32_ps(_mm_cvtps_epi32(a));
  // select result
  return ifelse(noRndReq, a, aRnd);
#endif
}

static SIMD_INLINE Vec<Double, 16> round(const Vec<Double, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_round_pd(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
#else
  // There is no _mm_cvtepi64_pd in SSE*, which makes the workaround
  // used for the float version not possible here.
  // Another workaround would probably be complicated and slow, so we just
  // round serially.
  // TODO: is there a better, vectorized workaround?
  Double inArr[2] SIMD_ATTR_ALIGNED(16);
  _mm_store_pd(inArr, a);
  Double outArr[2] SIMD_ATTR_ALIGNED(16);
  // std::round has different behavior
  outArr[0] = std::rint(inArr[0]);
  outArr[1] = std::rint(inArr[1]);
  return _mm_load_pd(outArr);
#endif
}

static SIMD_INLINE Vec<Float, 16> truncate(const Vec<Float, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_round_ps(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
#else
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  const __m128 limit = _mm_set1_ps(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  const __m128 absA =
    _mm_and_ps(a, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));
  const __m128 noRndReq = _mm_cmpge_ps(absA, limit);
  // truncated result (for |a| < limit) (cvtTps!)
  const __m128 aTrunc = _mm_cvtepi32_ps(_mm_cvttps_epi32(a));
  // select result
  return ifelse(noRndReq, a, aTrunc);
#endif
}

static SIMD_INLINE Vec<Double, 16> truncate(const Vec<Double, 16> &a)
{
#ifdef __SSE4_1__
  return _mm_round_pd(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
#else
  // There is no _mm_cvtepi64_pd in SSE*, which makes the workaround
  // used for the float version not possible here.
  // Another workaround would probably be complicated and slow, so we just
  // truncate serially.
  // TODO: is there a better, vectorized workaround?
  Double inArr[2] SIMD_ATTR_ALIGNED(16);
  _mm_store_pd(inArr, a);
  Double outArr[2] SIMD_ATTR_ALIGNED(16);
  outArr[0] = std::trunc(inArr[0]);
  outArr[1] = std::trunc(inArr[1]);
  return _mm_load_pd(outArr);
#endif
}

// ---------------------------------------------------------------------------
// elementary mathematical functions
// ---------------------------------------------------------------------------

// estimate of a reciprocal
static SIMD_INLINE Vec<Float, 16> rcp(const Vec<Float, 16> &a)
{
  return _mm_rcp_ps(a);
}

// estimate of a reciprocal
static SIMD_INLINE Vec<Double, 16> rcp(const Vec<Double, 16> &a)
{
  // _mm_rcp_pd does not exist, use _mm_div_pd instead
  return _mm_div_pd(_mm_set1_pd(1.0), a);
}

// estimate of a reverse square root
static SIMD_INLINE Vec<Float, 16> rsqrt(const Vec<Float, 16> &a)
{
  return _mm_rsqrt_ps(a);
}

// estimate of a reverse square root
static SIMD_INLINE Vec<Double, 16> rsqrt(const Vec<Double, 16> &a)
{
  // _mm_rsqrt_pd does not exist, use _mm_div_pd and _mm_sqrt_pd instead
  return _mm_div_pd(_mm_set1_pd(1.0), _mm_sqrt_pd(a));
}

// square root
static SIMD_INLINE Vec<Float, 16> sqrt(const Vec<Float, 16> &a)
{
  return _mm_sqrt_ps(a);
}

// square root
static SIMD_INLINE Vec<Double, 16> sqrt(const Vec<Double, 16> &a)
{
  return _mm_sqrt_pd(a);
}

// ---------------------------------------------------------------------------
// abs
// ---------------------------------------------------------------------------

// 25. Mar 25 (Jonas Keller): added abs for unsigned integers

// unsigned integers
template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 16> abs(const Vec<T, 16> &a)
{
  return a;
}

static SIMD_INLINE Vec<SignedByte, 16> abs(const Vec<SignedByte, 16> &a)
{
  return _mm_abs_epi8(a);
}

static SIMD_INLINE Vec<Short, 16> abs(const Vec<Short, 16> &a)
{
  return _mm_abs_epi16(a);
}

static SIMD_INLINE Vec<Int, 16> abs(const Vec<Int, 16> &a)
{
  return _mm_abs_epi32(a);
}

static SIMD_INLINE Vec<Long, 16> abs(const Vec<Long, 16> &a)
{
  // _mm_abs_epi64 is only supported in avx512
  // from Hacker's Delight, 2-4 Absolute Value Function:
  const __m128i signMask =
    _mm_shuffle_epi32(_mm_srai_epi32(a, 31), _MM_SHUFFLE(3, 3, 1, 1));
  return _mm_sub_epi64(_mm_xor_si128(a, signMask), signMask);
}

static SIMD_INLINE Vec<Float, 16> abs(const Vec<Float, 16> &a)
{
  return _mm_and_ps(a, _mm_castsi128_ps(_mm_set1_epi32(0x7FFFFFFF)));
}

static SIMD_INLINE Vec<Double, 16> abs(const Vec<Double, 16> &a)
{
  return _mm_and_pd(a, _mm_castsi128_pd(_mm_set1_epi64x(0x7FFFFFFFFFFFFFFF)));
}

// ---------------------------------------------------------------------------
// unpacklo
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<0>, Bytes<1>)
{
  return _mm_unpacklo_epi8(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<0>, Bytes<2>)
{
  return _mm_unpacklo_epi16(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<0>, Bytes<4>)
{
  return _mm_unpacklo_epi32(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<0>, Bytes<8>)
{
  return _mm_unpacklo_epi64(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> unpack(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b, Part<0>,
                                         Bytes<4>)
{
  return _mm_unpacklo_ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> unpack(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b, Part<0>,
                                         Bytes<8>)
{
  // this moves two lower floats from a and b
  return _mm_movelh_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 16> unpack(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b, Part<0>,
                                          Bytes<8>)
{
  return _mm_unpacklo_pd(a, b);
}

// ---------------------------------------------------------------------------
// unpackhi
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<1>, Bytes<1>)
{
  return _mm_unpackhi_epi8(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<1>, Bytes<2>)
{
  return _mm_unpackhi_epi16(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<1>, Bytes<4>)
{
  return _mm_unpackhi_epi32(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> unpack(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                     Part<1>, Bytes<8>)
{
  return _mm_unpackhi_epi64(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> unpack(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b, Part<1>,
                                         Bytes<4>)
{
  return _mm_unpackhi_ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> unpack(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b, Part<1>,
                                         Bytes<8>)
{
  // this moves two upper floats from a and b
  // order b, a
  return _mm_movehl_ps(b, a);
}

// double version
static SIMD_INLINE Vec<Double, 16> unpack(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b, Part<1>,
                                          Bytes<8>)
{
  return _mm_unpackhi_pd(a, b);
}

// contributed by Adam Marschall

// 16-byte-lane oriented unpack: for 16 bytes same as generalized unpack
// unpack blocks of NUM_ELEMS elements of type T
// PART=0: low half of input vectors,
// PART=1: high half of input vectors
template <size_t PART, size_t BYTES, typename T>
static SIMD_INLINE Vec<T, 16> unpack16(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                       Part<PART>, Bytes<BYTES>)
{
  return unpack(a, b, Part<PART>(), Bytes<BYTES>());
}

// ---------------------------------------------------------------------------
// extract 128-bit lane as Vec<T, 16>, does nothing for 16 bytes
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

template <size_t LANE_INDEX, typename T>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, 16> &a)
{
  return a;
}

// ---------------------------------------------------------------------------
// zip (two unpacks similar to ARM NEON vzip, but for different NUM_ELEMS)
// ---------------------------------------------------------------------------

// a, b are passed by-value to avoid problems with identical input/output args.

// here we can directly map zip to unpack<PART,NUM_ELEMS,T>
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip(const Vec<T, 16> a, const Vec<T, 16> b,
                            Vec<T, 16> &l, Vec<T, 16> &h)
{
  l = unpack(a, b, Part<0>(), Bytes<NUM_ELEMS * sizeof(T)>());
  h = unpack(a, b, Part<1>(), Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// zip16 hub  (16-byte-lane oriented zip): for 16 bytes same as zip
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// a, b are passed by-value to avoid problems with identical
// input/output args.

template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip16(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h)
{
  zip<NUM_ELEMS, T>(a, b, l, h);
}

// ---------------------------------------------------------------------------
// unzip (similar to ARM NEON vuzp, but for different NUM_ELEMS)
// ---------------------------------------------------------------------------

// solutions by Peter Cordes and Starvin Marvin:
// stackoverflow.com/q/45376193/3852630 and
// stackoverflow.com/a/45385216/3852630 and
// stackoverflow.com/q/20504618/3852630

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h, Bytes<1>)
{
  // mask is hopefully only set once if unzip is used multiple times
  const __m128i mask =
    _mm_set_epi8(15, 13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m128i atmp = _mm_shuffle_epi8(a, mask);
  const __m128i btmp = _mm_shuffle_epi8(b, mask);
  l                  = _mm_unpacklo_epi64(atmp, btmp);
  h                  = _mm_unpackhi_epi64(atmp, btmp);
}

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h, Bytes<2>)
{
  // mask is hopefully only set once if unzip is used multiple times
  const __m128i mask =
    _mm_set_epi8(15, 14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5, 4, 1, 0);
  const __m128i atmp = _mm_shuffle_epi8(a, mask);
  const __m128i btmp = _mm_shuffle_epi8(b, mask);
  l                  = _mm_unpacklo_epi64(atmp, btmp);
  h                  = _mm_unpackhi_epi64(atmp, btmp);
}

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h, Bytes<4>)
{
  const __m128 aps = _mm_castsi128_ps(a);
  const __m128 bps = _mm_castsi128_ps(b);
  l = _mm_castps_si128(_mm_shuffle_ps(aps, bps, _MM_SHUFFLE(2, 0, 2, 0)));
  h = _mm_castps_si128(_mm_shuffle_ps(aps, bps, _MM_SHUFFLE(3, 1, 3, 1)));
}

// all types
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h, Bytes<8>)
{
  l = unpack(a, b, Part<0>(), Bytes<8>());
  h = unpack(a, b, Part<1>(), Bytes<8>());
}

// Float
static SIMD_INLINE void unzip(const Vec<Float, 16> a, const Vec<Float, 16> b,
                              Vec<Float, 16> &l, Vec<Float, 16> &h, Bytes<4>)
{
  l = _mm_shuffle_ps(a, b, _MM_SHUFFLE(2, 0, 2, 0));
  h = _mm_shuffle_ps(a, b, _MM_SHUFFLE(3, 1, 3, 1));
}

// ---------------------------------------------------------------------------
// packs
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE Vec<SignedByte, 16> packs(const Vec<Short, 16> &a,
                                             const Vec<Short, 16> &b,
                                             OutputType<SignedByte>)
{
  return _mm_packs_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> packs(const Vec<Int, 16> &a,
                                        const Vec<Int, 16> &b,
                                        OutputType<Short>)
{
  return _mm_packs_epi32(a, b);
}

static SIMD_INLINE Vec<Short, 16> packs(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b,
                                        OutputType<Short>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Short>());
}

static SIMD_INLINE Vec<Float, 16> packs(const Vec<Long, 16> &a,
                                        const Vec<Long, 16> &b,
                                        OutputType<Float>)
{
  // _mm_cvtepi64_ps does not exist
  return _mm_shuffle_ps(_mm_cvtpd_ps(cvts(a, OutputType<Double>())),
                        _mm_cvtpd_ps(cvts(b, OutputType<Double>())),
                        _MM_SHUFFLE(1, 0, 1, 0));
}

static SIMD_INLINE Vec<Float, 16> packs(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b,
                                        OutputType<Float>)
{
  return _mm_shuffle_ps(_mm_cvtpd_ps(a), _mm_cvtpd_ps(b),
                        _MM_SHUFFLE(1, 0, 1, 0));
}

// Long to Int

static SIMD_INLINE Vec<Int, 16> packs(const Vec<Long, 16> &a,
                                      const Vec<Long, 16> &b, OutputType<Int>)
{
  // _mm_packs_epi64 does not exist
  // vectorized workaround seems to be complicated, so just using serial
  // workaround
  // TODO: is there a better, vectorized workaround?

  Long input[4] SIMD_ATTR_ALIGNED(16);
  _mm_store_si128((__m128i *) input, a);
  _mm_store_si128((__m128i *) (input + 2), b);
  Int output[4] SIMD_ATTR_ALIGNED(16);
  for (int i = 0; i < 4; ++i) {
    output[i] =
      (Int) std::min(std::max(input[i], (Long) std::numeric_limits<Int>::min()),
                     (Long) std::numeric_limits<Int>::max());
  }
  return _mm_load_si128((__m128i *) output);
}

// Double to Int

static SIMD_INLINE Vec<Int, 16> packs(const Vec<Double, 16> &a,
                                      const Vec<Double, 16> &b, OutputType<Int>)
{
  const __m128d clip = _mm_set1_pd(std::numeric_limits<Int>::max());
  const __m128 bI    = _mm_castsi128_ps(_mm_cvtpd_epi32(_mm_min_pd(clip, b)));
  const __m128 aI    = _mm_castsi128_ps(_mm_cvtpd_epi32(_mm_min_pd(clip, a)));
  return _mm_castps_si128(_mm_shuffle_ps(aI, bI, _MM_SHUFFLE(1, 0, 1, 0)));
}

// unsigned -> unsigned

static SIMD_INLINE Vec<Byte, 16> packs(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b, OutputType<Byte>)
{
  // _mm_packus_epu16 does not exist, so saturate inputs to byte range and then
  // use _mm_packus_epi16
  return _mm_packus_epi16(min(a, Vec<Word, 16>(_mm_set1_epi16(0xff))),
                          min(b, Vec<Word, 16>(_mm_set1_epi16(0xff))));
}

// signed -> unsigned

static SIMD_INLINE Vec<Byte, 16> packs(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b,
                                       OutputType<Byte>)
{
  return _mm_packus_epi16(a, b);
}

static SIMD_INLINE Vec<Word, 16> packs(const Vec<Int, 16> &a,
                                       const Vec<Int, 16> &b, OutputType<Word>)
{
#ifdef __SSE4_1__
  return _mm_packus_epi32(a, b);
#else
  // mask for lower 16 bit
  const __m128i mask = _mm_set1_epi32(0x0000ffff);
  // a >= 0 ? asat = a : asat = 0
  // 23. Nov 17 (rm): 32->31
  __m128i asat = _mm_andnot_si128(_mm_srai_epi32(a, 31), a);
  // cmp/or is used to restrict number to 16 bit
  // srai/slli is used for sign extension of 16 bit number,
  // makes signed saturation (in packs) a no-op, see
  // http://stackoverflow.com/questions/12118910/
  // converting-float-vector-to-16-bit-int-without-saturating
  // e.g.
  // a = 0xffffffff (-1)  -> asat  = 0x00000000
  //                      -> cmpgt = 0x00000000
  //                      -> slli  = 0x00000000
  //                      -> or    = 0x00000000
  //                      -> srai  = 0x00000000
  //                      -> packs = 0x0000
  // a = 0x7fffffff (>=0) -> asat  = 0x7fffffff
  //                      -> cmpgt = 0xffffffff
  //                      -> slli  = 0xffff0000
  //                      -> or    = 0xffffffff
  //                      -> srai  = 0xffffffff
  //                      -> packs = 0xffff
  // a = 0x0000ffff (>=0) -> asat  = 0x0000ffff
  //                      -> cmpgt = 0x00000000
  //                      -> slli  = 0xffff0000
  //                      -> or    = 0xffff0000
  //                      -> srai  = 0xffffffff
  //                      -> packs = 0xffff
  // a = 0x0000fffe (>=0) -> asat  = 0x0000fffe
  //                      -> cmpgt = 0x00000000
  //                      -> slli  = 0xfffe0000
  //                      -> or    = 0xfffe0000
  //                      -> srai  = 0xfffffffe
  //                      -> packs = 0xfffe
  // a = 0x00007fff (>=0) -> asat  = 0x00007fff
  //                      -> cmpgt = 0x00000000
  //                      -> slli  = 0x7fff0000
  //                      -> or    = 0x7fff0000
  //                      -> srai  = 0x00007fff
  //                      -> packs = 0x7fff
  asat = _mm_srai_epi32(
    _mm_or_si128(_mm_slli_epi32(asat, 16), _mm_cmpgt_epi32(asat, mask)), 16);
  // same for b
  // 23. Nov 17 (rm): 32->31
  __m128i bsat = _mm_andnot_si128(_mm_srai_epi32(b, 31), b);
  bsat         = _mm_srai_epi32(
    _mm_or_si128(_mm_slli_epi32(bsat, 16), _mm_cmpgt_epi32(bsat, mask)), 16);
  return _mm_packs_epi32(asat, bsat);
#endif
}

static SIMD_INLINE Vec<Word, 16> packs(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b,
                                       OutputType<Word>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Word>());
}

// unsigned -> signed

static SIMD_INLINE Vec<SignedByte, 16> packs(const Vec<Word, 16> &a,
                                             const Vec<Word, 16> &b,
                                             OutputType<SignedByte>)
{
  // _mm_packs_epu16 does not exist, so saturate inputs to signed byte range and
  // then use _mm_packs_epi16
  return _mm_packs_epi16(min(a, Vec<Word, 16>(_mm_set1_epi16(0x7f))),
                         min(b, Vec<Word, 16>(_mm_set1_epi16(0x7f))));
}

// ---------------------------------------------------------------------------
// generalized extend: no stage
// ---------------------------------------------------------------------------

// combinations:
// - signed   -> extended signed (sign extension)
// - unsigned -> extended unsigned (zero extension)
// - unsigned -> extended signed (zero extension)
// - signed   -> extended unsigned (saturation and zero extension)

// same types
template <typename T>
static SIMD_INLINE void extend(const Vec<T, 16> &vIn, Vec<T, 16> vOut[1])
{
  vOut[0] = vIn;
}

// same size, different types

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Byte, 16> vOut[1])
{
  vOut[0] = max(vIn, _mm_setzero_si128());
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn,
                               Vec<SignedByte, 16> vOut[1])
{
  vOut[0] = min(vIn, _mm_set1_epi8(0x7f));
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Word, 16> vOut[1])
{
  vOut[0] = _mm_max_epi16(vIn, _mm_setzero_si128());
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Short, 16> vOut[1])
{
  vOut[0] = min(vIn, _mm_set1_epi16(0x7fff));
}

// ---------------------------------------------------------------------------
// generalized extend: single stage
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Short, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi8_epi16(vIn);
  vOut[1] = _mm_cvtepi8_epi16(_mm_srli_si128(vIn, 8));
#else
  vOut[0] = _mm_srai_epi16(_mm_unpacklo_epi8(_mm_undefined_si128(), vIn), 8);
  vOut[1] = _mm_srai_epi16(_mm_unpackhi_epi8(_mm_undefined_si128(), vIn), 8);
#endif
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Int, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi16_epi32(vIn);
  vOut[1] = _mm_cvtepi16_epi32(_mm_srli_si128(vIn, 8));
#else
  vOut[0] = _mm_srai_epi32(_mm_unpacklo_epi16(_mm_undefined_si128(), vIn), 16);
  vOut[1] = _mm_srai_epi32(_mm_unpackhi_epi16(_mm_undefined_si128(), vIn), 16);
#endif
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn,
                               Vec<Float, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_ps(_mm_cvtepi16_epi32(vIn));
  vOut[1] = _mm_cvtepi32_ps(_mm_cvtepi16_epi32(_mm_srli_si128(vIn, 8)));
#else
  vOut[0] = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpacklo_epi16(vIn, vIn), 16));
  vOut[1] = _mm_cvtepi32_ps(_mm_srai_epi32(_mm_unpackhi_epi16(vIn, vIn), 16));
#endif
}

// unsigned -> unsigned

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Word, 16> vOut[2])
{
  // there's no _mm_cvtepu8_epu16()
  vOut[0] = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  vOut[1] = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Short, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepu8_epi16(vIn);
  vOut[1] = _mm_cvtepu8_epi16(_mm_srli_si128(vIn, 8));
#else
  vOut[0] = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  vOut[1] = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
#endif
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Int, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepu16_epi32(vIn);
  vOut[1] = _mm_cvtepu16_epi32(_mm_srli_si128(vIn, 8));
#else
  vOut[0] = _mm_unpacklo_epi16(vIn, _mm_setzero_si128());
  vOut[1] = _mm_unpackhi_epi16(vIn, _mm_setzero_si128());
#endif
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Float, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_ps(_mm_cvtepu16_epi32(vIn));
  vOut[1] = _mm_cvtepi32_ps(_mm_cvtepu16_epi32(_mm_srli_si128(vIn, 8)));
#else
  vOut[0] = _mm_cvtepi32_ps(_mm_unpacklo_epi16(vIn, _mm_setzero_si128()));
  vOut[1] = _mm_cvtepi32_ps(_mm_unpackhi_epi16(vIn, _mm_setzero_si128()));
#endif
}

static SIMD_INLINE void extend(const Vec<Int, 16> &vIn, Vec<Long, 16> vOut[2])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_epi64(vIn);
  vOut[1] = _mm_cvtepi32_epi64(_mm_srli_si128(vIn, 8));
#else
  const __m128i sign = _mm_srai_epi32(vIn, 31);
  vOut[0]            = _mm_unpacklo_epi32(vIn, sign);
  vOut[1]            = _mm_unpackhi_epi32(vIn, sign);
#endif
}

static SIMD_INLINE void extend(const Vec<Int, 16> &vIn, Vec<Double, 16> vOut[2])
{
  vOut[0] = _mm_cvtepi32_pd(vIn);
  vOut[1] = _mm_cvtepi32_pd(_mm_srli_si128(vIn, 8));
}

static SIMD_INLINE void extend(const Vec<Float, 16> &vIn, Vec<Long, 16> vOut[2])
{
  const auto clipped =
    _mm_min_ps(vIn, _mm_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT64));
  vOut[0] = cvts(_mm_cvtps_pd(clipped), OutputType<Long>());
  vOut[1] = cvts(_mm_cvtps_pd(_mm_castsi128_ps(
                   _mm_srli_si128(_mm_castps_si128(clipped), 8))),
                 OutputType<Long>());
}

static SIMD_INLINE void extend(const Vec<Float, 16> &vIn,
                               Vec<Double, 16> vOut[2])
{
  vOut[0] = _mm_cvtps_pd(vIn);
  vOut[1] =
    _mm_cvtps_pd(_mm_castsi128_ps(_mm_srli_si128(_mm_castps_si128(vIn), 8)));
}

// signed -> unsigned

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Word, 16> vOut[2])
{
  // bring input in positive range
#ifdef __SSE4_1__
  const __m128i vInPos = _mm_max_epi8(vIn, _mm_setzero_si128());
#else
  // from Agner Fog's VCL vectori128.h
  const __m128i signbit  = _mm_set1_epi32(0x80808080);
  const __m128i a1       = _mm_xor_si128(vIn, signbit); // add 0x80
  const __m128i m1       = _mm_max_epu8(a1, signbit);   // unsigned max
  const __m128i vInPos   = _mm_xor_si128(m1, signbit);  // sub 0x80
#endif
  vOut[0] = _mm_unpacklo_epi8(vInPos, _mm_setzero_si128());
  vOut[1] = _mm_unpackhi_epi8(vInPos, _mm_setzero_si128());
}

// ---------------------------------------------------------------------------
// generalized extend: two stages
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Int, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi8_epi32(vIn);
  vOut[1] = _mm_cvtepi8_epi32(_mm_srli_si128(vIn, 4));
  vOut[2] = _mm_cvtepi8_epi32(_mm_srli_si128(vIn, 8));
  vOut[3] = _mm_cvtepi8_epi32(_mm_srli_si128(vIn, 12));
#else
  const __m128i lo8      = _mm_unpacklo_epi8(_mm_undefined_si128(), vIn);
  const __m128i hi8      = _mm_unpackhi_epi8(_mm_undefined_si128(), vIn);
  const __m128i lolo16   = _mm_unpacklo_epi16(_mm_undefined_si128(), lo8);
  const __m128i lohi16   = _mm_unpackhi_epi16(_mm_undefined_si128(), lo8);
  const __m128i hilo16   = _mm_unpacklo_epi16(_mm_undefined_si128(), hi8);
  const __m128i hihi16   = _mm_unpackhi_epi16(_mm_undefined_si128(), hi8);
  vOut[0]                = _mm_srai_epi32(lolo16, 24);
  vOut[1]                = _mm_srai_epi32(lohi16, 24);
  vOut[2]                = _mm_srai_epi32(hilo16, 24);
  vOut[3]                = _mm_srai_epi32(hihi16, 24);
#endif
}

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Float, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_ps(_mm_cvtepi8_epi32(vIn));
  vOut[1] = _mm_cvtepi32_ps(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 4)));
  vOut[2] = _mm_cvtepi32_ps(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 8)));
  vOut[3] = _mm_cvtepi32_ps(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 12)));
#else
  const __m128i lo8      = _mm_unpacklo_epi8(_mm_undefined_si128(), vIn);
  const __m128i hi8      = _mm_unpackhi_epi8(_mm_undefined_si128(), vIn);
  const __m128i lolo16   = _mm_unpacklo_epi16(_mm_undefined_si128(), lo8);
  const __m128i lohi16   = _mm_unpackhi_epi16(_mm_undefined_si128(), lo8);
  const __m128i hilo16   = _mm_unpacklo_epi16(_mm_undefined_si128(), hi8);
  const __m128i hihi16   = _mm_unpackhi_epi16(_mm_undefined_si128(), hi8);
  vOut[0]                = _mm_cvtepi32_ps(_mm_srai_epi32(lolo16, 24));
  vOut[1]                = _mm_cvtepi32_ps(_mm_srai_epi32(lohi16, 24));
  vOut[2]                = _mm_cvtepi32_ps(_mm_srai_epi32(hilo16, 24));
  vOut[3]                = _mm_cvtepi32_ps(_mm_srai_epi32(hihi16, 24));
#endif
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Long, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi16_epi64(vIn);
  vOut[1] = _mm_cvtepi16_epi64(_mm_srli_si128(vIn, 4));
  vOut[2] = _mm_cvtepi16_epi64(_mm_srli_si128(vIn, 8));
  vOut[3] = _mm_cvtepi16_epi64(_mm_srli_si128(vIn, 12));
#else
  const __m128i lo16     = _mm_unpacklo_epi16(_mm_undefined_si128(), vIn);
  const __m128i hi16     = _mm_unpackhi_epi16(_mm_undefined_si128(), vIn);
  const __m128i lo16ext  = _mm_srai_epi32(lo16, 16);
  const __m128i hi16ext  = _mm_srai_epi32(hi16, 16);
  const __m128i lo16sign = _mm_srai_epi32(lo16, 31);
  const __m128i hi16sign = _mm_srai_epi32(hi16, 31);
  vOut[0]                = _mm_unpacklo_epi32(lo16ext, lo16sign);
  vOut[1]                = _mm_unpackhi_epi32(lo16ext, lo16sign);
  vOut[2]                = _mm_unpacklo_epi32(hi16ext, hi16sign);
  vOut[3]                = _mm_unpackhi_epi32(hi16ext, hi16sign);
#endif
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn,
                               Vec<Double, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_pd(_mm_cvtepi16_epi32(vIn));
  vOut[1] = _mm_cvtepi32_pd(_mm_cvtepi16_epi32(_mm_srli_si128(vIn, 4)));
  vOut[2] = _mm_cvtepi32_pd(_mm_cvtepi16_epi32(_mm_srli_si128(vIn, 8)));
  vOut[3] = _mm_cvtepi32_pd(_mm_cvtepi16_epi32(_mm_srli_si128(vIn, 12)));
#else
  const __m128i lo16 =
    _mm_srai_epi32(_mm_unpacklo_epi16(_mm_undefined_si128(), vIn), 16);
  const __m128i hi16 =
    _mm_srai_epi32(_mm_unpackhi_epi16(_mm_undefined_si128(), vIn), 16);
  vOut[0]           = _mm_cvtepi32_pd(lo16);
  vOut[1]           = _mm_cvtepi32_pd(_mm_srli_si128(lo16, 8));
  vOut[2]           = _mm_cvtepi32_pd(hi16);
  vOut[3]           = _mm_cvtepi32_pd(_mm_srli_si128(hi16, 8));
#endif
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Int, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepu8_epi32(vIn);
  vOut[1] = _mm_cvtepu8_epi32(_mm_srli_si128(vIn, 4));
  vOut[2] = _mm_cvtepu8_epi32(_mm_srli_si128(vIn, 8));
  vOut[3] = _mm_cvtepu8_epi32(_mm_srli_si128(vIn, 12));
#else
  const __m128i lo8 = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  const __m128i hi8 = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
  vOut[0]           = _mm_unpacklo_epi16(lo8, _mm_setzero_si128());
  vOut[1]           = _mm_unpackhi_epi16(lo8, _mm_setzero_si128());
  vOut[2]           = _mm_unpacklo_epi16(hi8, _mm_setzero_si128());
  vOut[3]           = _mm_unpackhi_epi16(hi8, _mm_setzero_si128());
#endif
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Float, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_ps(_mm_cvtepu8_epi32(vIn));
  vOut[1] = _mm_cvtepi32_ps(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 4)));
  vOut[2] = _mm_cvtepi32_ps(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 8)));
  vOut[3] = _mm_cvtepi32_ps(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 12)));
#else
  const __m128i lo8 = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  const __m128i hi8 = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
  vOut[0] = _mm_cvtepi32_ps(_mm_unpacklo_epi16(lo8, _mm_setzero_si128()));
  vOut[1] = _mm_cvtepi32_ps(_mm_unpackhi_epi16(lo8, _mm_setzero_si128()));
  vOut[2] = _mm_cvtepi32_ps(_mm_unpacklo_epi16(hi8, _mm_setzero_si128()));
  vOut[3] = _mm_cvtepi32_ps(_mm_unpackhi_epi16(hi8, _mm_setzero_si128()));
#endif
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Long, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepu16_epi64(vIn);
  vOut[1] = _mm_cvtepu16_epi64(_mm_srli_si128(vIn, 4));
  vOut[2] = _mm_cvtepu16_epi64(_mm_srli_si128(vIn, 8));
  vOut[3] = _mm_cvtepu16_epi64(_mm_srli_si128(vIn, 12));
#else
  const __m128i lo16       = _mm_unpacklo_epi16(vIn, _mm_setzero_si128());
  const __m128i hi16       = _mm_unpackhi_epi16(vIn, _mm_setzero_si128());
  vOut[0]                  = _mm_unpacklo_epi32(lo16, _mm_setzero_si128());
  vOut[1]                  = _mm_unpackhi_epi32(lo16, _mm_setzero_si128());
  vOut[2]                  = _mm_unpacklo_epi32(hi16, _mm_setzero_si128());
  vOut[3]                  = _mm_unpackhi_epi32(hi16, _mm_setzero_si128());
#endif
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn,
                               Vec<Double, 16> vOut[4])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_pd(_mm_cvtepu16_epi32(vIn));
  vOut[1] = _mm_cvtepi32_pd(_mm_cvtepu16_epi32(_mm_srli_si128(vIn, 4)));
  vOut[2] = _mm_cvtepi32_pd(_mm_cvtepu16_epi32(_mm_srli_si128(vIn, 8)));
  vOut[3] = _mm_cvtepi32_pd(_mm_cvtepu16_epi32(_mm_srli_si128(vIn, 12)));
#else
  const __m128i lo16       = _mm_unpacklo_epi16(vIn, _mm_setzero_si128());
  const __m128i hi16       = _mm_unpackhi_epi16(vIn, _mm_setzero_si128());
  vOut[0]                  = _mm_cvtepi32_pd(lo16);
  vOut[1]                  = _mm_cvtepi32_pd(_mm_srli_si128(lo16, 8));
  vOut[2]                  = _mm_cvtepi32_pd(hi16);
  vOut[3]                  = _mm_cvtepi32_pd(_mm_srli_si128(hi16, 8));
#endif
}

// ---------------------------------------------------------------------------
// generalized extend: three stages
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Long, 16> vOut[8])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi8_epi64(vIn);
  vOut[1] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 2));
  vOut[2] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 4));
  vOut[3] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 6));
  vOut[4] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 8));
  vOut[5] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 10));
  vOut[6] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 12));
  vOut[7] = _mm_cvtepi8_epi64(_mm_srli_si128(vIn, 14));
#else
  const __m128i lo8        = _mm_unpacklo_epi8(_mm_undefined_si128(), vIn);
  const __m128i hi8        = _mm_unpackhi_epi8(_mm_undefined_si128(), vIn);
  const __m128i lolo16     = _mm_unpacklo_epi16(_mm_undefined_si128(), lo8);
  const __m128i lohi16     = _mm_unpackhi_epi16(_mm_undefined_si128(), lo8);
  const __m128i hilo16     = _mm_unpacklo_epi16(_mm_undefined_si128(), hi8);
  const __m128i hihi16     = _mm_unpackhi_epi16(_mm_undefined_si128(), hi8);
  const __m128i lolo16ext  = _mm_srai_epi32(lolo16, 24);
  const __m128i lohi16ext  = _mm_srai_epi32(lohi16, 24);
  const __m128i hilo16ext  = _mm_srai_epi32(hilo16, 24);
  const __m128i hihi16ext  = _mm_srai_epi32(hihi16, 24);
  const __m128i lolo16sign = _mm_srai_epi32(lolo16, 31);
  const __m128i lohi16sign = _mm_srai_epi32(lohi16, 31);
  const __m128i hilo16sign = _mm_srai_epi32(hilo16, 31);
  const __m128i hihi16sign = _mm_srai_epi32(hihi16, 31);
  vOut[0]                  = _mm_unpacklo_epi32(lolo16ext, lolo16sign);
  vOut[1]                  = _mm_unpackhi_epi32(lolo16ext, lolo16sign);
  vOut[2]                  = _mm_unpacklo_epi32(lohi16ext, lohi16sign);
  vOut[3]                  = _mm_unpackhi_epi32(lohi16ext, lohi16sign);
  vOut[4]                  = _mm_unpacklo_epi32(hilo16ext, hilo16sign);
  vOut[5]                  = _mm_unpackhi_epi32(hilo16ext, hilo16sign);
  vOut[6]                  = _mm_unpacklo_epi32(hihi16ext, hihi16sign);
  vOut[7]                  = _mm_unpackhi_epi32(hihi16ext, hihi16sign);
#endif
}

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Double, 16> vOut[8])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(vIn));
  vOut[1] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 2)));
  vOut[2] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 4)));
  vOut[3] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 6)));
  vOut[4] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 8)));
  vOut[5] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 10)));
  vOut[6] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 12)));
  vOut[7] = _mm_cvtepi32_pd(_mm_cvtepi8_epi32(_mm_srli_si128(vIn, 14)));
#else
  const __m128i lo8        = _mm_unpacklo_epi8(_mm_undefined_si128(), vIn);
  const __m128i hi8        = _mm_unpackhi_epi8(_mm_undefined_si128(), vIn);
  const __m128i lolo16     = _mm_unpacklo_epi16(_mm_undefined_si128(), lo8);
  const __m128i lohi16     = _mm_unpackhi_epi16(_mm_undefined_si128(), lo8);
  const __m128i hilo16     = _mm_unpacklo_epi16(_mm_undefined_si128(), hi8);
  const __m128i hihi16     = _mm_unpackhi_epi16(_mm_undefined_si128(), hi8);
  const __m128i lolo16ext  = _mm_srai_epi32(lolo16, 24);
  const __m128i lohi16ext  = _mm_srai_epi32(lohi16, 24);
  const __m128i hilo16ext  = _mm_srai_epi32(hilo16, 24);
  const __m128i hihi16ext  = _mm_srai_epi32(hihi16, 24);
  vOut[0]                  = _mm_cvtepi32_pd(lolo16ext);
  vOut[1]                  = _mm_cvtepi32_pd(_mm_srli_si128(lolo16ext, 8));
  vOut[2]                  = _mm_cvtepi32_pd(lohi16ext);
  vOut[3]                  = _mm_cvtepi32_pd(_mm_srli_si128(lohi16ext, 8));
  vOut[4]                  = _mm_cvtepi32_pd(hilo16ext);
  vOut[5]                  = _mm_cvtepi32_pd(_mm_srli_si128(hilo16ext, 8));
  vOut[6]                  = _mm_cvtepi32_pd(hihi16ext);
  vOut[7]                  = _mm_cvtepi32_pd(_mm_srli_si128(hihi16ext, 8));
#endif
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Long, 16> vOut[8])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepu8_epi64(vIn);
  vOut[1] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 2));
  vOut[2] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 4));
  vOut[3] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 6));
  vOut[4] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 8));
  vOut[5] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 10));
  vOut[6] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 12));
  vOut[7] = _mm_cvtepu8_epi64(_mm_srli_si128(vIn, 14));
#else
  const __m128i lo8        = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  const __m128i hi8        = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
  const __m128i lolo16     = _mm_unpacklo_epi16(lo8, _mm_setzero_si128());
  const __m128i lohi16     = _mm_unpackhi_epi16(lo8, _mm_setzero_si128());
  const __m128i hilo16     = _mm_unpacklo_epi16(hi8, _mm_setzero_si128());
  const __m128i hihi16     = _mm_unpackhi_epi16(hi8, _mm_setzero_si128());
  vOut[0]                  = _mm_unpacklo_epi32(lolo16, _mm_setzero_si128());
  vOut[1]                  = _mm_unpackhi_epi32(lolo16, _mm_setzero_si128());
  vOut[2]                  = _mm_unpacklo_epi32(lohi16, _mm_setzero_si128());
  vOut[3]                  = _mm_unpackhi_epi32(lohi16, _mm_setzero_si128());
  vOut[4]                  = _mm_unpacklo_epi32(hilo16, _mm_setzero_si128());
  vOut[5]                  = _mm_unpackhi_epi32(hilo16, _mm_setzero_si128());
  vOut[6]                  = _mm_unpacklo_epi32(hihi16, _mm_setzero_si128());
  vOut[7]                  = _mm_unpackhi_epi32(hihi16, _mm_setzero_si128());
#endif
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn,
                               Vec<Double, 16> vOut[8])
{
#ifdef __SSE4_1__
  vOut[0] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(vIn));
  vOut[1] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 2)));
  vOut[2] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 4)));
  vOut[3] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 6)));
  vOut[4] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 8)));
  vOut[5] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 10)));
  vOut[6] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 12)));
  vOut[7] = _mm_cvtepi32_pd(_mm_cvtepu8_epi32(_mm_srli_si128(vIn, 14)));
#else
  const __m128i lo8        = _mm_unpacklo_epi8(vIn, _mm_setzero_si128());
  const __m128i hi8        = _mm_unpackhi_epi8(vIn, _mm_setzero_si128());
  const __m128i lolo16     = _mm_unpacklo_epi16(lo8, _mm_setzero_si128());
  const __m128i lohi16     = _mm_unpackhi_epi16(lo8, _mm_setzero_si128());
  const __m128i hilo16     = _mm_unpacklo_epi16(hi8, _mm_setzero_si128());
  const __m128i hihi16     = _mm_unpackhi_epi16(hi8, _mm_setzero_si128());
  vOut[0]                  = _mm_cvtepi32_pd(lolo16);
  vOut[1]                  = _mm_cvtepi32_pd(_mm_srli_si128(lolo16, 8));
  vOut[2]                  = _mm_cvtepi32_pd(lohi16);
  vOut[3]                  = _mm_cvtepi32_pd(_mm_srli_si128(lohi16, 8));
  vOut[4]                  = _mm_cvtepi32_pd(hilo16);
  vOut[5]                  = _mm_cvtepi32_pd(_mm_srli_si128(hilo16, 8));
  vOut[6]                  = _mm_cvtepi32_pd(hihi16);
  vOut[7]                  = _mm_cvtepi32_pd(_mm_srli_si128(hihi16, 8));
#endif
}

// ---------------------------------------------------------------------------
// generalized extend: special case int <-> float, long <-> double
// ---------------------------------------------------------------------------

template <typename Tout, typename Tin,
          SIMD_ENABLE_IF(sizeof(Tin) == sizeof(Tout) &&
                         std::is_floating_point<Tin>::value !=
                           std::is_floating_point<Tout>::value)>
static SIMD_INLINE void extend(const Vec<Tin, 16> &vIn, Vec<Tout, 16> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Tout>());
}

// ---------------------------------------------------------------------------
// srai
// ---------------------------------------------------------------------------

// 16. Oct 22 (Jonas Keller): added missing Byte and SignedByte versions

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 16> srai(const Vec<Byte, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    const __m128i odd  = _mm_srai_epi16(a, COUNT);
    const __m128i even = _mm_srai_epi16(_mm_slli_epi16(a, 8), COUNT + 8);
    const __m128i odd_masked =
      _mm_and_si128(odd, _mm_set1_epi16((int16_t) 0xFF00));
    const __m128i even_masked = _mm_and_si128(even, _mm_set1_epi16(0x00FF));
    return _mm_or_si128(odd_masked, even_masked);
  } else {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm_cmplt_epi8(a, _mm_setzero_si128());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 16> srai(const Vec<SignedByte, 16> &a)
{
  return reinterpret(srai<COUNT>(reinterpret(a, OutputType<Byte>())),
                     OutputType<SignedByte>());
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 16> srai(const Vec<Word, 16> &a)
{
  return _mm_srai_epi16(a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 16> srai(const Vec<Short, 16> &a)
{
  return _mm_srai_epi16(a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 16> srai(const Vec<Int, 16> &a)
{
  return _mm_srai_epi32(a, vec::min(COUNT, 31ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 16> srai(const Vec<Long, 16> &a)
{
  // _mm_srai_epi64 is not available
  // workaround from Hacker's Delight, 2–17 Double-Length Shifts, Shift right
  // double signed:
  const __m128i odd = _mm_srai_epi32(a, vec::min(COUNT, 31ul));
  __m128i even;
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    even = _mm_or_si128(_mm_srli_epi32(a, COUNT),
                        _mm_slli_epi32(_mm_srli_si128(a, 4), 32 - COUNT));
  } else {
    even = _mm_srai_epi32(_mm_srli_si128(a, 4), vec::min(COUNT - 32, 31ul));
  }
#ifdef __SSE4_1__
  return _mm_blend_epi16(even, odd, 0xcc);
#else
  return _mm_or_si128(_mm_and_si128(even, _mm_set1_epi64x(0x00000000FFFFFFFF)),
                      _mm_and_si128(odd, _mm_set1_epi64x(0xFFFFFFFF00000000)));
#endif
}

// ---------------------------------------------------------------------------
// srli
// ---------------------------------------------------------------------------

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 16> srli(const Vec<Byte, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm_and_si128(_mm_set1_epi8((int8_t) (0xff >> COUNT)),
                         _mm_srli_epi32(a, COUNT));
  } else {
    return _mm_setzero_si128();
  }
}

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 16> srli(const Vec<SignedByte, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm_and_si128(_mm_set1_epi8((int8_t) (0xff >> COUNT)),
                         _mm_srli_epi32(a, COUNT));
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 16> srli(const Vec<Word, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm_srli_epi16(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 16> srli(const Vec<Short, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm_srli_epi16(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 16> srli(const Vec<Int, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm_srli_epi32(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 16> srli(const Vec<Long, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm_srli_epi64(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

// ---------------------------------------------------------------------------
// slli
// ---------------------------------------------------------------------------

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 16> slli(const Vec<Byte, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm_and_si128(
      _mm_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm_slli_epi32(a, COUNT));
  } else {
    return _mm_setzero_si128();
  }
}

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 16> slli(const Vec<SignedByte, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm_and_si128(
      _mm_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm_slli_epi32(a, COUNT));
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 16> slli(const Vec<Word, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm_slli_epi16(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 16> slli(const Vec<Short, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm_slli_epi16(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 16> slli(const Vec<Int, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm_slli_epi32(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 16> slli(const Vec<Long, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm_slli_epi64(a, COUNT);
  } else {
    return _mm_setzero_si128();
  }
}

// 19. Dec 22 (Jonas Keller): added sra, srl and sll functions

// ---------------------------------------------------------------------------
// sra
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> sra(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  // there is no _mm_sra_epi8 intrinsic
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm_cmplt_epi8(a, _mm_setzero_si128());
  }
  __m128i odd = _mm_sra_epi16(a, _mm_cvtsi32_si128(count));
  __m128i even =
    _mm_sra_epi16(_mm_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  return ifelse<Byte>(_mm_set1_epi16((int16_t) 0xFF00), odd, even);
}

static SIMD_INLINE Vec<SignedByte, 16> sra(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  // there is no _mm_sra_epi8 intrinsic
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm_cmplt_epi8(a, _mm_setzero_si128());
  }
  __m128i odd = _mm_sra_epi16(a, _mm_cvtsi32_si128(count));
  __m128i even =
    _mm_sra_epi16(_mm_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  return ifelse<SignedByte>(_mm_set1_epi16((int16_t) 0xFF00), odd, even);
}

static SIMD_INLINE Vec<Word, 16> sra(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  return _mm_sra_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 16> sra(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  return _mm_sra_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 16> sra(const Vec<Int, 16> &a, const uint8_t count)
{
  return _mm_sra_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 16> sra(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  // workaround from Hacker's Delight, 2–17 Double-Length Shifts, Shift right
  // double signed:
  const __m128i odd = _mm_sra_epi32(a, _mm_cvtsi32_si128(count));
  __m128i even;
  if (count < 32) {
    even = _mm_or_si128(
      _mm_srl_epi32(a, _mm_cvtsi32_si128(count)),
      _mm_sll_epi32(_mm_srli_si128(a, 4), _mm_cvtsi32_si128(32 - count)));
  } else {
    even = _mm_sra_epi32(_mm_srli_si128(a, 4), _mm_cvtsi32_si128(count - 32));
  }
#ifdef __SSE4_1__
  return _mm_blend_epi16(even, odd, 0xcc);
#else
  return _mm_or_si128(_mm_and_si128(even, _mm_set1_epi64x(0x00000000FFFFFFFF)),
                      _mm_and_si128(odd, _mm_set1_epi64x(0xFFFFFFFF00000000)));
#endif
}

// ---------------------------------------------------------------------------
// srl
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> srl(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  return _mm_and_si128(_mm_srl_epi16(a, _mm_cvtsi32_si128(count)),
                       _mm_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<SignedByte, 16> srl(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  return _mm_and_si128(_mm_srl_epi16(a, _mm_cvtsi32_si128(count)),
                       _mm_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<Word, 16> srl(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  return _mm_srl_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 16> srl(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  return _mm_srl_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 16> srl(const Vec<Int, 16> &a, const uint8_t count)
{
  return _mm_srl_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 16> srl(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  return _mm_srl_epi64(a, _mm_cvtsi32_si128(count));
}

// ---------------------------------------------------------------------------
// sll
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> sll(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  return _mm_and_si128(
    _mm_sll_epi16(a, _mm_cvtsi32_si128(count)),
    _mm_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<SignedByte, 16> sll(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  return _mm_and_si128(
    _mm_sll_epi16(a, _mm_cvtsi32_si128(count)),
    _mm_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<Word, 16> sll(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  return _mm_sll_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 16> sll(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  return _mm_sll_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 16> sll(const Vec<Int, 16> &a, const uint8_t count)
{
  return _mm_sll_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 16> sll(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  return _mm_sll_epi64(a, _mm_cvtsi32_si128(count));
}

// 19. Sep 22 (Jonas Keller):
// added Byte and SignedByte versions of hadd, hadds, hsub and hsubs
// added Word version of hadds and hsubs

// ---------------------------------------------------------------------------
// hadd
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> hadd(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return add(x, y);
}

static SIMD_INLINE Vec<Word, 16> hadd(const Vec<Word, 16> &a,
                                      const Vec<Word, 16> &b)
{
  return _mm_hadd_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> hadd(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b)
{
  return _mm_hadd_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> hadd(const Vec<Int, 16> &a,
                                     const Vec<Int, 16> &b)
{
  return _mm_hadd_epi32(a, b);
}

static SIMD_INLINE Vec<Float, 16> hadd(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b)
{
  return _mm_hadd_ps(a, b);
}

// _mm_hadd_pd is only available with SSE3, if compiling without SSE3
// use template version above
#ifdef __SSE3__
static SIMD_INLINE Vec<Double, 16> hadd(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return _mm_hadd_pd(a, b);
}
#endif

// ---------------------------------------------------------------------------
// hadds
// ---------------------------------------------------------------------------

// 09. Mar 23 (Jonas Keller): made Int version of hadds saturating

template <typename T>
static SIMD_INLINE Vec<T, 16> hadds(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return adds(x, y);
}

static SIMD_INLINE Vec<Short, 16> hadds(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_hadds_epi16(a, b);
}

// Float not saturated
static SIMD_INLINE Vec<Float, 16> hadds(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_hadd_ps(a, b);
}

// _mm_hadd_pd is only available with SSE3, if compiling without SSE3
// use template version above
#ifdef __SSE3__
static SIMD_INLINE Vec<Double, 16> hadds(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_hadd_pd(a, b);
}
#endif

// ---------------------------------------------------------------------------
// hsub
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> hsub(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return sub(x, y);
}

static SIMD_INLINE Vec<Word, 16> hsub(const Vec<Word, 16> &a,
                                      const Vec<Word, 16> &b)
{
  return _mm_hsub_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> hsub(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b)
{
  return _mm_hsub_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> hsub(const Vec<Int, 16> &a,
                                     const Vec<Int, 16> &b)
{
  return _mm_hsub_epi32(a, b);
}

static SIMD_INLINE Vec<Float, 16> hsub(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b)
{
  return _mm_hsub_ps(a, b);
}

// _mm_hsub_pd is only available with SSE3, if compiling without SSE3
// use template version above
#ifdef __SSE3__
static SIMD_INLINE Vec<Double, 16> hsub(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return _mm_hsub_pd(a, b);
}
#endif

// ---------------------------------------------------------------------------
// hsubs
// ---------------------------------------------------------------------------

// 09. Mar 23 (Jonas Keller): made Int version of hsubs saturating

template <typename T>
static SIMD_INLINE Vec<T, 16> hsubs(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return subs(x, y);
}

static SIMD_INLINE Vec<Short, 16> hsubs(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_hsubs_epi16(a, b);
}

// Float not saturated
static SIMD_INLINE Vec<Float, 16> hsubs(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_hsub_ps(a, b);
}

// _mm_hsub_pd is only available with SSE3, if compiling without SSE3
// use template version above
#ifdef __SSE3__
static SIMD_INLINE Vec<Double, 16> hsubs(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_hsub_pd(a, b);
}
#endif

// ---------------------------------------------------------------------------
// element-wise shift right
// ---------------------------------------------------------------------------

template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> srle(const Vec<T, 16> &a)
{
  const auto intA = reinterpret(a, OutputType<Int>());
  const Vec<Int, 16> result =
    _mm_srli_si128(intA, vec::min(COUNT * sizeof(T), 16lu));
  return reinterpret(result, OutputType<T>());
}

// ---------------------------------------------------------------------------
// element-wise shift left
// ---------------------------------------------------------------------------

// all integer versions
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> slle(const Vec<T, 16> &a)
{
  const auto intA = reinterpret(a, OutputType<Int>());
  const Vec<Int, 16> result =
    _mm_slli_si128(intA, vec::min(COUNT * sizeof(T), 16lu));
  return reinterpret(result, OutputType<T>());
}

// ---------------------------------------------------------------------------
// alignre
// ---------------------------------------------------------------------------

// all integer versions
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> alignre(const Vec<T, 16> &h, const Vec<T, 16> &l)
{
  SIMD_IF_CONSTEXPR (COUNT * sizeof(T) < 32) {
    return _mm_alignr_epi8(h, l, COUNT * sizeof(T));
  } else {
    return _mm_setzero_si128();
  }
}

// float version
template <size_t COUNT>
static SIMD_INLINE Vec<Float, 16> alignre(const Vec<Float, 16> &h,
                                          const Vec<Float, 16> &l)
{
  SIMD_IF_CONSTEXPR (COUNT * sizeof(Float) < 32) {
    return _mm_castsi128_ps(_mm_alignr_epi8(
      _mm_castps_si128(h), _mm_castps_si128(l), COUNT * sizeof(Float)));
  } else {
    return _mm_setzero_ps();
  }
}

// double version
template <size_t COUNT>
static SIMD_INLINE Vec<Double, 16> alignre(const Vec<Double, 16> &h,
                                           const Vec<Double, 16> &l)
{
  SIMD_IF_CONSTEXPR (COUNT * sizeof(Double) < 32) {
    return _mm_castsi128_pd(_mm_alignr_epi8(
      _mm_castpd_si128(h), _mm_castpd_si128(l), COUNT * sizeof(Double)));
  } else {
    return _mm_setzero_pd();
  }
}

// ---------------------------------------------------------------------------
// swizzle
// ---------------------------------------------------------------------------

// swizzle masks (only for 8 and 16 bit element types)

// [masks generated from ~/texte/Talks/SSE/swizzle.c]

// Byte, SignedByte

static SIMD_INLINE __m128i get_swizzle_mask(Integer<2>, Integer<1>)
{
  return _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11, 13, 15);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<3>, Integer<1>)
{
  return _mm_setr_epi8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, -1, -1, -1, -1);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<4>, Integer<1>)
{
  return _mm_setr_epi8(0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<5>, Integer<1>)
{
  return _mm_setr_epi8(0, 5, 1, 6, 2, 7, 3, 8, 4, 9, -1, -1, -1, -1, -1, -1);
}

// Word, Short

static SIMD_INLINE __m128i get_swizzle_mask(Integer<2>, Integer<2>)
{
  return _mm_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11, 14, 15);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<3>, Integer<2>)
{
  return _mm_setr_epi8(0, 1, 6, 7, 2, 3, 8, 9, 4, 5, 10, 11, -1, -1, -1, -1);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<4>, Integer<2>)
{
  return _mm_setr_epi8(0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7, 14, 15);
}

static SIMD_INLINE __m128i get_swizzle_mask(Integer<5>, Integer<2>)
{
  return _mm_setr_epi8(0, 1, 10, 11, 2, 3, 12, 13, 4, 5, 14, 15, -1, -1, -1,
                       -1);
}

// hub
template <size_t N, typename T>
static SIMD_INLINE __m128i get_swizzle_mask()
{
  return get_swizzle_mask(Integer<N>(), Integer<sizeof(T)>());
}

// ---------- swizzle aux functions -----------

// alignoff is the element-wise offset (relates to size of byte)
template <size_t ALIGNOFF>
static SIMD_INLINE __m128i align_shuffle_128(__m128i lo, __m128i hi,
                                             __m128i mask)
{
  static_assert(ALIGNOFF >= 0 && ALIGNOFF < 32, "");
  return _mm_shuffle_epi8(_mm_alignr_epi8(hi, lo, ALIGNOFF), mask);
}

// ---------- swizzle (AoS to SoA) ----------

// 01. Apr 23 (Jonas Keller): switched from using tag dispatching to using
// enable_if SFINAE, which allows more cases with the same implementation
// to be combined

// -------------------- n = 1 --------------------

// all types
template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16>[1], Integer<1>)
{
  // v remains unchanged
}

// -------------------- n = 2 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) <= 2 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 16> v[2], Integer<2>)
{
  __m128i s[2];
  s[0] = _mm_shuffle_epi8(v[0], get_swizzle_mask<2, T>());
  s[1] = _mm_shuffle_epi8(v[1], get_swizzle_mask<2, T>());
  v[0] = _mm_unpacklo_epi64(s[0], s[1]);
  v[1] = _mm_unpackhi_epi64(s[0], s[1]);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[2], Integer<2>)
{
  const __m128 v0tmp = reinterpret(v[0], OutputType<Float>());
  const __m128 v1tmp = reinterpret(v[1], OutputType<Float>());
  const Vec<Float, 16> v0TmpOut =
    _mm_shuffle_ps(v0tmp, v1tmp, _MM_SHUFFLE(2, 0, 2, 0));
  const Vec<Float, 16> v1TmpOut =
    _mm_shuffle_ps(v0tmp, v1tmp, _MM_SHUFFLE(3, 1, 3, 1));
  v[0] = reinterpret(v0TmpOut, OutputType<T>());
  v[1] = reinterpret(v1TmpOut, OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[2], Integer<2>)
{
  const __m128d v0tmp = reinterpret(v[0], OutputType<Double>());
  const __m128d v1tmp = reinterpret(v[1], OutputType<Double>());
  const Vec<Double, 16> v0TmpOut =
    _mm_shuffle_pd(v0tmp, v1tmp, _MM_SHUFFLE2(0, 0));
  const Vec<Double, 16> v1TmpOut =
    _mm_shuffle_pd(v0tmp, v1tmp, _MM_SHUFFLE2(1, 1));
  v[0] = reinterpret(v0TmpOut, OutputType<T>());
  v[1] = reinterpret(v1TmpOut, OutputType<T>());
}

// -------------------- n = 3 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) <= 2 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 16> v[3], Integer<3>)
{
  __m128i mask = get_swizzle_mask<3, T>();
  __m128i s0   = align_shuffle_128<0>(v[0], v[1], mask);
  __m128i s1   = align_shuffle_128<12>(v[0], v[1], mask);
  __m128i s2   = align_shuffle_128<8>(v[1], v[2], mask);
  __m128i s3   = align_shuffle_128<4>(v[2], _mm_undefined_si128(), mask);
  __m128i l01  = _mm_unpacklo_epi32(s0, s1);
  __m128i h01  = _mm_unpackhi_epi32(s0, s1);
  __m128i l23  = _mm_unpacklo_epi32(s2, s3);
  __m128i h23  = _mm_unpackhi_epi32(s2, s3);
  v[0]         = _mm_unpacklo_epi64(l01, l23);
  v[1]         = _mm_unpackhi_epi64(l01, l23);
  v[2]         = _mm_unpacklo_epi64(h01, h23);
}

// 32 bit types
// from Stan Melax: "3D Vector Normalization..."
// https://software.intel.com/en-us/articles/3d-vector-normalization-using-256-bit-intel-advanced-vector-extensions-intel-avx
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[3], Integer<3>)
{
  const __m128 x0y0z0x1 = reinterpret(v[0], OutputType<Float>());
  const __m128 y1z1x2y2 = reinterpret(v[1], OutputType<Float>());
  const __m128 z2x3y3z3 = reinterpret(v[2], OutputType<Float>());
  const __m128 x2y2x3y3 =
    _mm_shuffle_ps(y1z1x2y2, z2x3y3z3, _MM_SHUFFLE(2, 1, 3, 2));
  const __m128 y0z0y1z1 =
    _mm_shuffle_ps(x0y0z0x1, y1z1x2y2, _MM_SHUFFLE(1, 0, 2, 1));
  const Vec<Float, 16> x0x1x2x3 =
    _mm_shuffle_ps(x0y0z0x1, x2y2x3y3, _MM_SHUFFLE(2, 0, 3, 0));
  const Vec<Float, 16> y0y1y2y3 =
    _mm_shuffle_ps(y0z0y1z1, x2y2x3y3, _MM_SHUFFLE(3, 1, 2, 0));
  const Vec<Float, 16> z0z1z2z3 =
    _mm_shuffle_ps(y0z0y1z1, z2x3y3z3, _MM_SHUFFLE(3, 0, 3, 1));
  v[0] = reinterpret(x0x1x2x3, OutputType<T>());
  v[1] = reinterpret(y0y1y2y3, OutputType<T>());
  v[2] = reinterpret(z0z1z2z3, OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void, typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[3], Integer<3>)
{
  const __m128d x0y0         = reinterpret(v[0], OutputType<Double>());
  const __m128d z0x1         = reinterpret(v[1], OutputType<Double>());
  const __m128d y1z1         = reinterpret(v[2], OutputType<Double>());
  const Vec<Double, 16> x0x1 = _mm_shuffle_pd(x0y0, z0x1, _MM_SHUFFLE2(1, 0));
  const Vec<Double, 16> y0y1 = _mm_shuffle_pd(x0y0, y1z1, _MM_SHUFFLE2(0, 1));
  const Vec<Double, 16> z0z1 = _mm_shuffle_pd(z0x1, y1z1, _MM_SHUFFLE2(1, 0));
  v[0]                       = reinterpret(x0x1, OutputType<T>());
  v[1]                       = reinterpret(y0y1, OutputType<T>());
  v[2]                       = reinterpret(z0z1, OutputType<T>());
}

// -------------------- n = 4 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) <= 2 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 16> v[4], Integer<4>)
{
  __m128i mask = get_swizzle_mask<4, T>();
  __m128i s[4];
  s[0]        = _mm_shuffle_epi8(v[0], mask);
  s[1]        = _mm_shuffle_epi8(v[1], mask);
  s[2]        = _mm_shuffle_epi8(v[2], mask);
  s[3]        = _mm_shuffle_epi8(v[3], mask);
  __m128i l01 = _mm_unpacklo_epi32(s[0], s[1]);
  __m128i h01 = _mm_unpackhi_epi32(s[0], s[1]);
  __m128i l23 = _mm_unpacklo_epi32(s[2], s[3]);
  __m128i h23 = _mm_unpackhi_epi32(s[2], s[3]);
  v[0]        = _mm_unpacklo_epi64(l01, l23);
  v[1]        = _mm_unpackhi_epi64(l01, l23);
  v[2]        = _mm_unpacklo_epi64(h01, h23);
  v[3]        = _mm_unpackhi_epi64(h01, h23);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[4], Integer<4>)
{
  __m128 vFloat[4];
  for (size_t i = 0; i < 4; ++i) {
    vFloat[i] = reinterpret(v[i], OutputType<Float>());
  }
  __m128 s[4];
  s[0] = _mm_shuffle_ps(vFloat[0], vFloat[1], _MM_SHUFFLE(1, 0, 1, 0));
  s[1] = _mm_shuffle_ps(vFloat[0], vFloat[1], _MM_SHUFFLE(3, 2, 3, 2));
  s[2] = _mm_shuffle_ps(vFloat[2], vFloat[3], _MM_SHUFFLE(1, 0, 1, 0));
  s[3] = _mm_shuffle_ps(vFloat[2], vFloat[3], _MM_SHUFFLE(3, 2, 3, 2));
  Vec<Float, 16> vOut[4];
  vOut[0] = _mm_shuffle_ps(s[0], s[2], _MM_SHUFFLE(2, 0, 2, 0));
  vOut[1] = _mm_shuffle_ps(s[0], s[2], _MM_SHUFFLE(3, 1, 3, 1));
  vOut[2] = _mm_shuffle_ps(s[1], s[3], _MM_SHUFFLE(2, 0, 2, 0));
  vOut[3] = _mm_shuffle_ps(s[1], s[3], _MM_SHUFFLE(3, 1, 3, 1));
  for (size_t i = 0; i < 4; ++i) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[4], Integer<4>)
{
  const __m128d x0y0         = reinterpret(v[0], OutputType<Double>());
  const __m128d z0w0         = reinterpret(v[1], OutputType<Double>());
  const __m128d x1y1         = reinterpret(v[2], OutputType<Double>());
  const __m128d z1w1         = reinterpret(v[3], OutputType<Double>());
  const Vec<Double, 16> x0x1 = _mm_unpacklo_pd(x0y0, x1y1);
  const Vec<Double, 16> y0y1 = _mm_unpackhi_pd(x0y0, x1y1);
  const Vec<Double, 16> z0z1 = _mm_unpacklo_pd(z0w0, z1w1);
  const Vec<Double, 16> w0w1 = _mm_unpackhi_pd(z0w0, z1w1);
  v[0]                       = reinterpret(x0x1, OutputType<T>());
  v[1]                       = reinterpret(y0y1, OutputType<T>());
  v[2]                       = reinterpret(z0z1, OutputType<T>());
  v[3]                       = reinterpret(w0w1, OutputType<T>());
}

// -------------------- n = 5 --------------------

// 8 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 1 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 16> v[5], Integer<5>)
{
  __m128i mask    = get_swizzle_mask<5, T>();
  __m128i s0      = align_shuffle_128<0>(v[0], v[1], mask);
  __m128i s1      = align_shuffle_128<10>(v[0], v[1], mask);
  __m128i s2      = align_shuffle_128<4>(v[1], v[2], mask);
  __m128i s3      = align_shuffle_128<14>(v[1], v[2], mask);
  __m128i s4      = align_shuffle_128<8>(v[2], v[3], mask);
  __m128i s5      = align_shuffle_128<2>(v[3], _mm_undefined_si128(), mask);
  __m128i s6      = align_shuffle_128<12>(v[3], v[4], mask);
  __m128i s7      = align_shuffle_128<6>(v[4], _mm_undefined_si128(), mask);
  __m128i l01     = _mm_unpacklo_epi16(s0, s1);
  __m128i h01     = _mm_unpackhi_epi16(s0, s1);
  __m128i l23     = _mm_unpacklo_epi16(s2, s3);
  __m128i h23     = _mm_unpackhi_epi16(s2, s3);
  __m128i l45     = _mm_unpacklo_epi16(s4, s5);
  __m128i h45     = _mm_unpackhi_epi16(s4, s5);
  __m128i l67     = _mm_unpacklo_epi16(s6, s7);
  __m128i h67     = _mm_unpackhi_epi16(s6, s7);
  __m128i ll01l23 = _mm_unpacklo_epi32(l01, l23);
  __m128i hl01l23 = _mm_unpackhi_epi32(l01, l23);
  __m128i ll45l67 = _mm_unpacklo_epi32(l45, l67);
  __m128i hl45l67 = _mm_unpackhi_epi32(l45, l67);
  __m128i lh01h23 = _mm_unpacklo_epi32(h01, h23);
  __m128i lh45h67 = _mm_unpacklo_epi32(h45, h67);
  v[0]            = _mm_unpacklo_epi64(ll01l23, ll45l67);
  v[1]            = _mm_unpackhi_epi64(ll01l23, ll45l67);
  v[2]            = _mm_unpacklo_epi64(hl01l23, hl45l67);
  v[3]            = _mm_unpackhi_epi64(hl01l23, hl45l67);
  v[4]            = _mm_unpacklo_epi64(lh01h23, lh45h67);
}

// 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 2 && std::is_integral<T>::value),
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[5], Integer<5>)
{
  __m128i mask = get_swizzle_mask<5, T>();
  __m128i s0   = align_shuffle_128<0>(v[0], v[1], mask);
  __m128i s1   = align_shuffle_128<6>(v[0], v[1], mask);
  __m128i s2   = align_shuffle_128<4>(v[1], v[2], mask);
  __m128i s3   = align_shuffle_128<10>(v[1], v[2], mask);
  __m128i s4   = align_shuffle_128<8>(v[2], v[3], mask);
  __m128i s5   = align_shuffle_128<14>(v[2], v[3], mask);
  __m128i s6   = align_shuffle_128<12>(v[3], v[4], mask);
  __m128i s7   = align_shuffle_128<2>(v[4], _mm_undefined_si128(), mask);
  __m128i l02  = _mm_unpacklo_epi32(s0, s2);
  __m128i h02  = _mm_unpackhi_epi32(s0, s2);
  __m128i l13  = _mm_unpacklo_epi32(s1, s3);
  __m128i l46  = _mm_unpacklo_epi32(s4, s6);
  __m128i h46  = _mm_unpackhi_epi32(s4, s6);
  __m128i l57  = _mm_unpacklo_epi32(s5, s7);
  v[0]         = _mm_unpacklo_epi64(l02, l46);
  v[1]         = _mm_unpackhi_epi64(l02, l46);
  v[2]         = _mm_unpacklo_epi64(h02, h46);
  v[3]         = _mm_unpacklo_epi64(l13, l57);
  v[4]         = _mm_unpackhi_epi64(l13, l57);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> vT[5], Integer<5>)
{
  __m128i v[5];
  for (size_t i = 0; i < 5; i++) {
    v[i] = reinterpret(vT[i], OutputType<Int>());
  };
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  // v[0]: 0 1 2 3
  // v[1]: 4 x x x
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                   x x x   x
  // 5 6 7 8
  __m128i s2 = _mm_alignr_epi8(v[2], v[1], 4);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                             x  x  x    x
  // 9 x x x
  __m128i s3 = _mm_alignr_epi8(v[3], v[2], 4);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                x  x    x  x
  // 10 11 12 13
  __m128i s4 = _mm_alignr_epi8(v[3], v[2], 8);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                              x  x    x  x
  // 14 x x x
  __m128i s5 = _mm_alignr_epi8(v[4], v[3], 8);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                 X    X  X  X
  // 15 16 17 18
  __m128i s6 = _mm_alignr_epi8(v[4], v[3], 12);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                               X X X X
  // 19 x x x
  __m128i s7 = _mm_alignr_epi8(v[0], v[4], 12);
  // 0 1 2 3 / 5 6 7 8 -> 0 5 1 6 / 2 7 3 8
  __m128i l02 = _mm_unpacklo_epi32(v[0], s2);
  __m128i h02 = _mm_unpackhi_epi32(v[0], s2);
  // 4 x x x / 9 x x x -> 4 9 x x
  __m128i l13 = _mm_unpacklo_epi32(v[1], s3);
  // 10 11 12 13 / 15 16 17 18 -> 10 15 11 13 / 12 17 13 18
  __m128i l46 = _mm_unpacklo_epi32(s4, s6);
  __m128i h46 = _mm_unpackhi_epi32(s4, s6);
  // 14 x x x / 19 x x x -> 14 19 x x
  __m128i l57 = _mm_unpacklo_epi32(s5, s7);

  const Vec<Int, 16> vOut[5] = {
    // 0 5 1 6 / 10 15 11 13 -> 0 5 10 15 / 1 6 11 16
    _mm_unpacklo_epi64(l02, l46),
    _mm_unpackhi_epi64(l02, l46),
    // 2 7 3 8 / 12 17 13 18 -> 2 7 12 17 / 3 8 13 18
    _mm_unpacklo_epi64(h02, h46),
    _mm_unpackhi_epi64(h02, h46),
    // 4 9 x x / 14 19 x x -> 4 9 14 19
    _mm_unpacklo_epi64(l13, l57),
  };
  for (size_t i = 0; i < 5; ++i) {
    vT[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void, typename = void>
static SIMD_INLINE void swizzle(Vec<T, 16> v[5], Integer<5>)
{
  const __m128d a0b0         = reinterpret(v[0], OutputType<Double>());
  const __m128d c0d0         = reinterpret(v[1], OutputType<Double>());
  const __m128d e0a1         = reinterpret(v[2], OutputType<Double>());
  const __m128d b1c1         = reinterpret(v[3], OutputType<Double>());
  const __m128d d1e1         = reinterpret(v[4], OutputType<Double>());
  const Vec<Double, 16> a0a1 = _mm_shuffle_pd(a0b0, e0a1, _MM_SHUFFLE2(1, 0));
  const Vec<Double, 16> b0b1 = _mm_shuffle_pd(a0b0, b1c1, _MM_SHUFFLE2(0, 1));
  const Vec<Double, 16> c0c1 = _mm_shuffle_pd(c0d0, b1c1, _MM_SHUFFLE2(1, 0));
  const Vec<Double, 16> d0d1 = _mm_shuffle_pd(c0d0, d1e1, _MM_SHUFFLE2(0, 1));
  const Vec<Double, 16> e0e1 = _mm_shuffle_pd(e0a1, d1e1, _MM_SHUFFLE2(1, 0));
  v[0]                       = reinterpret(a0a1, OutputType<T>());
  v[1]                       = reinterpret(b0b1, OutputType<T>());
  v[2]                       = reinterpret(c0c1, OutputType<T>());
  v[3]                       = reinterpret(d0d1, OutputType<T>());
  v[4]                       = reinterpret(e0e1, OutputType<T>());
}

// ---------------------------------------------------------------------------
// compare <
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/sse-compare-packed-unsigned-bytes
static SIMD_INLINE Vec<Byte, 16> cmplt(const Vec<Byte, 16> &a,
                                       const Vec<Byte, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80808080);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x80
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x80
  return _mm_cmplt_epi8(a1, b1);
}

static SIMD_INLINE Vec<SignedByte, 16> cmplt(const Vec<SignedByte, 16> &a,
                                             const Vec<SignedByte, 16> &b)
{
  return _mm_cmplt_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> cmplt(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80008000);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x8000
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x8000
  return _mm_cmplt_epi16(a1, b1);
}

static SIMD_INLINE Vec<Short, 16> cmplt(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_cmplt_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> cmplt(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return _mm_cmplt_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 16> cmplt(const Vec<Long, 16> &a,
                                       const Vec<Long, 16> &b)
{
  // _mm_cmplt_epi64 does not exist
#ifdef __SSE4_2__
  return _mm_cmpgt_epi64(b, a);
#else
  // from Hacker's Delight, 2-12 Comparison Predicates:
  const __m128i diff = _mm_sub_epi64(a, b);
#if 1 // TODO: check which is faster
  const __m128i res  = _mm_xor_si128(
    diff, _mm_and_si128(_mm_xor_si128(a, b), _mm_xor_si128(diff, a)));
#else
  const __m128i res = _mm_or_si128(_mm_andnot_si128(b, a),
                                   _mm_andnot_si128(_mm_xor_si128(a, b), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  return _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmplt(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_cmplt_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmplt(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_cmplt_pd(a, b);
}

// ---------------------------------------------------------------------------
// compare <=
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/sse-compare-packed-unsigned-bytes
static SIMD_INLINE Vec<Byte, 16> cmple(const Vec<Byte, 16> &a,
                                       const Vec<Byte, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80808080);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x80
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x80
  return _mm_or_si128(_mm_cmplt_epi8(a1, b1), _mm_cmpeq_epi8(a1, b1));
}

static SIMD_INLINE Vec<SignedByte, 16> cmple(const Vec<SignedByte, 16> &a,
                                             const Vec<SignedByte, 16> &b)
{
  return _mm_or_si128(_mm_cmplt_epi8(a, b), _mm_cmpeq_epi8(a, b));
}

static SIMD_INLINE Vec<Word, 16> cmple(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80008000);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x8000
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x8000
  return _mm_or_si128(_mm_cmplt_epi16(a1, b1), _mm_cmpeq_epi16(a1, b1));
}

static SIMD_INLINE Vec<Short, 16> cmple(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_or_si128(_mm_cmplt_epi16(a, b), _mm_cmpeq_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 16> cmple(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return _mm_or_si128(_mm_cmplt_epi32(a, b), _mm_cmpeq_epi32(a, b));
}

static SIMD_INLINE Vec<Long, 16> cmple(const Vec<Long, 16> &a,
                                       const Vec<Long, 16> &b)
{
  // _mm_cmplt_epi64 does not exist
#ifdef __SSE4_2__
  return _mm_or_si128(_mm_cmpgt_epi64(b, a), _mm_cmpeq_epi64(a, b));
#else
  // Hacker's Delight, 2-12 Comparison Predicates:
  const __m128i res = _mm_and_si128(
    _mm_or_si128(a, _mm_xor_si128(b, _mm_set1_epi32(-1))),
    _mm_or_si128(_mm_xor_si128(a, b),
                 _mm_xor_si128(_mm_sub_epi64(b, a), _mm_set1_epi32(-1))));
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  return _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmple(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_cmple_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmple(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_cmple_pd(a, b);
}

// ---------------------------------------------------------------------------
// compare ==
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> cmpeq(const Vec<Byte, 16> &a,
                                       const Vec<Byte, 16> &b)
{
  return _mm_cmpeq_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 16> cmpeq(const Vec<SignedByte, 16> &a,
                                             const Vec<SignedByte, 16> &b)
{
  return _mm_cmpeq_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> cmpeq(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b)
{
  return _mm_cmpeq_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 16> cmpeq(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_cmpeq_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> cmpeq(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return _mm_cmpeq_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 16> cmpeq(const Vec<Long, 16> &a,
                                       const Vec<Long, 16> &b)
{
#ifdef __SSE_4_1__
  return _mm_cmpeq_epi64(a, b);
#else
  const __m128i res32 = _mm_cmpeq_epi32(a, b);
  return _mm_and_si128(res32,
                       _mm_shuffle_epi32(res32, _MM_SHUFFLE(2, 3, 0, 1)));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmpeq(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_cmpeq_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmpeq(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_cmpeq_pd(a, b);
}

// ---------------------------------------------------------------------------
// compare >
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/sse-compare-packed-unsigned-bytes
static SIMD_INLINE Vec<Byte, 16> cmpgt(const Vec<Byte, 16> &a,
                                       const Vec<Byte, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80808080);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x80
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x80
  return _mm_cmpgt_epi8(a1, b1);
}

static SIMD_INLINE Vec<SignedByte, 16> cmpgt(const Vec<SignedByte, 16> &a,
                                             const Vec<SignedByte, 16> &b)
{
  return _mm_cmpgt_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 16> cmpgt(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80008000);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x8000
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x8000
  return _mm_cmpgt_epi16(a1, b1);
}

static SIMD_INLINE Vec<Short, 16> cmpgt(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_cmpgt_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 16> cmpgt(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return _mm_cmpgt_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 16> cmpgt(const Vec<Long, 16> &a,
                                       const Vec<Long, 16> &b)
{
#ifdef __SSE4_2__
  return _mm_cmpgt_epi64(a, b);
#else
  // from Hacker's Delight, 2-12 Comparison Predicates: (swapped lt)
  const __m128i diff = _mm_sub_epi64(b, a);
#if 1 // TODO: check which is faster
  const __m128i res  = _mm_xor_si128(
    diff, _mm_and_si128(_mm_xor_si128(b, a), _mm_xor_si128(diff, b)));
#else
  const __m128i res = _mm_or_si128(_mm_andnot_si128(a, b),
                                   _mm_andnot_si128(_mm_xor_si128(b, a), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  return _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmpgt(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_cmpgt_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmpgt(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_cmpgt_pd(a, b);
}

// ---------------------------------------------------------------------------
// compare >=
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/sse-compare-packed-unsigned-bytes
static SIMD_INLINE Vec<Byte, 16> cmpge(const Vec<Byte, 16> &a,
                                       const Vec<Byte, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80808080);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x80
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x80
  return _mm_or_si128(_mm_cmpgt_epi8(a1, b1), _mm_cmpeq_epi8(a1, b1));
}

static SIMD_INLINE Vec<SignedByte, 16> cmpge(const Vec<SignedByte, 16> &a,
                                             const Vec<SignedByte, 16> &b)
{
  return _mm_or_si128(_mm_cmpgt_epi8(a, b), _mm_cmpeq_epi8(a, b));
}

static SIMD_INLINE Vec<Word, 16> cmpge(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b)
{
  __m128i signbit = _mm_set1_epi32(0x80008000);
  __m128i a1      = _mm_xor_si128(a, signbit); // sub 0x8000
  __m128i b1      = _mm_xor_si128(b, signbit); // sub 0x8000
  return _mm_or_si128(_mm_cmpgt_epi16(a1, b1), _mm_cmpeq_epi16(a1, b1));
}

static SIMD_INLINE Vec<Short, 16> cmpge(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return _mm_or_si128(_mm_cmpgt_epi16(a, b), _mm_cmpeq_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 16> cmpge(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return _mm_or_si128(_mm_cmpgt_epi32(a, b), _mm_cmpeq_epi32(a, b));
}

static SIMD_INLINE Vec<Long, 16> cmpge(const Vec<Long, 16> &a,
                                       const Vec<Long, 16> &b)
{
#ifdef __SSE4_2__
  return _mm_or_si128(_mm_cmpgt_epi64(a, b), _mm_cmpeq_epi64(a, b));
#else
  // Hacker's Delight, 2-12 Comparison Predicates: (swapped le)
  const __m128i res = _mm_and_si128(
    _mm_or_si128(b, _mm_xor_si128(a, _mm_set1_epi32(-1))),
    _mm_or_si128(_mm_xor_si128(b, a),
                 _mm_xor_si128(_mm_sub_epi64(a, b), _mm_set1_epi32(-1))));
  // result in highest bit of res
  // spread highest bit to all bits
  const __m128i spread32 = _mm_srai_epi32(res, 31);
  return _mm_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmpge(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return _mm_cmpge_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmpge(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return _mm_cmpge_pd(a, b);
}

// ---------------------------------------------------------------------------
// compare !=
// ---------------------------------------------------------------------------

// there is no cmpneq for integers and no not, so use cmpeq and xor with all
// ones to invert the result

static SIMD_INLINE Vec<Byte, 16> cmpneq(const Vec<Byte, 16> &a,
                                        const Vec<Byte, 16> &b)
{
  return _mm_xor_si128(_mm_cmpeq_epi8(a, b), _mm_set1_epi32(-1));
}

static SIMD_INLINE Vec<SignedByte, 16> cmpneq(const Vec<SignedByte, 16> &a,
                                              const Vec<SignedByte, 16> &b)
{
  return _mm_xor_si128(_mm_cmpeq_epi8(a, b), _mm_set1_epi32(-1));
}

static SIMD_INLINE Vec<Word, 16> cmpneq(const Vec<Word, 16> &a,
                                        const Vec<Word, 16> &b)
{
  return _mm_xor_si128(_mm_cmpeq_epi16(a, b), _mm_set1_epi32(-1));
}

static SIMD_INLINE Vec<Short, 16> cmpneq(const Vec<Short, 16> &a,
                                         const Vec<Short, 16> &b)
{
  return _mm_xor_si128(_mm_cmpeq_epi16(a, b), _mm_set1_epi32(-1));
}

static SIMD_INLINE Vec<Int, 16> cmpneq(const Vec<Int, 16> &a,
                                       const Vec<Int, 16> &b)
{
  return _mm_xor_si128(_mm_cmpeq_epi32(a, b), _mm_set1_epi32(-1));
}

static SIMD_INLINE Vec<Long, 16> cmpneq(const Vec<Long, 16> &a,
                                        const Vec<Long, 16> &b)
{
#ifdef __SSE4_1__
  return _mm_xor_si128(_mm_cmpeq_epi64(a, b), _mm_set1_epi32(-1));
#else
  const __m128i eq32        = _mm_cmpeq_epi32(a, b);
  const __m128i shuffledRes = _mm_shuffle_epi32(eq32, _MM_SHUFFLE(2, 3, 0, 1));
  const __m128i eq64        = _mm_and_si128(eq32, shuffledRes);
  return _mm_xor_si128(eq64, _mm_set1_epi32(-1));
#endif
}

static SIMD_INLINE Vec<Float, 16> cmpneq(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b)
{
  return _mm_cmpneq_ps(a, b);
}

static SIMD_INLINE Vec<Double, 16> cmpneq(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b)
{
  return _mm_cmpneq_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_and
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> bit_and(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  return _mm_and_si128(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> bit_and(const Vec<Float, 16> &a,
                                          const Vec<Float, 16> &b)
{
  return _mm_and_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 16> bit_and(const Vec<Double, 16> &a,
                                           const Vec<Double, 16> &b)
{
  return _mm_and_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_or
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> bit_or(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  return _mm_or_si128(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> bit_or(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b)
{
  return _mm_or_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 16> bit_or(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b)
{
  return _mm_or_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_andnot
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> bit_andnot(const Vec<T, 16> &a,
                                         const Vec<T, 16> &b)
{
  return _mm_andnot_si128(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> bit_andnot(const Vec<Float, 16> &a,
                                             const Vec<Float, 16> &b)
{
  return _mm_andnot_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 16> bit_andnot(const Vec<Double, 16> &a,
                                              const Vec<Double, 16> &b)
{
  return _mm_andnot_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_xor
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> bit_xor(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  return _mm_xor_si128(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 16> bit_xor(const Vec<Float, 16> &a,
                                          const Vec<Float, 16> &b)
{
  return _mm_xor_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 16> bit_xor(const Vec<Double, 16> &a,
                                           const Vec<Double, 16> &b)
{
  return _mm_xor_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_not
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 16> bit_not(const Vec<T, 16> &a)
{
  // there is no _mm_not_si128, so xor with all 1's
  return _mm_xor_si128(a, _mm_set1_epi32(-1));
}

// float version
static SIMD_INLINE Vec<Float, 16> bit_not(const Vec<Float, 16> &a)
{
  // there is no _mm_not_ps, so xor with all 1's
  return _mm_xor_ps(a, _mm_castsi128_ps(_mm_set1_epi32(-1)));
}

// double version
static SIMD_INLINE Vec<Double, 16> bit_not(const Vec<Double, 16> &a)
{
  // there is no _mm_not_pd, so xor with all 1's
  return _mm_xor_pd(a, _mm_castsi128_pd(_mm_set1_epi32(-1)));
}

// ---------------------------------------------------------------------------
// avg: average with rounding up
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> avg(const Vec<Byte, 16> &a,
                                     const Vec<Byte, 16> &b)
{
  return _mm_avg_epu8(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<SignedByte, 16> avg(const Vec<SignedByte, 16> &a,
                                           const Vec<SignedByte, 16> &b)
{
  // from Agner Fog's VCL vectori128.h
  __m128i signbit = _mm_set1_epi32(0x80808080);
  __m128i a1      = _mm_xor_si128(a, signbit); // add 0x80
  __m128i b1      = _mm_xor_si128(b, signbit); // add 0x80
  __m128i m1      = _mm_avg_epu8(a1, b1);      // unsigned avg
  return _mm_xor_si128(m1, signbit);           // sub 0x80
}

static SIMD_INLINE Vec<Word, 16> avg(const Vec<Word, 16> &a,
                                     const Vec<Word, 16> &b)
{
  return _mm_avg_epu16(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Short, 16> avg(const Vec<Short, 16> &a,
                                      const Vec<Short, 16> &b)
{
  // from Agner Fog's VCL vectori128.h
  __m128i signbit = _mm_set1_epi32(0x80008000);
  __m128i a1      = _mm_xor_si128(a, signbit); // add 0x8000
  __m128i b1      = _mm_xor_si128(b, signbit); // add 0x8000
  __m128i m1      = _mm_avg_epu16(a1, b1);     // unsigned avg
  return _mm_xor_si128(m1, signbit);           // sub 0x8000
}

static SIMD_INLINE Vec<Int, 16> avg(const Vec<Int, 16> &a,
                                    const Vec<Int, 16> &b)
{
  // from Hacker's Delight, 2-5 Average of Two Integers:
  return _mm_sub_epi32(_mm_or_si128(a, b),
                       _mm_srai_epi32(_mm_xor_si128(a, b), 1));
}

static SIMD_INLINE Vec<Long, 16> avg(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // from Hacker's Delight, 2-5 Average of Two Integers:
  return _mm_sub_epi64(_mm_or_si128(a, b),
                       srai<1>(Vec<Long, 16>(_mm_xor_si128(a, b))));
}

// NOTE: Float version doesn't round!
static SIMD_INLINE Vec<Float, 16> avg(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  __m128 half = _mm_set1_ps(0.5f);
  return _mm_mul_ps(_mm_add_ps(a, b), half);
}

// NOTE: Double version doesn't round!
static SIMD_INLINE Vec<Double, 16> avg(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  __m128d half = _mm_set1_pd(0.5);
  return _mm_mul_pd(_mm_add_pd(a, b), half);
}

// ---------------------------------------------------------------------------
// test_all_zeros
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_zeros(const Vec<T, 16> &a)
{
  // reinterpret to int in case T is float
  const auto intA = reinterpret(a, OutputType<Int>());
#ifdef __SSE4_1__
  // 10. Oct 22 (Jonas Keller):
  // replaced unnecessary "_mm_cmpeq_epi8(a, a)" with "a"
  // return _mm_test_all_zeros(a, _mm_cmpeq_epi8(a, a));
  return _mm_test_all_zeros(intA, intA);
#else
  return (_mm_movemask_epi8(_mm_cmpeq_epi8(_mm_setzero_si128(), intA)) ==
          0xffff);
#endif
}

// ---------------------------------------------------------------------------
// test_all_ones
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_ones(const Vec<T, 16> &a)
{
  // reinterpret to int in case T is float
  const auto intA = reinterpret(a, OutputType<Int>());
#ifdef __SSE4_1__
  return _mm_test_all_ones(intA);
#else
  __m128i undef = _mm_undefined_si128();
  __m128i ones  = _mm_cmpeq_epi8(undef, undef);
  return _mm_movemask_epi8(_mm_cmpeq_epi8(ones, intA)) == 0xffff;
#endif
}

// ---------------------------------------------------------------------------
// reverse
// ---------------------------------------------------------------------------

// All reverse operations below are courtesy of Yannick Sander.
// modified

static SIMD_INLINE Vec<Byte, 16> reverse(const Vec<Byte, 16> &a)
{
  const __m128i mask =
    _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

  // supported by sse3 and higher
  // compat available
  // no faster instruction available in newer instruction sets
  return _mm_shuffle_epi8(a, mask);
}

static SIMD_INLINE Vec<SignedByte, 16> reverse(const Vec<SignedByte, 16> &a)
{
  const __m128i mask =
    _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

  return _mm_shuffle_epi8(a, mask);
}

static SIMD_INLINE Vec<Short, 16> reverse(const Vec<Short, 16> &a)
{
  const __m128i mask =
    _mm_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);

  return _mm_shuffle_epi8(a, mask);
}

static SIMD_INLINE Vec<Word, 16> reverse(const Vec<Word, 16> &a)
{
  const __m128i mask =
    _mm_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14);

  return _mm_shuffle_epi8(a, mask);
}

static SIMD_INLINE Vec<Int, 16> reverse(const Vec<Int, 16> &a)
{
  return _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));
}

static SIMD_INLINE Vec<Long, 16> reverse(const Vec<Long, 16> &a)
{
  return _mm_shuffle_epi32(a, _MM_SHUFFLE(1, 0, 3, 2));
}

static SIMD_INLINE Vec<Float, 16> reverse(const Vec<Float, 16> &a)
{
  return _mm_shuffle_ps(a, a, _MM_SHUFFLE(0, 1, 2, 3));
}

static SIMD_INLINE Vec<Double, 16> reverse(const Vec<Double, 16> &a)
{
  return _mm_shuffle_pd(a, a, _MM_SHUFFLE2(0, 1));
}

// ---------------------------------------------------------------------------
// msb2int
// ---------------------------------------------------------------------------

// 26. Aug 22 (Jonas Keller): added msb2int functions

static SIMD_INLINE uint64_t msb2int(const Vec<Byte, 16> &a)
{
  return _mm_movemask_epi8(a);
}

static SIMD_INLINE uint64_t msb2int(const Vec<SignedByte, 16> &a)
{
  return _mm_movemask_epi8(a);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Short, 16> &a)
{
  // move the upper bytes of the 8 shorts to the lower 8 bytes of the vector
  // and set the upper 8 bytes of to 0, so that _mm_movemask_epi8
  // can be used to extract the upper bit of each short
  const __m128i mask =
    _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, 15, 13, 11, 9, 7, 5, 3, 1);
  const __m128i shuffled = _mm_shuffle_epi8(a, mask);
  return _mm_movemask_epi8(shuffled);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Word, 16> &a)
{
  // move the upper bytes of the 8 words to the lower 8 bytes of the vector
  // and set the upper 8 bytes of to 0, so that _mm_movemask_epi8
  // can be used to extract the upper bit of each word
  const __m128i mask =
    _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, 15, 13, 11, 9, 7, 5, 3, 1);
  const __m128i shuffled = _mm_shuffle_epi8(a, mask);
  return _mm_movemask_epi8(shuffled);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Int, 16> &a)
{
  return _mm_movemask_ps(_mm_castsi128_ps(a));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Long, 16> &a)
{
  return _mm_movemask_pd(_mm_castsi128_pd(a));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Float, 16> &a)
{
  return _mm_movemask_ps(a);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Double, 16> &a)
{
  return _mm_movemask_pd(a);
}

// ---------------------------------------------------------------------------
// int2msb
// ---------------------------------------------------------------------------

// 06. Oct 22 (Jonas Keller): added int2msb functions

static SIMD_INLINE Vec<Byte, 16> int2msb(const uint64_t a, OutputType<Byte>,
                                         Integer<16>)
{
  // ssse3 version from https://stackoverflow.com/a/72899629
#ifdef __SSSE3__
  __m128i shuffleIndeces = _mm_set_epi64x(0x0101010101010101, 0);
  __m128i aVec = _mm_shuffle_epi8(_mm_cvtsi32_si128(a), shuffleIndeces);
#else
  __m128i maskLo = _mm_set_epi64x(0, 0xffffffffffffffff);
  __m128i aLo    = _mm_and_si128(maskLo, _mm_set1_epi8(a));
  __m128i aHi    = _mm_andnot_si128(maskLo, _mm_set1_epi8(a >> 8));
  __m128i aVec   = _mm_or_si128(aLo, aHi);
#endif
  __m128i sel      = _mm_set1_epi64x(0x8040201008040201);
  __m128i selected = _mm_and_si128(aVec, sel);
  __m128i result   = _mm_cmpeq_epi8(selected, sel);
  return _mm_and_si128(result, _mm_set1_epi8((int8_t) 0x80));
}

static SIMD_INLINE Vec<SignedByte, 16> int2msb(const uint64_t a,
                                               OutputType<SignedByte>,
                                               Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Byte>(), Integer<16>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 16> int2msb(const uint64_t a, OutputType<Short>,
                                          Integer<16>)
{
  __m128i aVec = _mm_set1_epi16(a);
  __m128i sel  = _mm_set_epi16(0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004,
                               0x0002, 0x0001);
  __m128i selected = _mm_and_si128(aVec, sel);
  __m128i result   = _mm_cmpeq_epi16(selected, sel);
  return _mm_and_si128(result, _mm_set1_epi16((int16_t) 0x8000));
}

static SIMD_INLINE Vec<Word, 16> int2msb(const uint64_t a, OutputType<Word>,
                                         Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Short>(), Integer<16>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 16> int2msb(const uint64_t a, OutputType<Int>,
                                        Integer<16>)
{
  __m128i aVec = _mm_set1_epi32(a);
  __m128i sel  = _mm_set_epi32(0x00000008, 0x00000004, 0x00000002, 0x00000001);
  __m128i selected = _mm_and_si128(aVec, sel);
  __m128i result   = _mm_cmpeq_epi32(selected, sel);
  return _mm_and_si128(result, _mm_set1_epi32(0x80000000));
}

static SIMD_INLINE Vec<Long, 16> int2msb(const uint64_t a, OutputType<Long>,
                                         Integer<16>)
{
  return _mm_set_epi64x((a & 2) ? 0x8000000000000000 : 0,
                        (a & 1) ? 0x8000000000000000 : 0);
}

static SIMD_INLINE Vec<Float, 16> int2msb(const uint64_t a, OutputType<Float>,
                                          Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Int>(), Integer<16>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 16> int2msb(const uint64_t a, OutputType<Double>,
                                           Integer<16>)
{
  return _mm_set_pd((a & 2) ? -0.0 : 0.0, (a & 1) ? -0.0 : 0.0);
}

// ---------------------------------------------------------------------------
// int2bits
// ---------------------------------------------------------------------------

// 09. Oct 22 (Jonas Keller): added int2bits functions

static SIMD_INLINE Vec<Byte, 16> int2bits(const uint64_t a, OutputType<Byte>,
                                          Integer<16>)
{
  // ssse3 version from https://stackoverflow.com/a/72899629
#ifdef __SSSE3__
  __m128i shuffleIndeces = _mm_set_epi64x(0x0101010101010101, 0);
  __m128i aVec = _mm_shuffle_epi8(_mm_cvtsi32_si128(a), shuffleIndeces);
#else
  __m128i maskLo = _mm_set_epi64x(0, 0xffffffffffffffff);
  __m128i aLo    = _mm_and_si128(maskLo, _mm_set1_epi8(a));
  __m128i aHi    = _mm_andnot_si128(maskLo, _mm_set1_epi8(a >> 8));
  __m128i aVec   = _mm_or_si128(aLo, aHi);
#endif
  __m128i sel      = _mm_set1_epi64x(0x8040201008040201);
  __m128i selected = _mm_and_si128(aVec, sel);
  return _mm_cmpeq_epi8(selected, sel);
}

static SIMD_INLINE Vec<SignedByte, 16> int2bits(const uint64_t a,
                                                OutputType<SignedByte>,
                                                Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Byte>(), Integer<16>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 16> int2bits(const uint64_t a, OutputType<Short>,
                                           Integer<16>)
{
  __m128i aVec = _mm_set1_epi16(a);
  __m128i sel  = _mm_set_epi16(0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004,
                               0x0002, 0x0001);
  __m128i selected = _mm_and_si128(aVec, sel);
  return _mm_cmpeq_epi16(selected, sel);
}

static SIMD_INLINE Vec<Word, 16> int2bits(const uint64_t a, OutputType<Word>,
                                          Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Short>(), Integer<16>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 16> int2bits(const uint64_t a, OutputType<Int>,
                                         Integer<16>)
{
  __m128i aVec = _mm_set1_epi32(a);
  __m128i sel  = _mm_set_epi32(0x00000008, 0x00000004, 0x00000002, 0x00000001);
  __m128i selected = _mm_and_si128(aVec, sel);
  return _mm_cmpeq_epi32(selected, sel);
}

static SIMD_INLINE Vec<Long, 16> int2bits(const uint64_t a, OutputType<Long>,
                                          Integer<16>)
{
  return _mm_set_epi64x((a & 2) ? -1 : 0, (a & 1) ? -1 : 0);
}

static SIMD_INLINE Vec<Float, 16> int2bits(const uint64_t a, OutputType<Float>,
                                           Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Int>(), Integer<16>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 16> int2bits(const uint64_t a,
                                            OutputType<Double>, Integer<16>)
{
  const auto trueVal = TypeInfo<Double>::trueval();
  return _mm_set_pd((a & 2) ? trueVal : 0.0, (a & 1) ? trueVal : 0.0);
}

// ---------------------------------------------------------------------------
// iota
// ---------------------------------------------------------------------------

// 30. Jan 23 (Jonas Keller): added iota

static SIMD_INLINE Vec<Byte, 16> iota(OutputType<Byte>, Integer<16>)
{
  return _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<SignedByte, 16> iota(OutputType<SignedByte>, Integer<16>)
{
  return _mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Short, 16> iota(OutputType<Short>, Integer<16>)
{
  return _mm_set_epi16(7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Word, 16> iota(OutputType<Word>, Integer<16>)
{
  return _mm_set_epi16(7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Int, 16> iota(OutputType<Int>, Integer<16>)
{
  return _mm_set_epi32(3, 2, 1, 0);
}

static SIMD_INLINE Vec<Long, 16> iota(OutputType<Long>, Integer<16>)
{
  return _mm_set_epi64x(1, 0);
}

static SIMD_INLINE Vec<Float, 16> iota(OutputType<Float>, Integer<16>)
{
  return _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f);
}

static SIMD_INLINE Vec<Double, 16> iota(OutputType<Double>, Integer<16>)
{
  return _mm_set_pd(1.0, 0.0);
}

} // namespace base
} // namespace internal
} // namespace simd

#endif

#endif // SIMD_VEC_BASE_IMPL_INTEL_16_H_

// ===========================================================================
//
// SIMDVecBaseImplIntel32.H --
// encapsulation for AVX/AVX2 Intel vector extensions
// inspired by Agner Fog's C++ Vector Class Library
// http://www.agner.org/optimize/#vectorclass
// (VCL License: GNU General Public License Version 3,
//  http://www.gnu.org/licenses/gpl-3.0.en.html)
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace
// 13. May 23 (Jonas Keller): added Double support

#ifndef SIMD_VEC_BASE_IMPL_INTEL_32_H_
#define SIMD_VEC_BASE_IMPL_INTEL_32_H_

#include <cstddef>
#include <cstdint>
#include <limits>
#include <type_traits>

#if defined(SIMDVEC_INTEL_ENABLE) && defined(_SIMD_VEC_32_AVAIL_) &&           \
  !defined(SIMDVEC_SANDBOX)

namespace simd {

// ===========================================================================
// NOTES:
//
// - setting zero inside the function is not inefficient, see:
//   http://stackoverflow.com/questions/26807285/...
//   ...are-static-static-local-sse-avx-variables-blocking-a-xmm-ymm-register
//
// - for some data types (Int, Float) there are no saturated versions
//   of add/sub instructions; in this case we use the unsaturated version;
//   the user is responsible to avoid overflows
//
// - _mm512_alignr_epi32/64 are *not* lane-oriented and could be a better
//   solution than the _epi8 version which *is* lane-oriented
//
// - should we replace set1 with broadcast? probably the compiler
//   generates broadcast anyhow? apparently not without -O3!
//
// - we could improve performance by using 256-bit instructions from
//   AVX512-VL (e.g. permute instructions); at the moment the idea is that
//   typically the widest vector width is used, so if AVX512 is available,
//   AVX/AVX2 would only rarely be used
//
// ===========================================================================

// ===========================================================================
// Vec integer specialization for AVX2
// ===========================================================================

// partial specialization for SIMD_WIDTH = 32
template <typename T>
class Vec<T, 32>
{
  __m256i ymm = _mm256_setzero_si256();

public:
  using Type                       = T;
  static constexpr size_t elements = 32 / sizeof(T);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 32;

  Vec() = default;
  Vec(const __m256i &x) { ymm = x; }
  Vec &operator=(const __m256i &x)
  {
    ymm = x;
    return *this;
  }
  operator __m256i() const { return ymm; }
  // for avx2 emulation
  Vec(const Vec<T, 16> &lo, const Vec<T, 16> &hi)
  {
    ymm = _mm256_set_m128i(hi, lo);
  }
  SIMD_INLINE Vec<T, 16> lo() const { return _mm256_castsi256_si128(ymm); }
  SIMD_INLINE Vec<T, 16> hi() const { return _mm256_extractf128_si256(ymm, 1); }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<T, bytes>, bytes>;
};

// ===========================================================================
// Vec float specialization for AVX
// ===========================================================================

template <>
class Vec<Float, 32>
{
  __m256 ymm = _mm256_setzero_ps();

public:
  using Type                       = Float;
  static constexpr size_t elements = 32 / sizeof(Float);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 32;

  Vec() = default;
  Vec(const __m256 &x) { ymm = x; }
  Vec &operator=(const __m256 &x)
  {
    ymm = x;
    return *this;
  }
  operator __m256() const { return ymm; }
  // for avx2 emulation
  Vec(const Vec<Float, 16> &lo, const Vec<Float, 16> &hi)
  {
    ymm = _mm256_set_m128(hi, lo);
  }
  SIMD_INLINE Vec<Float, 16> lo() const { return _mm256_castps256_ps128(ymm); }
  SIMD_INLINE Vec<Float, 16> hi() const
  {
    return _mm256_extractf128_ps(ymm, 1);
  }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<Float, bytes>, bytes>;
};

// ===========================================================================
// Vec double specialization for AVX
// ===========================================================================

template <>
class Vec<Double, 32>
{
  __m256d ymm;

public:
  using Type                       = Double;
  static constexpr size_t elements = 32 / sizeof(Double);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 32;

  Vec() = default;
  Vec(const __m256d &x) { ymm = x; }
  Vec &operator=(const __m256d &x)
  {
    ymm = x;
    return *this;
  }
  operator __m256d() const { return ymm; }
  // for avx2 emulation
  Vec(const Vec<Double, 16> &lo, const Vec<Double, 16> &hi)
  {
    ymm = _mm256_set_m128d(hi, lo);
  }
  SIMD_INLINE Vec<Double, 16> lo() const { return _mm256_castpd256_pd128(ymm); }
  SIMD_INLINE Vec<Double, 16> hi() const
  {
    return _mm256_extractf128_pd(ymm, 1);
  }
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  using allocator = simd_aligned_allocator<Vec<Double, bytes>, bytes>;
};

namespace internal {
namespace base {
// ===========================================================================
// auxiliary functions
// ===========================================================================

// These functions either wrap AVX intrinsics (e.g. to handle
// immediate arguments as template parameter), or switch between
// implementations with different AVX* extensions, or provide
// altered or additional functionality.
// Only for use in wrapper functions!

// 01. Apr 23 (Jonas Keller): removed some not really necessary internal
// wrapper functions and inlined them directly into where they were used

// ---------------------------------------------------------------------------
// swizzle_32_16: swizzling of 128-bit lanes (for swizzle)
// ---------------------------------------------------------------------------

// rearrange vectors such that lane-oriented processing finds the
// right vectors to combine in corresponding lanes
//
// example: (li,hi are lanes)
//
//      --v0- --v1- --v2-
// N=3: l0 h0 l1 h1 l2 h2
//      --       --
//         --       --
//            --       --
//  ->  l0 h1 h0 l2 l1 h2  (distance = 3 lanes)
//      a0 b1              I=0, a=v0, b=v1
//            a1 b0        I=1, a=v0, b=v1
//                  a0 b1  I=2, a=v1, b=v2
//
//      --v0- --v1- --v2- --v3-
// N=4: l0 h0 l1 h1 l2 h2 l3 h3
//      --          --
//         --          --
//            --          --
//               --          --
//  ->  l0 l2 h0 h2 l1 l3 h1 h3  (distance = 4 lanes)
//      a0 b0                    I=0, a=v0, b=v2
//            a1 b0              I=1, a=v0, b=v2
//                  a0 b1        I=2, a=v1, b=v3
//                        a1 b1  I=3, a=v1, b=v3

// primary template
template <size_t N, size_t I = 0>
struct Swizzle_32_16
{
  template <typename T>
  static SIMD_INLINE void _swizzle_32_16(const Vec<T, 32> vIn[N],
                                         Vec<T, 32> vOut[N])
  {
    // example: N=3                                         v     v
    // I=0: permute_32_16(vIn[0], vIn[1], _MM_SHUFFLE(0, 2+ 1, 0, 0));
    // I=1: permute_32_16(vIn[0], vIn[2], _MM_SHUFFLE(0, 2+ 0, 0, 1));
    // I=2: permute_32_16(vIn[1], vIn[2], _MM_SHUFFLE(0, 2+ 1, 0, 0));
    //
    // example: N=4:                                        v     v
    // I=0: permute_32_16(vIn[0], vIn[2], _MM_SHUFFLE(0, 2+ 0, 0, 0));
    // I=1: permute_32_16(vIn[0], vIn[2], _MM_SHUFFLE(0, 2+ 1, 0, 0));
    // I=2: permute_32_16(vIn[1], vIn[3], _MM_SHUFFLE(0, 2+ 0, 0, 1));
    // I=3: permute_32_16(vIn[1], vIn[3], _MM_SHUFFLE(0, 2+ 1, 0, 1));
    //
    // "2+" means: take from second vector
    vOut[I] =
      _mm256_permute2f128_si256(vIn[I / 2], vIn[(I + N) / 2],
                                _MM_SHUFFLE(0, (2 + (I + N) % 2), 0, (I % 2)));
    Swizzle_32_16<N, I + 1>::_swizzle_32_16(vIn, vOut);
  }

  // Float version
  static SIMD_INLINE void _swizzle_32_16(const Vec<Float, 32> vIn[N],
                                         Vec<Float, 32> vOut[N])
  {
    vOut[I] =
      _mm256_permute2f128_ps(vIn[I / 2], vIn[(I + N) / 2],
                             _MM_SHUFFLE(0, (2 + (I + N) % 2), 0, (I % 2)));
    Swizzle_32_16<N, I + 1>::_swizzle_32_16(vIn, vOut);
  }

  // Double version
  static SIMD_INLINE void _swizzle_32_16(const Vec<Double, 32> vIn[N],
                                         Vec<Double, 32> vOut[N])
  {
    vOut[I] =
      _mm256_permute2f128_pd(vIn[I / 2], vIn[(I + N) / 2],
                             _MM_SHUFFLE(0, (2 + (I + N) % 2), 0, (I % 2)));
    Swizzle_32_16<N, I + 1>::_swizzle_32_16(vIn, vOut);
  }
};

// termination
template <size_t N>
struct Swizzle_32_16<N, N>
{
  template <typename T>
  static SIMD_INLINE void _swizzle_32_16(const Vec<T, 32>[N], Vec<T, 32>[N])
  {}
};

// swizzle lanes (for implementation of swizzle functions)
// from Stan Melax: 3D Vector Normalization... (adapted)
template <size_t N, typename T>
static SIMD_INLINE void swizzle_32_16(const Vec<T, 32> vIn[N],
                                      Vec<T, 32> vOut[N])
{
  Swizzle_32_16<N>::_swizzle_32_16(vIn, vOut);
}

// ---------------------------------------------------------------------------
// alignr
// ---------------------------------------------------------------------------

// 21. Apr 23 (Jonas Keller): replaced IMM range handling via tag dispatch
// with static_assert, since we don't need the range handling anymore,
// we just assert that IMM is in range

template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr_epi8(__m256i h, __m256i l)
{
  //  2. Jul 18 (rm) BUGFIX: 64 -> 32 (2 lanes only, lane-oriented!)
  static_assert(COUNT < 32, "");
#ifdef __AVX2__
  return _mm256_alignr_epi8(h, l, COUNT);
#else
  // non-avx2 workaround
  // (easy since AVX2 instructions operate on lanes anyhow)
  return _mm256_set_m128i(_mm_alignr_epi8(_mm256_extractf128_si256(h, 1),
                                          _mm256_extractf128_si256(l, 1),
                                          COUNT),
                          _mm_alignr_epi8(_mm256_castsi256_si128(h),
                                          _mm256_castsi256_si128(l), COUNT));

#endif
}

// ---------------------------------------------------------------------------
// auxiliary function for right shift over full 32 byte
// ---------------------------------------------------------------------------

// (difficulty: _mm256_srli_si256 only works in 128-bit lanes)
// http://stackoverflow.com/questions/25248766/emulating-shifts-on-32-bytes-with-avx
// TODO: finer case distinction using permute4x64?

//  7. Jun 16 (rm): if replaced by tag dispatching
// (reason: all branches are compiles and at least icc complains
// about exceeded ranges in immediates)

// COUNT = 0
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_srli256_si256(__m256i a, Range<true, 0, 16>)
{
  return a;
}

// COUNT = 1..15
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_srli256_si256(__m256i a, Range<false, 0, 16>)
{
  // _MM_SHUFFLE(2,0, 0,1) = 0x81, MS-bit set -> setting elements to zero
  // higher lane set to zero (2,0), lower lane taken from higher lane (0,1)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLllllllll
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh (2,0) (0,1)
  __m256i _0h = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(2, 0, 0, 1));
  // e.g. COUNT=5
  // a:              HHHHHHHHhhhhhhhh LLLLLLLllllllll
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh
  // alignr H lane:  0000000000000000 HHHHHHHHhhhhhhh
  // selected:                  ----- -----------
  // alignr L lane:  HHHHHHHHhhhhhhhh LLLLLLLLlllllll
  // selected:                  ----- -----------
  // alignr:         00000HHHHHHHHhhh hhhhhLLLLLLLlll
  return x_mm256_alignr_epi8<COUNT>(_0h, a);
}

// COUNT = 16
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_srli256_si256(__m256i a, Range<true, 16, 32>)
{
  // _MM_SHUFFLE(2,0, 0,1) = 0x81, MS-bit set -> setting elements to zero
  // higher lane set to zero (2,0), lower lane taken from higher lane (0,1)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLllllllll
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh (2,0) (0,1)
  __m256i _0h = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(2, 0, 0, 1));
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh (2,0) (0,1)
  return _0h;
}

// COUNT = 17..31
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_srli256_si256(__m256i a,
                                                 Range<false, 16, 32>)
{
  // _MM_SHUFFLE(2,0, 0,1) = 0x81, MS-bit set -> setting elements to zero
  // higher lane set to zero (2,0), lower lane taken from higher lane (0,1)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLllllllll
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh (2,0) (0,1)
  __m256i _0h = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(2, 0, 0, 1));
  // e.g. COUNT=18 (18-16 = 2)
  // _0h:            0000000000000000 HHHHHHHhhhhhhhh
  // srli:           0000000000000000 00HHHHHHHHhhhhh
#ifdef __AVX2__
  return _mm256_srli_si256(_0h, COUNT - 16);
#else
  return _mm256_set_m128i(
    _mm_srli_si128(_mm256_extractf128_si256(_0h, 1), COUNT - 16),
    _mm_srli_si128(_mm256_castsi256_si128(_0h), COUNT - 16));
#endif
}

// COUNT >= 32
template <size_t, bool AT_LOW_LIM, size_t LOW_LIM_INCL, size_t UP_LIM_EXCL>
static SIMD_INLINE __m256i
x_mm256_srli256_si256(__m256i, Range<AT_LOW_LIM, LOW_LIM_INCL, UP_LIM_EXCL>)
{
  return _mm256_setzero_si256();
}

// hub
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_srli256_si256(__m256i a)
{
  return x_mm256_srli256_si256<COUNT>(a, SizeRange<COUNT, 16>());
}

// ---------------------------------------------------------------------------
// auxiliary function for left shift over full 32 bytes
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/25248766/
//        emulating-shifts-on-32-bytes-with-avx
// TODO: finer case distinction using permute4x64?

//  7. Jun 16 (rm): if replaced by tag dispatching
// (reason: all branches are compiles and at least icc complains
// about exceeded ranges in immediates)

// COUNT = 0
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_slli256_si256(__m256i a, Range<true, 0, 16>)
{
  return a;
}

// COUNT = 1..15
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_slli256_si256(__m256i a, Range<false, 0, 16>)
{
  // _MM_SHUFFLE(0,0, 2,0) = 0x08, MS-bit set -> setting elements to zero
  // higher lane taken from lower lane (0,0), lower lane set to zero (2,0)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLLllllllll
  // _l0:            LLLLLLLLllllllll 0000000000000000 (0,0) (2,0)
  __m256i _l0 = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(0, 0, 2, 0));
  // e.g. COUNT = 5: (16-5=11)
  // _l0:            LLLLLLLLllllllll 0000000000000000
  // a:              HHHHHHHHhhhhhhhh LLLLLLLLllllllll
  // alignr H lane:  HHHHHHHHhhhhhhhh LLLLLLLLllllllll
  // selected:            ----------- -----
  // alignr L lane:  LLLLLLLLllllllll 0000000000000000
  // selected:            ----------- -----
  // alignr:         HHHhhhhhhhhLLLLL LLLllllllll00000
  return x_mm256_alignr_epi8<16 - COUNT>(a, _l0);
}

// COUNT = 16
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_slli256_si256(__m256i a, Range<true, 16, 32>)
{
  // _MM_SHUFFLE(0,0, 2,0) = 0x08, MS-bit set -> setting elements to zero
  // higher lane taken from lower lane (0,0), lower lane set to zero (2,0)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLLllllllll
  // _l0:            LLLLLLLLllllllll 0000000000000000 (0,0) (2,0)
  __m256i _l0 = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(0, 0, 2, 0));
  // _l0:            LLLLLLLLllllllll 0000000000000000 (0,0) (2,0)
  return _l0;
}

// COUNT = 17..31
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_slli256_si256(__m256i a,
                                                 Range<false, 16, 32>)
{
  // _MM_SHUFFLE(0,0, 2,0) = 0x08, MS-bit set -> setting elements to zero
  // higher lane taken from lower lane (0,0), lower lane set to zero (2,0)
  // a:              HHHHHHHHhhhhhhhh LLLLLLLLllllllll
  // _l0:            LLLLLLLLllllllll 0000000000000000 (0,0) (2,0)
  __m256i _l0 = _mm256_permute2f128_si256(a, a, _MM_SHUFFLE(0, 0, 2, 0));
  // e.g. COUNT = 18 (18-16=2)
  // _l0:            LLLLLLLLllllllll 0000000000000000
  // slri:           LLLLLLllllllll00 0000000000000000
#ifdef __AVX2__
  return _mm256_slli_si256(_l0, COUNT - 16);
#else
  return _mm256_set_m128i(
    _mm_slli_si128(_mm256_extractf128_si256(_l0, 1), COUNT - 16),
    _mm_slli_si128(_mm256_castsi256_si128(_l0), COUNT - 16));
#endif
}

// COUNT >= 32
template <size_t, bool AT_LOW_LIM, size_t LOW_LIM_INCL, size_t UP_LIM_EXCL>
static SIMD_INLINE __m256i
x_mm256_slli256_si256(__m256i, Range<AT_LOW_LIM, LOW_LIM_INCL, UP_LIM_EXCL>)
{
  return _mm256_setzero_si256();
}

// hub
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_slli256_si256(__m256i a)
{
  return x_mm256_slli256_si256<COUNT>(a, SizeRange<COUNT, 16>());
}

// ---------------------------------------------------------------------------
// full 32 byte alignr ("alignr256")
// ---------------------------------------------------------------------------

// h:  HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
// l:  LLLLLLLLLLLLLLLL llllllllllllllll
//     000 HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL llllllllllllllll
// 0:                                        ---------------- ----------------
// 5:                                 ------ ---------------- ----------
// 16:                      ---------------- ----------------
// 18:                  --- ---------------- -------------
// 32:     ---------------- ----------------
// 35: --- ---------------- -------------

// modified from emmanualLattia at
// https://idz-smita-idzdev.ssgisp.com/fr-fr/forums/topic/500664

//  7. Jun 16 (rm): if replaced by tag dispatching
// (reason: all branches are compiles and at least icc complains
// about exceeded ranges in immediates)

// COUNT = 0
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i, __m256i low,
                                                  Range<true, 0, 16>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // COUNT == 0:        LLLLLLLLLLLLLLLL llllllllllllllll
  return low;
}

// COUNT = 1..15
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i low,
                                                  Range<false, 0, 16>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // high0low1:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL (0,2) (0,1)
  __m256i high0_low1 =
    _mm256_permute2f128_si256(low, high, _MM_SHUFFLE(0, 2, 0, 1));
  // e.g. COUNT = 5
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // high0low1:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL (0,2) (0,1)
  // alignr H lane:   hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL
  // selected:                   ----- -----------
  // alignr L lane:   LLLLLLLLLLLLLLLL llllllllllllllll
  // selected:                   ----- -----------
  // alignr:          hhhhhLLLLLLLLLLL LLLLLlllllllllll
  return x_mm256_alignr_epi8<COUNT>(high0_low1, low);
}

// COUNT = 16
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i low,
                                                  Range<true, 16, 32>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // high0low1:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL (0,2) (0,1)
  __m256i high0_low1 =
    _mm256_permute2f128_si256(low, high, _MM_SHUFFLE(0, 2, 0, 1));
  // COUNT == 16:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL
  return high0_low1;
}

// COUNT = 17..31
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i low,
                                                  Range<false, 16, 32>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // high0low1:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL (0,2) (0,1)
  __m256i high0_low1 =
    _mm256_permute2f128_si256(low, high, _MM_SHUFFLE(0, 2, 0, 1));
  // e.g. COUNT = 18 (COUNT - 16 = 2)
  // high0low1:       hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // alignr H lane:   HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // selected:                      -- --------------
  // alignr L lane:   hhhhhhhhhhhhhhhh LLLLLLLLLLLLLLLL
  // selected:                      -- --------------
  // alignr:          HHhhhhhhhhhhhhhh hhLLLLLLLLLLLLLL
  return x_mm256_alignr_epi8<COUNT - 16>(high, high0_low1);
}

// COUNT = 32
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i,
                                                  Range<true, 32, 48>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  //                  HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  return high;
}

// COUNT = 33..47
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i,
                                                  Range<false, 32, 48>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH (2,0) (0,1)
  __m256i null_high1 =
    _mm256_permute2f128_si256(high, high, _MM_SHUFFLE(2, 0, 0, 1));
  // e.g. COUNT = 37 (37-32 = 5)
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH
  // alignr H lane    0000000000000000 HHHHHHHHHHHHHHHH
  // selected:                   ----- -----------
  // alignr L lane    HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // selected:                   ----- -----------
  // alignr:          00000HHHHHHHHHHH HHHHHhhhhhhhhhhh
  return x_mm256_alignr_epi8<COUNT - 32>(null_high1, high);
}

// COUNT == 48
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i,
                                                  Range<true, 48, 64>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH (2,0) (0,1)
  __m256i null_high1 =
    _mm256_permute2f128_si256(high, high, _MM_SHUFFLE(2, 0, 0, 1));
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH
  return null_high1;
}

// COUNT = 49..63
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i,
                                                  Range<false, 48, 64>)
{
  // high:            HHHHHHHHHHHHHHHH hhhhhhhhhhhhhhhh
  // low:             LLLLLLLLLLLLLLLL llllllllllllllll
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH (2,0) (0,1)
  __m256i null_high1 =
    _mm256_permute2f128_si256(high, high, _MM_SHUFFLE(2, 0, 0, 1));
  // e.g. COUNT = 50 (50 - 48 = 2)
  // null_high1:      0000000000000000 HHHHHHHHHHHHHHHH
  // zero:            0000000000000000 0000000000000000
  // alignr H lane:   0000000000000000 0000000000000000
  // selected:                      -- --------------
  // alignr L lane:   0000000000000000 HHHHHHHHHHHHHHHH
  // selected:                      -- --------------
  // alignr:          0000000000000000 00HHHHHHHHHHHHHH
  return x_mm256_alignr_epi8<COUNT - 48>(_mm256_setzero_si256(), null_high1);
}

// COUNT >= 64
template <size_t COUNT, bool AT_LOW_LIM, size_t LOW_LIM_INCL,
          size_t UP_LIM_EXCL>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(
  __m256i, __m256i, Range<AT_LOW_LIM, LOW_LIM_INCL, UP_LIM_EXCL>)
{
  return _mm256_setzero_si256();
}

// hub
template <size_t COUNT>
static SIMD_INLINE __m256i x_mm256_alignr256_epi8(__m256i high, __m256i low)
{
  return x_mm256_alignr256_epi8<COUNT>(high, low, SizeRange<COUNT, 16>());
}

// ---------------------------------------------------------------------------
// insert 16 byte vector a into both lanes of a 32 byte vector
// ---------------------------------------------------------------------------

static SIMD_INLINE __m256i x_mm256_duplicate_si128(__m128i a)
{
  return _mm256_set_m128i(a, a);
}

// ---------------------------------------------------------------------------
// transpose4x64
// ---------------------------------------------------------------------------

// in  = Hh Hl Lh Ll
//        |   X   |
// out = Hh Lh Hl Ll

static SIMD_INLINE __m256i x_mm256_transpose4x64_epi64(__m256i a)
{
#ifdef __AVX2__
  return _mm256_permute4x64_epi64(a, _MM_SHUFFLE(3, 1, 2, 0));
#else
  // non-avx2 workarounds (different versions)

#if 1
  // non-avx2 workaround
  // (more efficient)

  __m256d in, x1, x2;
  // in = Hh Hl Lh Ll
  in = _mm256_castsi256_pd(a);
  // only lower 4 bit are used
  // in = Hh Hl Lh Ll
  //       0  1  0  1  = (0,0,1,1)
  // x1 = Hl Hh Ll Lh
  x1 = _mm256_permute_pd(in, _MM_SHUFFLE(0, 0, 1, 1));
  // all 8 bit are used
  // x1 = Hl Hh Ll Lh
  //       0  0  1  1
  // x2 = Ll Lh Hl Hh
  x2 = _mm256_permute2f128_pd(x1, x1, _MM_SHUFFLE(0, 0, 1, 1));
  // only lower 4 bit are used
  // in = Hh Hl Lh Ll
  // x2 = Ll Lh Hl Hh
  //       0  1  1  0 = (0,0,1,2)
  // ret: Hh Lh Hl Ll
  return _mm256_castpd_si256(_mm256_blend_pd(in, x2, _MM_SHUFFLE(0, 0, 1, 2)));
#else
  // non-avx2 workaround
  // (less efficient)

  __m128i lo    = _mm256_castsi256_si128(a);
  __m128i hi    = _mm256_extractf128_si256(a, 1);
  __m128i loRes = _mm_unpacklo_epi64(lo, hi);
  __m128i hiRes = _mm_unpackhi_epi64(lo, hi);
  return _mm256_set_m128i(hiRes, loRes);
#endif

#endif
}

static SIMD_INLINE __m256 x_mm256_transpose4x64_ps(__m256 a)
{
  return _mm256_castsi256_ps(
    x_mm256_transpose4x64_epi64(_mm256_castps_si256(a)));
}

static SIMD_INLINE __m256d x_mm256_transpose4x64_pd(__m256d a)
{
  return _mm256_castsi256_pd(
    x_mm256_transpose4x64_epi64(_mm256_castpd_si256(a)));
}

// ---------------------------------------------------------------------------
// unpack of 2 ps
// ---------------------------------------------------------------------------

static SIMD_INLINE __m256 x_mm256_unpacklo_2ps(__m256 a, __m256 b)
{
  return _mm256_castpd_ps(
    _mm256_unpacklo_pd(_mm256_castps_pd(a), _mm256_castps_pd(b)));
}

static SIMD_INLINE __m256 x_mm256_unpackhi_2ps(__m256 a, __m256 b)
{
  return _mm256_castpd_ps(
    _mm256_unpackhi_pd(_mm256_castps_pd(a), _mm256_castps_pd(b)));
}

// ---------------------------------------------------------------------------
// binary functions with non-avx2 workarounds
// ---------------------------------------------------------------------------

#ifdef __AVX2__
// avx2 is available
#define SIMDVEC_INTEL_X_INT_BINFCT_32(INTRIN)                                  \
  static SIMD_INLINE __m256i x_mm256_##INTRIN(__m256i a, __m256i b)            \
  {                                                                            \
    return _mm256_##INTRIN(a, b);                                              \
  }
#else
// non-avx2 workaround
#define SIMDVEC_INTEL_X_INT_BINFCT_32(INTRIN)                                  \
  static SIMD_INLINE __m256i x_mm256_##INTRIN(__m256i a, __m256i b)            \
  {                                                                            \
    return _mm256_set_m128i(                                                   \
      _mm_##INTRIN(_mm256_extractf128_si256(a, 1),                             \
                   _mm256_extractf128_si256(b, 1)),                            \
      _mm_##INTRIN(_mm256_castsi256_si128(a), _mm256_castsi256_si128(b)));     \
  }
#endif

SIMDVEC_INTEL_X_INT_BINFCT_32(unpacklo_epi8)
SIMDVEC_INTEL_X_INT_BINFCT_32(unpackhi_epi8)
SIMDVEC_INTEL_X_INT_BINFCT_32(unpacklo_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(unpackhi_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(shuffle_epi8)
SIMDVEC_INTEL_X_INT_BINFCT_32(packs_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(packs_epi32)
SIMDVEC_INTEL_X_INT_BINFCT_32(packus_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(packus_epi32)
SIMDVEC_INTEL_X_INT_BINFCT_32(hadd_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(hadd_epi32)
SIMDVEC_INTEL_X_INT_BINFCT_32(hadds_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(hsub_epi16)
SIMDVEC_INTEL_X_INT_BINFCT_32(hsub_epi32)
SIMDVEC_INTEL_X_INT_BINFCT_32(hsubs_epi16)

// non-avx2 workarounds via analogous ps, pd functions
#ifdef __AVX2__
// avx2 is available
#define SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(INTRIN, INTSUFFIX, PSPDSUFFIX)      \
  static SIMD_INLINE __m256i x_mm256_##INTRIN##_##INTSUFFIX(__m256i a,         \
                                                            __m256i b)         \
  {                                                                            \
    return _mm256_##INTRIN##_##INTSUFFIX(a, b);                                \
  }
#else
// non-avx2 workaround
#define SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(INTRIN, INTSUFFIX, PSPDSUFFIX)      \
  static SIMD_INLINE __m256i x_mm256_##INTRIN##_##INTSUFFIX(__m256i a,         \
                                                            __m256i b)         \
  {                                                                            \
    return _mm256_cast##PSPDSUFFIX##_si256(                                    \
      _mm256_##INTRIN##_##PSPDSUFFIX(_mm256_castsi256##_##PSPDSUFFIX(a),       \
                                     _mm256_castsi256##_##PSPDSUFFIX(b)));     \
  }
#endif

// better non-avx2 workarounds for unpacks (32, 64) via ps, pd
SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(unpacklo, epi32, ps)
SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(unpackhi, epi32, ps)
SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(unpacklo, epi64, pd)
SIMDVEC_INTEL_X_INT_BINFCT_PSPD_32(unpackhi, epi64, pd)

// ###########################################################################
// ###########################################################################
// ###########################################################################

// ===========================================================================
// Vec template function specializations or overloading for AVX
// ===========================================================================

// ---------------------------------------------------------------------------
// reinterpretation casts
// ---------------------------------------------------------------------------

// 08. Apr 23 (Jonas Keller): used enable_if for cleaner implementation

// between all integer types
template <typename Tdst, typename Tsrc,
          SIMD_ENABLE_IF((!std::is_same<Tdst, Tsrc>::value &&
                          std::is_integral<Tdst>::value &&
                          std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Tdst, 32> reinterpret(const Vec<Tsrc, 32> &vec,
                                             OutputType<Tdst>)
{
  // 26. Nov 22 (Jonas Keller): reinterpret_cast is technically undefined
  // behavior, so just rewrapping the vector register in a new Vec instead
  // return reinterpret_cast<const Vec<Tdst,32>&>(vec);
  return Vec<Tdst, 32>(__m256i(vec));
}

// from float to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 32> reinterpret(const Vec<Float, 32> &vec,
                                             OutputType<Tdst>)
{
  return _mm256_castps_si256(vec);
}

// from any integer type to float
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Float, 32> reinterpret(const Vec<Tsrc, 32> &vec,
                                              OutputType<Float>)
{
  return _mm256_castsi256_ps(vec);
}

// from double to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 32> reinterpret(const Vec<Double, 32> &vec,
                                             OutputType<Tdst>)
{
  return _mm256_castpd_si256(vec);
}

// from any integer type to double
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Double, 32> reinterpret(const Vec<Tsrc, 32> &vec,
                                               OutputType<Double>)
{
  return _mm256_castsi256_pd(vec);
}

// from float to double
static SIMD_INLINE Vec<Double, 32> reinterpret(const Vec<Float, 32> &vec,
                                               OutputType<Double>)
{
  return _mm256_castps_pd(vec);
}

// from double to float
static SIMD_INLINE Vec<Float, 32> reinterpret(const Vec<Double, 32> &vec,
                                              OutputType<Float>)
{
  return _mm256_castpd_ps(vec);
}

// between identical types
template <typename T>
static SIMD_INLINE Vec<T, 32> reinterpret(const Vec<T, 32> &vec, OutputType<T>)
{
  return vec;
}

// ---------------------------------------------------------------------------
// convert (without changes in the number of of elements)
// ---------------------------------------------------------------------------

// conversion with saturation; we wanted to have a fast solution that
// doesn't trigger the overflow which results in a negative two's
// complement result ("invalid int32": 0x80000000); therefore we clamp
// the positive values at the maximal positive float which is
// convertible to int32 without overflow (0x7fffffbf = 2147483520);
// negative values cannot overflow (they are clamped to invalid int
// which is the most negative int32)
static SIMD_INLINE Vec<Int, 32> cvts(const Vec<Float, 32> &a, OutputType<Int>)
{
  // TODO: analyze much more complex solution for cvts at
  // TODO: http://stackoverflow.com/questions/9157373/
  // TODO: most-efficient-way-to-convert-vector-of-float-to-vector-of-uint32
  __m256 clip = _mm256_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT32);
  return _mm256_cvtps_epi32(_mm256_min_ps(clip, a));
}

// saturation is not necessary in this case
static SIMD_INLINE Vec<Float, 32> cvts(const Vec<Int, 32> &a, OutputType<Float>)
{
  return _mm256_cvtepi32_ps(a);
}

static SIMD_INLINE Vec<Long, 32> cvts(const Vec<Double, 32> &a,
                                      OutputType<Long>)
{
  // _mm256_cvtpd_epi64 is only available with AVX512
  // using serial workaround instead
  Double tmpD[4] SIMD_ATTR_ALIGNED(32);
  _mm256_store_pd(tmpD, a);
  Long tmpL[4] SIMD_ATTR_ALIGNED(32);
  for (int i = 0; i < 4; ++i) {
    tmpL[i] =
      Long(std::rint(std::min(tmpD[i], MAX_POS_DOUBLE_CONVERTIBLE_TO_INT64)));
  }
  return _mm256_load_si256((__m256i *) tmpL);
}

static SIMD_INLINE Vec<Double, 32> cvts(const Vec<Long, 32> &a,
                                        OutputType<Double>)
{
#ifdef __AVX2__
  // workaround from https://stackoverflow.com/a/41148578 (modified)
  __m256i xH = _mm256_srai_epi32(a, 16);
  xH         = _mm256_and_si256(xH, _mm256_set1_epi64x(0xffffffff00000000));
  xH         = _mm256_add_epi64(
    xH, _mm256_castpd_si256(_mm256_set1_pd(442721857769029238784.))); // 3*2^67
  __m256i xL = _mm256_blend_epi16(
    a, _mm256_castpd_si256(_mm256_set1_pd(0x0010000000000000)), 0x88); //  2^52
  __m256d f =
    _mm256_sub_pd(_mm256_castsi256_pd(xH),
                  _mm256_set1_pd(442726361368656609280.)); //  3*2^67 + 2^52
  return _mm256_add_pd(f, _mm256_castsi256_pd(xL));
#else
  // non-avx2 workaround
  return Vec<Double, 32>(cvts(a.lo(), OutputType<Double>()),
                         cvts(a.hi(), OutputType<Double>()));
#endif
}

// ---------------------------------------------------------------------------
// setzero
// ---------------------------------------------------------------------------

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 32> setzero(OutputType<T>, Integer<32>)
{
  return _mm256_setzero_si256();
}

static SIMD_INLINE Vec<Float, 32> setzero(OutputType<Float>, Integer<32>)
{
  return _mm256_setzero_ps();
}

static SIMD_INLINE Vec<Double, 32> setzero(OutputType<Double>, Integer<32>)
{
  return _mm256_setzero_pd();
}

// ---------------------------------------------------------------------------
// set1
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 32> set1(Byte a, Integer<32>)
{
  return _mm256_set1_epi8(a);
}

static SIMD_INLINE Vec<SignedByte, 32> set1(SignedByte a, Integer<32>)
{
  return _mm256_set1_epi8(a);
}

static SIMD_INLINE Vec<Word, 32> set1(Word a, Integer<32>)
{
  return _mm256_set1_epi16(a);
}

static SIMD_INLINE Vec<Short, 32> set1(Short a, Integer<32>)
{
  return _mm256_set1_epi16(a);
}

static SIMD_INLINE Vec<Int, 32> set1(Int a, Integer<32>)
{
  return _mm256_set1_epi32(a);
}

static SIMD_INLINE Vec<Long, 32> set1(Long a, Integer<32>)
{
  return _mm256_set1_epi64x(a);
}

static SIMD_INLINE Vec<Float, 32> set1(Float a, Integer<32>)
{
  return _mm256_set1_ps(a);
}

static SIMD_INLINE Vec<Double, 32> set1(Double a, Integer<32>)
{
  return _mm256_set1_pd(a);
}

// ---------------------------------------------------------------------------
// load
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 32> load(const T *const p, Integer<32>)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  return _mm256_load_si256((__m256i *) p);
}

static SIMD_INLINE Vec<Float, 32> load(const Float *const p, Integer<32>)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  return _mm256_load_ps(p);
}

static SIMD_INLINE Vec<Double, 32> load(const Double *const p, Integer<32>)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  return _mm256_load_pd(p);
}

// ---------------------------------------------------------------------------
// loadu
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 32> loadu(const T *const p, Integer<32>)
{
  return _mm256_loadu_si256((__m256i *) p);
}

static SIMD_INLINE Vec<Float, 32> loadu(const Float *const p, Integer<32>)
{
  return _mm256_loadu_ps(p);
}

static SIMD_INLINE Vec<Double, 32> loadu(const Double *const p, Integer<32>)
{
  return _mm256_loadu_pd(p);
}

// ---------------------------------------------------------------------------
// store
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void store(T *const p, const Vec<T, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_store_si256((__m256i *) p, a);
}

// float version
static SIMD_INLINE void store(Float *const p, const Vec<Float, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_store_ps(p, a);
}

// double version
static SIMD_INLINE void store(Double *const p, const Vec<Double, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_store_pd(p, a);
}

// ---------------------------------------------------------------------------
// storeu
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void storeu(T *const p, const Vec<T, 32> &a)
{
  _mm256_storeu_si256((__m256i *) p, a);
}

// float version
static SIMD_INLINE void storeu(Float *const p, const Vec<Float, 32> &a)
{
  _mm256_storeu_ps(p, a);
}

// double version
static SIMD_INLINE void storeu(Double *const p, const Vec<Double, 32> &a)
{
  _mm256_storeu_pd(p, a);
}

// ---------------------------------------------------------------------------
// stream_store
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void stream_store(T *const p, const Vec<T, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_stream_si256((__m256i *) p, a);
}

// float version
static SIMD_INLINE void stream_store(Float *const p, const Vec<Float, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_stream_ps(p, a);
}

// double version
static SIMD_INLINE void stream_store(Double *const p, const Vec<Double, 32> &a)
{
  // AVX load and store instructions need alignment to 32 byte
  // (lower 5 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 32);
  _mm256_stream_pd(p, a);
}

// ---------------------------------------------------------------------------
// extract
// ---------------------------------------------------------------------------

template <size_t COUNT>
static SIMD_INLINE Byte extract(const Vec<Byte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    // strange, Intel intrinsics guide says this is AVX2, but it is already
    // available in avxintrin.h
    return _mm256_extract_epi8(a, COUNT);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE SignedByte extract(const Vec<SignedByte, 32> &a)
{
  return ::simd::internal::bit_cast<SignedByte>(
    extract<COUNT>(reinterpret(a, OutputType<Byte>())));
}

template <size_t COUNT>
static SIMD_INLINE Word extract(const Vec<Word, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    // strange, Intel intrinsics guide says this is AVX2, but it is already
    // available in avxintrin.h
    return _mm256_extract_epi16(a, COUNT);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Short extract(const Vec<Short, 32> &a)
{
  return ::simd::internal::bit_cast<Short>(
    extract<COUNT>(reinterpret(a, OutputType<Word>())));
}

template <size_t COUNT>
static SIMD_INLINE Int extract(const Vec<Int, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm256_extract_epi32(a, COUNT);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Long extract(const Vec<Long, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 4) {
    return _mm256_extract_epi64(a, COUNT);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Float extract(const Vec<Float, 32> &a)
{
  return ::simd::internal::bit_cast<Float>(
    extract<COUNT>(reinterpret(a, OutputType<Int>())));
}

template <size_t COUNT>
static SIMD_INLINE Double extract(const Vec<Double, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 4) {
    return ::simd::internal::bit_cast<Double>(
      _mm256_extract_epi64(_mm256_castpd_si256(a), COUNT));
  } else {
    return 0;
  }
}

// ---------------------------------------------------------------------------
// add
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> add(const Vec<Byte, 32> &a,
                                     const Vec<Byte, 32> &b)
{
  return _mm256_add_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> add(const Vec<SignedByte, 32> &a,
                                           const Vec<SignedByte, 32> &b)
{
  return _mm256_add_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> add(const Vec<Word, 32> &a,
                                     const Vec<Word, 32> &b)
{
  return _mm256_add_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 32> add(const Vec<Short, 32> &a,
                                      const Vec<Short, 32> &b)
{
  return _mm256_add_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> add(const Vec<Int, 32> &a,
                                    const Vec<Int, 32> &b)
{
  return _mm256_add_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 32> add(const Vec<Long, 32> &a,
                                     const Vec<Long, 32> &b)
{
  return _mm256_add_epi64(a, b);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> add(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(add(a.lo(), b.lo()), add(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> add(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_add_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> add(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// adds
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> adds(const Vec<Byte, 32> &a,
                                      const Vec<Byte, 32> &b)
{
  return _mm256_adds_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> adds(const Vec<SignedByte, 32> &a,
                                            const Vec<SignedByte, 32> &b)
{
  return _mm256_adds_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> adds(const Vec<Word, 32> &a,
                                      const Vec<Word, 32> &b)
{
  return _mm256_adds_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 32> adds(const Vec<Short, 32> &a,
                                       const Vec<Short, 32> &b)
{
  return _mm256_adds_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> adds(const Vec<Int, 32> &a,
                                     const Vec<Int, 32> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm256_adds_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  __m256i sum             = _mm256_add_epi32(a, b);
  __m256i opsHaveDiffSign = _mm256_xor_si256(a, b);
  __m256i sumHasDiffSign  = _mm256_xor_si256(a, sum);
  // indicates when an overflow has occurred
  __m256i overflow =
    _mm256_srai_epi32(_mm256_andnot_si256(opsHaveDiffSign, sumHasDiffSign), 31);
  // saturated sum for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  __m256i saturatedSum =
    _mm256_xor_si256(_mm256_srai_epi32(a, 31), _mm256_set1_epi32(0x7FFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return _mm256_or_si256(_mm256_andnot_si256(overflow, sum),
                         _mm256_and_si256(overflow, saturatedSum));
}

static SIMD_INLINE Vec<Long, 32> adds(const Vec<Long, 32> &a,
                                      const Vec<Long, 32> &b)
{
  // _mm256_adds_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  __m256i sum             = _mm256_add_epi64(a, b);
  __m256i opsHaveDiffSign = _mm256_xor_si256(a, b);
  __m256i sumHasDiffSign  = _mm256_xor_si256(a, sum);
  // indicates when an overflow has occurred
  __m256i overflow32 =
    _mm256_srai_epi32(_mm256_andnot_si256(opsHaveDiffSign, sumHasDiffSign), 31);
  // duplicate result to other half of 64 bit int
  __m256i overflow = _mm256_shuffle_epi32(overflow32, _MM_SHUFFLE(3, 3, 1, 1));
  // saturated sum for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0, 0x8000000000000000=min
  // negative long, when sign of a (and thus b as well) is 1)
  __m256i saturatedSum = _mm256_xor_si256(
    _mm256_shuffle_epi32(_mm256_srai_epi32(a, 31), _MM_SHUFFLE(3, 3, 1, 1)),
    _mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return _mm256_or_si256(_mm256_andnot_si256(overflow, sum),
                         _mm256_and_si256(overflow, saturatedSum));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> adds(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(adds(a.lo(), b.lo()), adds(a.hi(), b.hi()));
}

#endif

// Float not saturated
static SIMD_INLINE Vec<Float, 32> adds(const Vec<Float, 32> &a,
                                       const Vec<Float, 32> &b)
{
  return _mm256_add_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 32> adds(const Vec<Double, 32> &a,
                                        const Vec<Double, 32> &b)
{
  return _mm256_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// sub
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> sub(const Vec<Byte, 32> &a,
                                     const Vec<Byte, 32> &b)
{
  return _mm256_sub_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> sub(const Vec<SignedByte, 32> &a,
                                           const Vec<SignedByte, 32> &b)
{
  return _mm256_sub_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> sub(const Vec<Word, 32> &a,
                                     const Vec<Word, 32> &b)
{
  return _mm256_sub_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 32> sub(const Vec<Short, 32> &a,
                                      const Vec<Short, 32> &b)
{
  return _mm256_sub_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> sub(const Vec<Int, 32> &a,
                                    const Vec<Int, 32> &b)
{
  return _mm256_sub_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 32> sub(const Vec<Long, 32> &a,
                                     const Vec<Long, 32> &b)
{
  return _mm256_sub_epi64(a, b);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> sub(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(sub(a.lo(), b.lo()), sub(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> sub(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_sub_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> sub(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// subs
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> subs(const Vec<Byte, 32> &a,
                                      const Vec<Byte, 32> &b)
{
  return _mm256_subs_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> subs(const Vec<SignedByte, 32> &a,
                                            const Vec<SignedByte, 32> &b)
{
  return _mm256_subs_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> subs(const Vec<Word, 32> &a,
                                      const Vec<Word, 32> &b)
{
  return _mm256_subs_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 32> subs(const Vec<Short, 32> &a,
                                       const Vec<Short, 32> &b)
{
  return _mm256_subs_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> subs(const Vec<Int, 32> &a,
                                     const Vec<Int, 32> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm256_subs_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  __m256i diff            = _mm256_sub_epi32(a, b);
  __m256i opsHaveDiffSign = _mm256_xor_si256(a, b);
  __m256i diffHasDiffSign = _mm256_xor_si256(a, diff);
  // indicates when an overflow has occurred
  __m256i overflow =
    _mm256_srai_epi32(_mm256_and_si256(opsHaveDiffSign, diffHasDiffSign), 31);
  // saturated diff for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  __m256i saturatedDiff =
    _mm256_xor_si256(_mm256_srai_epi32(a, 31), _mm256_set1_epi32(0x7FFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return _mm256_or_si256(_mm256_andnot_si256(overflow, diff),
                         _mm256_and_si256(overflow, saturatedDiff));
}

static SIMD_INLINE Vec<Long, 32> subs(const Vec<Long, 32> &a,
                                      const Vec<Long, 32> &b)
{
  // _mm256_subs_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  __m256i diff            = _mm256_sub_epi64(a, b);
  __m256i opsHaveDiffSign = _mm256_xor_si256(a, b);
  __m256i diffHasDiffSign = _mm256_xor_si256(a, diff);
  // indicates when an overflow has occurred
  __m256i overflow32 =
    _mm256_srai_epi32(_mm256_and_si256(opsHaveDiffSign, diffHasDiffSign), 31);
  // duplicate result to other half of 64 bit int
  __m256i overflow = _mm256_shuffle_epi32(overflow32, _MM_SHUFFLE(3, 3, 1, 1));
  // saturated diff for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0, 0x8000000000000000=min
  // negative long, when sign of a (and thus b as well) is 1)
  __m256i saturatedDiff = _mm256_xor_si256(
    _mm256_shuffle_epi32(_mm256_srai_epi32(a, 31), _MM_SHUFFLE(3, 3, 1, 1)),
    _mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return _mm256_or_si256(_mm256_andnot_si256(overflow, diff),
                         _mm256_and_si256(overflow, saturatedDiff));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> subs(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(subs(a.lo(), b.lo()), subs(a.hi(), b.hi()));
}

#endif

// Float not saturated
static SIMD_INLINE Vec<Float, 32> subs(const Vec<Float, 32> &a,
                                       const Vec<Float, 32> &b)
{
  return _mm256_sub_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 32> subs(const Vec<Double, 32> &a,
                                        const Vec<Double, 32> &b)
{
  return _mm256_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// neg (negate = two's complement or unary minus), only signed types
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<SignedByte, 32> neg(const Vec<SignedByte, 32> &a)
{
  return _mm256_sub_epi8(_mm256_setzero_si256(), a);
}

static SIMD_INLINE Vec<Short, 32> neg(const Vec<Short, 32> &a)
{
  return _mm256_sub_epi16(_mm256_setzero_si256(), a);
}

static SIMD_INLINE Vec<Int, 32> neg(const Vec<Int, 32> &a)
{
  return _mm256_sub_epi32(_mm256_setzero_si256(), a);
}

static SIMD_INLINE Vec<Long, 32> neg(const Vec<Long, 32> &a)
{
  return _mm256_sub_epi64(_mm256_setzero_si256(), a);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> neg(const Vec<T, 32> &a)
{
  return Vec<T, 32>(neg(a.lo()), neg(a.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> neg(const Vec<Float, 32> &a)
{
  return _mm256_sub_ps(_mm256_setzero_ps(), a);
}

static SIMD_INLINE Vec<Double, 32> neg(const Vec<Double, 32> &a)
{
  return _mm256_sub_pd(_mm256_setzero_pd(), a);
}

// ---------------------------------------------------------------------------
// min
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> min(const Vec<Byte, 32> &a,
                                     const Vec<Byte, 32> &b)
{
  return _mm256_min_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> min(const Vec<SignedByte, 32> &a,
                                           const Vec<SignedByte, 32> &b)
{
  return _mm256_min_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> min(const Vec<Word, 32> &a,
                                     const Vec<Word, 32> &b)
{
  return _mm256_min_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 32> min(const Vec<Short, 32> &a,
                                      const Vec<Short, 32> &b)
{
  return _mm256_min_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> min(const Vec<Int, 32> &a,
                                    const Vec<Int, 32> &b)
{
  return _mm256_min_epi32(a, b);
}

// there is an unsigned version of min for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 32> min(const Vec<Long, 32> &a,
                                     const Vec<Long, 32> &b)
{
  // from Hacker's Delight, 2-12 Comparison Predicates: (swapped lt)
  const __m256i diff = _mm256_sub_epi64(b, a);
#if 1 // TODO: check which is faster
  const __m256i res = _mm256_xor_si256(
    diff, _mm256_and_si256(_mm256_xor_si256(b, a), _mm256_xor_si256(diff, b)));
#else
  const __m256i res =
    _mm256_or_si256(_mm256_andnot_si256(a, b),
                    _mm256_andnot_si256(_mm256_xor_si256(b, a), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m256i spread32 = _mm256_srai_epi32(res, 31);
  const __m256i gt = _mm256_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));

  // blend a and b according to gt
  return _mm256_blendv_epi8(a, b, gt);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> min(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(min(a.lo(), b.lo()), min(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> min(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_min_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> min(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_min_pd(a, b);
}

// ---------------------------------------------------------------------------
// max
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> max(const Vec<Byte, 32> &a,
                                     const Vec<Byte, 32> &b)
{
  return _mm256_max_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> max(const Vec<SignedByte, 32> &a,
                                           const Vec<SignedByte, 32> &b)
{
  return _mm256_max_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> max(const Vec<Word, 32> &a,
                                     const Vec<Word, 32> &b)
{
  return _mm256_max_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 32> max(const Vec<Short, 32> &a,
                                      const Vec<Short, 32> &b)
{
  return _mm256_max_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> max(const Vec<Int, 32> &a,
                                    const Vec<Int, 32> &b)
{
  return _mm256_max_epi32(a, b);
}

// there is an unsigned version of max for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 32> max(const Vec<Long, 32> &a,
                                     const Vec<Long, 32> &b)
{
  // from Hacker's Delight, 2-12 Comparison Predicates: (swapped lt)
  const __m256i diff = _mm256_sub_epi64(b, a);
#if 1 // TODO: check which is faster
  const __m256i res = _mm256_xor_si256(
    diff, _mm256_and_si256(_mm256_xor_si256(b, a), _mm256_xor_si256(diff, b)));
#else
  const __m256i res =
    _mm256_or_si256(_mm256_andnot_si256(a, b),
                    _mm256_andnot_si256(_mm256_xor_si256(b, a), diff));
#endif
  // result in highest bit of res
  // spread highest bit to all bits
  const __m256i spread32 = _mm256_srai_epi32(res, 31);
  const __m256i gt = _mm256_shuffle_epi32(spread32, _MM_SHUFFLE(3, 3, 1, 1));

  // blend a and b according to gt
  return _mm256_blendv_epi8(b, a, gt);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> max(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(max(a.lo(), b.lo()), max(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> max(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_max_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> max(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_max_pd(a, b);
}

// ---------------------------------------------------------------------------
// mul, div
// ---------------------------------------------------------------------------

// TODO: add mul/div versions for int types? or make special versions of mul
// TODO: and div where the result is scaled?

static SIMD_INLINE Vec<Float, 32> mul(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_mul_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> mul(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_mul_pd(a, b);
}

static SIMD_INLINE Vec<Float, 32> div(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_div_ps(a, b);
}

static SIMD_INLINE Vec<Double, 32> div(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_div_pd(a, b);
}

// ---------------------------------------------------------------------------
// ceil, floor, round, truncate
// ---------------------------------------------------------------------------

// 25. Mar 23 (Jonas Keller): added versions for integer types

// versions for integer types do nothing:

template <typename T>
static SIMD_INLINE Vec<T, 32> ceil(const Vec<T, 32> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 32> floor(const Vec<T, 32> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 32> round(const Vec<T, 32> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 32> truncate(const Vec<T, 32> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

static SIMD_INLINE Vec<Float, 32> ceil(const Vec<Float, 32> &a)
{
  return _mm256_ceil_ps(a);
}

static SIMD_INLINE Vec<Double, 32> ceil(const Vec<Double, 32> &a)
{
  return _mm256_ceil_pd(a);
}

static SIMD_INLINE Vec<Float, 32> floor(const Vec<Float, 32> &a)
{
  return _mm256_floor_ps(a);
}

static SIMD_INLINE Vec<Double, 32> floor(const Vec<Double, 32> &a)
{
  return _mm256_floor_pd(a);
}

static SIMD_INLINE Vec<Float, 32> round(const Vec<Float, 32> &a)
{
  // old: use _MM_SET_ROUNDING_MODE to adjust rounding direction
  // return _mm256_round_ps(a, _MM_FROUND_CUR_DIRECTION);
  // new  4. Aug 16 (rm): round to nearest, and suppress exceptions
  return _mm256_round_ps(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 32> round(const Vec<Double, 32> &a)
{
  return _mm256_round_pd(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 32> truncate(const Vec<Float, 32> &a)
{
  return _mm256_round_ps(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 32> truncate(const Vec<Double, 32> &a)
{
  return _mm256_round_pd(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// ---------------------------------------------------------------------------
// elementary mathematical functions
// ---------------------------------------------------------------------------

// estimate of a reciprocal
static SIMD_INLINE Vec<Float, 32> rcp(const Vec<Float, 32> &a)
{
  return _mm256_rcp_ps(a);
}

static SIMD_INLINE Vec<Double, 32> rcp(const Vec<Double, 32> &a)
{
  // _mm256_rcp_pd does not exist
  return Vec<Double, 32>(rcp(a.lo()), rcp(a.hi()));
}

// estimate of reverse square root
static SIMD_INLINE Vec<Float, 32> rsqrt(const Vec<Float, 32> &a)
{
  return _mm256_rsqrt_ps(a);
}

static SIMD_INLINE Vec<Double, 32> rsqrt(const Vec<Double, 32> &a)
{
  // _mm256_rsqrt_pd does not exist
  return Vec<Double, 32>(rsqrt(a.lo()), rsqrt(a.hi()));
}

// square root
static SIMD_INLINE Vec<Float, 32> sqrt(const Vec<Float, 32> &a)
{
  return _mm256_sqrt_ps(a);
}

static SIMD_INLINE Vec<Double, 32> sqrt(const Vec<Double, 32> &a)
{
  return _mm256_sqrt_pd(a);
}

// ---------------------------------------------------------------------------
// abs
// ---------------------------------------------------------------------------

// 25. Mar 25 (Jonas Keller): added abs for unsigned integers

// unsigned integers
template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 32> abs(const Vec<T, 32> &a)
{
  return a;
}

static SIMD_INLINE Vec<SignedByte, 32> abs(const Vec<SignedByte, 32> &a)
{
#ifdef __AVX2__
  return _mm256_abs_epi8(a);
#else
  // non-avx2 workaround
  return Vec<SignedByte, 32>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Short, 32> abs(const Vec<Short, 32> &a)
{
#ifdef __AVX2__
  return _mm256_abs_epi16(a);
#else
  // non-avx2 workaround
  return Vec<Short, 32>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Int, 32> abs(const Vec<Int, 32> &a)
{
#ifdef __AVX2__
  return _mm256_abs_epi32(a);
#else
  // non-avx2 workaround
  return Vec<Int, 32>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Long, 32> abs(const Vec<Long, 32> &a)
{
#ifdef __AVX2__
  // _mm256_abs_epi64 is only supported in avx512
  // from Hacker's Delight, 2-4 Absolute Value Function:
  const __m256i signMask =
    _mm256_shuffle_epi32(_mm256_srai_epi32(a, 31), _MM_SHUFFLE(3, 3, 1, 1));
  return _mm256_sub_epi64(_mm256_xor_si256(a, signMask), signMask);
#else
  // non-avx2 workaround
  return Vec<Long, 32>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Float, 32> abs(const Vec<Float, 32> &a)
{
  // there's no _mm256_abs_ps, we have to emulated it:
  // -0.0F is 0x8000000, 0x7fffffff by andnot, sign bit is cleared
  return _mm256_andnot_ps(_mm256_set1_ps(-0.0F), a);
}

static SIMD_INLINE Vec<Double, 32> abs(const Vec<Double, 32> &a)
{
  // there's no _mm256_abs_pd, we have to emulated it:
  // -0.0 is 0x8000000000000000, 0x7fffffffffffffff by andnot, sign bit is
  // cleared
  return _mm256_andnot_pd(_mm256_set1_pd(-0.0), a);
}

// ---------------------------------------------------------------------------
// unpacklo
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<0>, Bytes<1>)
{
  return x_mm256_unpacklo_epi8(x_mm256_transpose4x64_epi64(a),
                               x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<0>, Bytes<2>)
{
  return x_mm256_unpacklo_epi16(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<0>, Bytes<4>)
{
  return x_mm256_unpacklo_epi32(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<0>, Bytes<8>)
{
  return x_mm256_unpacklo_epi64(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<0>, Bytes<16>)
{
  return _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<0>,
                                         Bytes<4>)
{
  return _mm256_unpacklo_ps(x_mm256_transpose4x64_ps(a),
                            x_mm256_transpose4x64_ps(b));
}

// float versions
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<0>,
                                         Bytes<8>)
{
  return x_mm256_unpacklo_2ps(x_mm256_transpose4x64_ps(a),
                              x_mm256_transpose4x64_ps(b));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<0>,
                                         Bytes<16>)
{
  return _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b, Part<0>,
                                          Bytes<8>)
{
  return _mm256_unpacklo_pd(x_mm256_transpose4x64_pd(a),
                            x_mm256_transpose4x64_pd(b));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b, Part<0>,
                                          Bytes<16>)
{
  return _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// ---------------------------------------------------------------------------
// unpackhi
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<1>, Bytes<1>)
{
  return x_mm256_unpackhi_epi8(x_mm256_transpose4x64_epi64(a),
                               x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<1>, Bytes<2>)
{
  return x_mm256_unpackhi_epi16(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<1>, Bytes<4>)
{
  return x_mm256_unpackhi_epi32(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<1>, Bytes<8>)
{
  return x_mm256_unpackhi_epi64(x_mm256_transpose4x64_epi64(a),
                                x_mm256_transpose4x64_epi64(b));
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                     Part<1>, Bytes<16>)
{
  return _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<1>,
                                         Bytes<4>)
{
  return _mm256_unpackhi_ps(x_mm256_transpose4x64_ps(a),
                            x_mm256_transpose4x64_ps(b));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<1>,
                                         Bytes<8>)
{
  return x_mm256_unpackhi_2ps(x_mm256_transpose4x64_ps(a),
                              x_mm256_transpose4x64_ps(b));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b, Part<1>,
                                         Bytes<16>)
{
  return _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b, Part<1>,
                                          Bytes<8>)
{
  return _mm256_unpackhi_pd(x_mm256_transpose4x64_pd(a),
                            x_mm256_transpose4x64_pd(b));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b, Part<1>,
                                          Bytes<16>)
{
  return _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// ---------------------------------------------------------------------------
// 16-byte-lane oriented unpacklo
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<0>, Bytes<1>)
{
  return x_mm256_unpacklo_epi8(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<0>, Bytes<2>)
{
  return x_mm256_unpacklo_epi16(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<0>, Bytes<4>)
{
  return x_mm256_unpacklo_epi32(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<0>, Bytes<8>)
{
  return x_mm256_unpacklo_epi64(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<0>, Bytes<16>)
{
  return _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<0>,
                                           Bytes<4>)
{
  return _mm256_unpacklo_ps(a, b);
}

// float versions
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<0>,
                                           Bytes<8>)
{
  return x_mm256_unpacklo_2ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<0>,
                                           Bytes<16>)
{
  return _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack16(const Vec<Double, 32> &a,
                                            const Vec<Double, 32> &b, Part<0>,
                                            Bytes<8>)
{
  return _mm256_unpacklo_pd(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack16(const Vec<Double, 32> &a,
                                            const Vec<Double, 32> &b, Part<0>,
                                            Bytes<16>)
{
  return _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 2, 0, 0));
}

// ---------------------------------------------------------------------------
// 128-bit-lane oriented unpackhi
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<1>, Bytes<1>)
{
  return x_mm256_unpackhi_epi8(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<1>, Bytes<2>)
{
  return x_mm256_unpackhi_epi16(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<1>, Bytes<4>)
{
  return x_mm256_unpackhi_epi32(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<1>, Bytes<8>)
{
  return x_mm256_unpackhi_epi64(a, b);
}

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> unpack16(const Vec<T, 32> &a, const Vec<T, 32> &b,
                                       Part<1>, Bytes<16>)
{
  return _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<1>,
                                           Bytes<4>)
{
  return _mm256_unpackhi_ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<1>,
                                           Bytes<8>)
{
  return x_mm256_unpackhi_2ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 32> unpack16(const Vec<Float, 32> &a,
                                           const Vec<Float, 32> &b, Part<1>,
                                           Bytes<16>)
{
  return _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack16(const Vec<Double, 32> &a,
                                            const Vec<Double, 32> &b, Part<1>,
                                            Bytes<8>)
{
  return _mm256_unpackhi_pd(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> unpack16(const Vec<Double, 32> &a,
                                            const Vec<Double, 32> &b, Part<1>,
                                            Bytes<16>)
{
  return _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// ---------------------------------------------------------------------------
// extract 128-bit-lane as Vec<T, 16>
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// generalized extract of 128-bit-lanes
// LANE_INDEX=0: first lane of input vector,
// LANE_INDEX=1: second lane of input vector
template <size_t LANE_INDEX, typename T>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, 32> &a)
{
  const auto intA           = reinterpret(a, OutputType<Int>());
  const Vec<Int, 16> intRes = _mm256_extractf128_si256(intA, LANE_INDEX);
  return reinterpret(intRes, OutputType<T>());
}

// ---------------------------------------------------------------------------
// zip
// ---------------------------------------------------------------------------

// a, b are passed by-value to avoid problems with identical
// input/output args.

// here we typically have to transpose the inputs in the same way
// for both output computations, so we define separate functions for
// all T and Bytes<> (combinations of unpack functions above)

// all integer versions
template <typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h, Bytes<1>)
{
  __m256i at = x_mm256_transpose4x64_epi64(a);
  __m256i bt = x_mm256_transpose4x64_epi64(b);
  l          = x_mm256_unpacklo_epi8(at, bt);
  h          = x_mm256_unpackhi_epi8(at, bt);
}

// all integer versions
template <typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h, Bytes<2>)
{
  __m256i at = x_mm256_transpose4x64_epi64(a);
  __m256i bt = x_mm256_transpose4x64_epi64(b);
  l          = x_mm256_unpacklo_epi16(at, bt);
  h          = x_mm256_unpackhi_epi16(at, bt);
}

// all integer versions
template <typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h, Bytes<4>)
{
  __m256i at = x_mm256_transpose4x64_epi64(a);
  __m256i bt = x_mm256_transpose4x64_epi64(b);
  l          = x_mm256_unpacklo_epi32(at, bt);
  h          = x_mm256_unpackhi_epi32(at, bt);
}

// all integer versions
template <typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h, Bytes<8>)
{
  __m256i at = x_mm256_transpose4x64_epi64(a);
  __m256i bt = x_mm256_transpose4x64_epi64(b);
  l          = x_mm256_unpacklo_epi64(at, bt);
  h          = x_mm256_unpackhi_epi64(at, bt);
}

// all integer versions
template <typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h, Bytes<16>)
{
  l = _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_si256(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// float version
static SIMD_INLINE void zip(const Vec<Float, 32> a, const Vec<Float, 32> b,
                            Vec<Float, 32> &l, Vec<Float, 32> &h, Bytes<4>)
{
  __m256 at = x_mm256_transpose4x64_ps(a);
  __m256 bt = x_mm256_transpose4x64_ps(b);
  l         = _mm256_unpacklo_ps(at, bt);
  h         = _mm256_unpackhi_ps(at, bt);
}

// float version
static SIMD_INLINE void zip(const Vec<Float, 32> a, const Vec<Float, 32> b,
                            Vec<Float, 32> &l, Vec<Float, 32> &h, Bytes<8>)
{
  __m256 at = x_mm256_transpose4x64_ps(a);
  __m256 bt = x_mm256_transpose4x64_ps(b);
  l         = x_mm256_unpacklo_2ps(at, bt);
  h         = x_mm256_unpackhi_2ps(at, bt);
}

// float version
static SIMD_INLINE void zip(const Vec<Float, 32> a, const Vec<Float, 32> b,
                            Vec<Float, 32> &l, Vec<Float, 32> &h, Bytes<16>)
{
  l = _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_ps(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// double version
static SIMD_INLINE void zip(const Vec<Double, 32> a, const Vec<Double, 32> b,
                            Vec<Double, 32> &l, Vec<Double, 32> &h, Bytes<8>)
{
  __m256d at = x_mm256_transpose4x64_pd(a);
  __m256d bt = x_mm256_transpose4x64_pd(b);
  l          = _mm256_unpacklo_pd(at, bt);
  h          = _mm256_unpackhi_pd(at, bt);
}

// double version
static SIMD_INLINE void zip(const Vec<Double, 32> a, const Vec<Double, 32> b,
                            Vec<Double, 32> &l, Vec<Double, 32> &h, Bytes<16>)
{
  l = _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_pd(a, b, _MM_SHUFFLE(0, 3, 0, 1));
}

// ---------------------------------------------------------------------------
// zip hub
// ---------------------------------------------------------------------------

// zips blocks of NUM_ELEMS elements of type T
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip(const Vec<T, 32> a, const Vec<T, 32> b,
                            Vec<T, 32> &l, Vec<T, 32> &h)
{
  return zip(a, b, l, h, Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// zip16 hub (16-byte-lane oriented zip)
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// zips blocks of NUM_ELEMS elements of type T
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip16(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h)
{
  l = unpack16(a, b, Part<0>(), Bytes<NUM_ELEMS * sizeof(T)>());
  h = unpack16(a, b, Part<1>(), Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// unzip
// ---------------------------------------------------------------------------

// a, b are passed by-value to avoid problems with identical input/output args.

// here we typically have to transpose the inputs in the same way
// for both output computations, so we define separate functions for
// all T and Bytes<> (combinations of unpack functions above)

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h, Bytes<1>)
{
  // mask is hopefully only set once if unzip is used multiple times
  const __m256i mask =
    _mm256_set_epi8(15, 13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8, 6, 4, 2, 0, 15,
                    13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m256i atmp =
    x_mm256_transpose4x64_epi64(x_mm256_shuffle_epi8(a, mask));
  const __m256i btmp =
    x_mm256_transpose4x64_epi64(x_mm256_shuffle_epi8(b, mask));
  l = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h, Bytes<2>)
{
  // mask is hopefully only set once if unzip is used multiple times
  const __m256i mask =
    _mm256_set_epi8(15, 14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5, 4, 1, 0, 15,
                    14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5, 4, 1, 0);
  const __m256i atmp =
    x_mm256_transpose4x64_epi64(x_mm256_shuffle_epi8(a, mask));
  const __m256i btmp =
    x_mm256_transpose4x64_epi64(x_mm256_shuffle_epi8(b, mask));
  l = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h, Bytes<4>)
{
#ifdef __AVX2__
  const __m256i aShuffled = _mm256_shuffle_epi32(a, _MM_SHUFFLE(3, 1, 2, 0));
  const __m256i bShuffled = _mm256_shuffle_epi32(b, _MM_SHUFFLE(3, 1, 2, 0));
#else
  const __m256i aShuffled = _mm256_castps_si256(_mm256_shuffle_ps(
    _mm256_castsi256_ps(a), _mm256_castsi256_ps(a), _MM_SHUFFLE(3, 1, 2, 0)));
  const __m256i bShuffled = _mm256_castps_si256(_mm256_shuffle_ps(
    _mm256_castsi256_ps(b), _mm256_castsi256_ps(b), _MM_SHUFFLE(3, 1, 2, 0)));
#endif
  const __m256i atmp = x_mm256_transpose4x64_epi64(aShuffled);
  const __m256i btmp = x_mm256_transpose4x64_epi64(bShuffled);
  l = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// all integer versions
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h, Bytes<8>)
{
  const __m256i atmp = x_mm256_transpose4x64_epi64(a);
  const __m256i btmp = x_mm256_transpose4x64_epi64(b);
  l = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_si256(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// all types
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h, Bytes<16>)
{
  l = unpack(a, b, Part<0>(), Bytes<16>());
  h = unpack(a, b, Part<1>(), Bytes<16>());
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 32> a, const Vec<Float, 32> b,
                              Vec<Float, 32> &l, Vec<Float, 32> &h, Bytes<4>)
{
  const __m256 atmp =
    x_mm256_transpose4x64_ps(_mm256_shuffle_ps(a, a, _MM_SHUFFLE(3, 1, 2, 0)));
  const __m256 btmp =
    x_mm256_transpose4x64_ps(_mm256_shuffle_ps(b, b, _MM_SHUFFLE(3, 1, 2, 0)));
  l = _mm256_permute2f128_ps(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_ps(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 32> a, const Vec<Float, 32> b,
                              Vec<Float, 32> &l, Vec<Float, 32> &h, Bytes<8>)
{
  const __m256 atmp = x_mm256_transpose4x64_ps(a);
  const __m256 btmp = x_mm256_transpose4x64_ps(b);
  l = _mm256_permute2f128_ps(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_ps(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// double version
static SIMD_INLINE void unzip(const Vec<Double, 32> a, const Vec<Double, 32> b,
                              Vec<Double, 32> &l, Vec<Double, 32> &h, Bytes<8>)
{
  const __m256d atmp = x_mm256_transpose4x64_pd(a);
  const __m256d btmp = x_mm256_transpose4x64_pd(b);
  l = _mm256_permute2f128_pd(atmp, btmp, _MM_SHUFFLE(0, 2, 0, 0));
  h = _mm256_permute2f128_pd(atmp, btmp, _MM_SHUFFLE(0, 3, 0, 1));
}

// ---------------------------------------------------------------------------
// unzip hub
// ---------------------------------------------------------------------------

// hub
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void unzip(const Vec<T, 32> a, const Vec<T, 32> b,
                              Vec<T, 32> &l, Vec<T, 32> &h)
{
  return unzip(a, b, l, h, Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// packs
// ---------------------------------------------------------------------------

// ========== signed -> signed ==========

static SIMD_INLINE Vec<SignedByte, 32> packs(const Vec<Short, 32> &a,
                                             const Vec<Short, 32> &b,
                                             OutputType<SignedByte>)
{
  return x_mm256_transpose4x64_epi64(x_mm256_packs_epi16(a, b));
}

static SIMD_INLINE Vec<Short, 32> packs(const Vec<Int, 32> &a,
                                        const Vec<Int, 32> &b,
                                        OutputType<Short>)
{
  return x_mm256_transpose4x64_epi64(x_mm256_packs_epi32(a, b));
}

static SIMD_INLINE Vec<Short, 32> packs(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b,
                                        OutputType<Short>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Short>());
}

static SIMD_INLINE Vec<Float, 32> packs(const Vec<Long, 32> &a,
                                        const Vec<Long, 32> &b,
                                        OutputType<Float>)
{
  // _mm256_cvtepi64_ps is not available in avx
  return _mm256_set_m128(_mm256_cvtpd_ps(cvts(b, OutputType<Double>())),
                         _mm256_cvtpd_ps(cvts(a, OutputType<Double>())));
}

static SIMD_INLINE Vec<Int, 32> packs(const Vec<Long, 32> &a,
                                      const Vec<Long, 32> &b, OutputType<Int>)
{
  // _mm256_packs_epi64 is not available in avx

#ifdef __AVX2__
  const auto maxClip = _mm256_set1_epi64x(0x000000007fffffff);
  const auto minClip = _mm256_set1_epi64x(0xffffffff80000000);
  const auto aSaturatedMin =
    _mm256_blendv_epi8(a, minClip, _mm256_cmpgt_epi64(minClip, a));
  const auto aSaturated =
    _mm256_blendv_epi8(aSaturatedMin, maxClip, _mm256_cmpgt_epi64(a, maxClip));
  const auto bSaturatedMin =
    _mm256_blendv_epi8(b, minClip, _mm256_cmpgt_epi64(minClip, b));
  const auto bSaturated =
    _mm256_blendv_epi8(bSaturatedMin, maxClip, _mm256_cmpgt_epi64(b, maxClip));
  return x_mm256_transpose4x64_epi64(_mm256_castps_si256(_mm256_shuffle_ps(
    _mm256_castsi256_ps(aSaturated), _mm256_castsi256_ps(bSaturated),
    _MM_SHUFFLE(2, 0, 2, 0))));
#else

  // vectorized workaround for when AVX2 is not available seems to be
  // complicated, so just using serial workaround
  // TODO: is there a better, vectorized workaround?

  Long input[8] SIMD_ATTR_ALIGNED(32);
  _mm256_store_si256((__m256i *) input, a);
  _mm256_store_si256((__m256i *) (input + 4), b);
  Int output[8] SIMD_ATTR_ALIGNED(32);
  for (int i = 0; i < 8; ++i) {
    output[i] =
      (Int) std::min(std::max(input[i], (Long) std::numeric_limits<Int>::min()),
                     (Long) std::numeric_limits<Int>::max());
  }
  return _mm256_load_si256((__m256i *) output);
#endif
}

static SIMD_INLINE Vec<Float, 32> packs(const Vec<Double, 32> &a,
                                        const Vec<Double, 32> &b,
                                        OutputType<Float>)
{
  return _mm256_set_m128(_mm256_cvtpd_ps(b), _mm256_cvtpd_ps(a));
}

static SIMD_INLINE Vec<Int, 32> packs(const Vec<Double, 32> &a,
                                      const Vec<Double, 32> &b, OutputType<Int>)
{
  const __m256d clip = _mm256_set1_pd(std::numeric_limits<Int>::max());
  return _mm256_set_m128i(_mm256_cvtpd_epi32(_mm256_min_pd(clip, b)),
                          _mm256_cvtpd_epi32(_mm256_min_pd(clip, a)));
}

// ========== unsigned -> unsigned ==========

static SIMD_INLINE Vec<Byte, 32> packs(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b, OutputType<Byte>)
{
#ifdef __AVX2__
  // _mm256_packus_epu16 does not exist, so saturate inputs to byte range and
  // then use _mm256_packus_epi16
  return x_mm256_transpose4x64_epi64(
    _mm256_packus_epi16(_mm256_min_epu16(a, _mm256_set1_epi16(0xff)),
                        _mm256_min_epu16(b, _mm256_set1_epi16(0xff))));
#else
  return x_mm256_transpose4x64_epi64(
    Vec<Byte, 32>(packs(a.lo(), b.lo(), OutputType<Byte>()),
                  packs(a.hi(), b.hi(), OutputType<Byte>())));
#endif
}

// ========== signed -> unsigned ==========

// non-avx2 workaround
static SIMD_INLINE Vec<Byte, 32> packs(const Vec<Short, 32> &a,
                                       const Vec<Short, 32> &b,
                                       OutputType<Byte>)
{
  return x_mm256_transpose4x64_epi64(x_mm256_packus_epi16(a, b));
}

// non-avx2 workaround
static SIMD_INLINE Vec<Word, 32> packs(const Vec<Int, 32> &a,
                                       const Vec<Int, 32> &b, OutputType<Word>)
{
  return x_mm256_transpose4x64_epi64(x_mm256_packus_epi32(a, b));
}

static SIMD_INLINE Vec<Word, 32> packs(const Vec<Float, 32> &a,
                                       const Vec<Float, 32> &b,
                                       OutputType<Word>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Word>());
}

// ========== unsigned -> signed ==========
static SIMD_INLINE Vec<SignedByte, 32> packs(const Vec<Word, 32> &a,
                                             const Vec<Word, 32> &b,
                                             OutputType<SignedByte>)
{
#ifdef __AVX2__
  // _mm256_packs_epu16 does not exist, so saturate inputs to signed byte range
  // and then use _mm256_packs_epi16
  return x_mm256_transpose4x64_epi64(
    _mm256_packs_epi16(_mm256_min_epu16(a, _mm256_set1_epi16(0x7f)),
                       _mm256_min_epu16(b, _mm256_set1_epi16(0x7f))));
#else
  return x_mm256_transpose4x64_epi64(
    Vec<SignedByte, 32>(packs(a.lo(), b.lo(), OutputType<SignedByte>()),
                        packs(a.hi(), b.hi(), OutputType<SignedByte>())));
#endif
}

// ---------------------------------------------------------------------------
// generalized extend: no stage
// ---------------------------------------------------------------------------

// combinations:
// - signed   -> extended signed (sign extension)
// - unsigned -> extended unsigned (zero extension)
// - unsigned -> extended signed (zero extension)
// - signed   -> extended unsigned (saturation and zero extension)

// 7. Aug 16 (rm):
// tried to remove this to SIMDVecExt.H, but then we get ambiguities with
// non-avx2 workaround

// same types
template <typename T>
static SIMD_INLINE void extend(const Vec<T, 32> &vIn, Vec<T, 32> vOut[1])
{
  vOut[0] = vIn;
}

// same size, different types

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Byte, 32> vOut[1])
{
  vOut[0] = max(vIn, Vec<SignedByte, 32>(_mm256_setzero_si256()));
}

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn,
                               Vec<SignedByte, 32> vOut[1])
{
  vOut[0] = min(vIn, Vec<Byte, 32>(_mm256_set1_epi8(0x7f)));
}

static SIMD_INLINE void extend(const Vec<Short, 32> &vIn, Vec<Word, 32> vOut[1])
{
  vOut[0] = max(vIn, Vec<Short, 32>(_mm256_setzero_si256()));
}

static SIMD_INLINE void extend(const Vec<Word, 32> &vIn, Vec<Short, 32> vOut[1])
{
  vOut[0] = min(vIn, Vec<Word, 32>(_mm256_set1_epi16(0x7fff)));
}

// ---------------------------------------------------------------------------
// generalized extend: single stage
// ---------------------------------------------------------------------------

#ifdef __AVX2__

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Short, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepi8_epi16(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Short, 32> &vIn, Vec<Int, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepi16_epi32(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Short, 32> &vIn,
                               Vec<Float, 32> vOut[2])
{
  vOut[0] =
    _mm256_cvtepi32_ps(_mm256_cvtepi16_epi32(_mm256_castsi256_si128(vIn)));
  vOut[1] =
    _mm256_cvtepi32_ps(_mm256_cvtepi16_epi32(_mm256_extractf128_si256(vIn, 1)));
}

static SIMD_INLINE void extend(const Vec<Int, 32> &vIn, Vec<Long, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepi32_epi64(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepi32_epi64(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Int, 32> &vIn, Vec<Double, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepi32_pd(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepi32_pd(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Float, 32> &vIn, Vec<Long, 32> vOut[2])
{
  // _mm256_cvtps_epi64 is not available in avx
  const auto clipped =
    _mm256_min_ps(vIn, _mm256_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT64));
  vOut[0] =
    cvts(_mm256_cvtps_pd(_mm256_castps256_ps128(clipped)), OutputType<Long>());
  vOut[1] = cvts(_mm256_cvtps_pd(_mm256_extractf128_ps(clipped, 1)),
                 OutputType<Long>());
}

static SIMD_INLINE void extend(const Vec<Float, 32> &vIn,
                               Vec<Double, 32> vOut[2])
{
  vOut[0] = _mm256_cvtps_pd(_mm256_castps256_ps128(vIn));
  vOut[1] = _mm256_cvtps_pd(_mm256_extractf128_ps(vIn, 1));
}

// unsigned -> unsigned

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn, Vec<Word, 32> vOut[2])
{
  // there's no _mm256_cvtepu8_epu16()
  Vec<Byte, 32> zero = setzero(OutputType<Byte>(), Integer<32>());
  // 16. Jul 16 (rm): here we avoid to use generalized unpack from
  // SIMDVecExt.H
  vOut[0] = unpack(vIn, zero, Part<0>(), Bytes<1>());
  vOut[1] = unpack(vIn, zero, Part<1>(), Bytes<1>());
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn, Vec<Short, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepu8_epi16(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepu8_epi16(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Word, 32> &vIn, Vec<Int, 32> vOut[2])
{
  vOut[0] = _mm256_cvtepu16_epi32(_mm256_castsi256_si128(vIn));
  vOut[1] = _mm256_cvtepu16_epi32(_mm256_extractf128_si256(vIn, 1));
}

static SIMD_INLINE void extend(const Vec<Word, 32> &vIn, Vec<Float, 32> vOut[2])
{
  vOut[0] =
    _mm256_cvtepi32_ps(_mm256_cvtepu16_epi32(_mm256_castsi256_si128(vIn)));
  vOut[1] =
    _mm256_cvtepi32_ps(_mm256_cvtepu16_epi32(_mm256_extractf128_si256(vIn, 1)));
}

// signed -> unsigned

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Word, 32> vOut[2])
{
  // there's no _mm256_cvtepi8_epu16()
  const Vec<SignedByte, 32> saturated =
    _mm256_max_epi8(vIn, _mm256_setzero_si256());
  const Vec<SignedByte, 32> zero = _mm256_setzero_si256();
  vOut[0] = unpack(saturated, zero, Part<0>(), Bytes<1>());
  vOut[1] = unpack(saturated, zero, Part<1>(), Bytes<1>());
}

// ---------------------------------------------------------------------------
// generalized extend: two stages
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Int, 32> vOut[4])
{
  __m128i vInLo128 = _mm256_castsi256_si128(vIn);
  vOut[0]          = _mm256_cvtepi8_epi32(vInLo128);
  vOut[1]          = _mm256_cvtepi8_epi32(_mm_srli_si128(vInLo128, 8));
  __m128i vInHi128 = _mm256_extractf128_si256(vIn, 1);
  vOut[2]          = _mm256_cvtepi8_epi32(vInHi128);
  vOut[3]          = _mm256_cvtepi8_epi32(_mm_srli_si128(vInHi128, 8));
}

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Float, 32> vOut[4])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  for (size_t i = 0; i < 4; i++) vOut[i] = cvts(vTmp[i], OutputType<Float>());
}

static SIMD_INLINE void extend(const Vec<Short, 32> &vIn, Vec<Long, 32> vOut[4])
{
  Vec<Int, 32> vTmp[2];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<Short, 32> &vIn,
                               Vec<Double, 32> vOut[4])
{
  Vec<Int, 32> vTmp[2];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn, Vec<Int, 32> vOut[4])
{
  __m128i vInLo128 = _mm256_castsi256_si128(vIn);
  vOut[0]          = _mm256_cvtepu8_epi32(vInLo128);
  vOut[1]          = _mm256_cvtepu8_epi32(_mm_srli_si128(vInLo128, 8));
  __m128i vInHi128 = _mm256_extractf128_si256(vIn, 1);
  vOut[2]          = _mm256_cvtepu8_epi32(vInHi128);
  vOut[3]          = _mm256_cvtepu8_epi32(_mm_srli_si128(vInHi128, 8));
}

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn, Vec<Float, 32> vOut[4])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  for (size_t i = 0; i < 4; i++) vOut[i] = cvts(vTmp[i], OutputType<Float>());
}

static SIMD_INLINE void extend(const Vec<Word, 32> &vIn, Vec<Long, 32> vOut[4])
{
  Vec<Int, 32> vTmp[2];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<Word, 32> &vIn,
                               Vec<Double, 32> vOut[4])
{
  Vec<Int, 32> vTmp[2];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
}

// ---------------------------------------------------------------------------
// generalized extend: three stages
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Long, 32> vOut[8])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
  extend(vTmp[2], vOut + 4);
  extend(vTmp[3], vOut + 6);
}

static SIMD_INLINE void extend(const Vec<SignedByte, 32> &vIn,
                               Vec<Double, 32> vOut[8])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
  extend(vTmp[2], vOut + 4);
  extend(vTmp[3], vOut + 6);
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn, Vec<Long, 32> vOut[8])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
  extend(vTmp[2], vOut + 4);
  extend(vTmp[3], vOut + 6);
}

static SIMD_INLINE void extend(const Vec<Byte, 32> &vIn,
                               Vec<Double, 32> vOut[8])
{
  Vec<Int, 32> vTmp[4];
  extend(vIn, vTmp);
  extend(vTmp[0], vOut);
  extend(vTmp[1], vOut + 2);
  extend(vTmp[2], vOut + 4);
  extend(vTmp[3], vOut + 6);
}

#else // __AVX2__

// ---------------------------------------------------------------------------
// generalized extend: non-avx2 workaround
// ---------------------------------------------------------------------------

// non-avx2 workaround
template <typename Tout, typename Tin,
          SIMD_ENABLE_IF(sizeof(Tout) > sizeof(Tin))>
static SIMD_INLINE void extend(const Vec<Tin, 32> &vIn,
                               Vec<Tout, 32> vOut[sizeof(Tout) / sizeof(Tin)])
{
  const size_t nOut = sizeof(Tout) / sizeof(Tin), nOutHalf = nOut / 2;
  Vec<Tout, 16> vOutLo16[nOut], vOutHi16[nOut];
  extend(vIn.lo(), vOutLo16);
  extend(vIn.hi(), vOutHi16);
  for (size_t i = 0; i < nOutHalf; i++) {
    vOut[i]            = Vec<Tout, 32>(vOutLo16[2 * i], vOutLo16[2 * i + 1]);
    vOut[i + nOutHalf] = Vec<Tout, 32>(vOutHi16[2 * i], vOutHi16[2 * i + 1]);
  }
}

#endif

// ---------------------------------------------------------------------------
// generalized extend: special case int <-> float, long <-> double
// ---------------------------------------------------------------------------

template <typename Tout, typename Tin,
          SIMD_ENABLE_IF(sizeof(Tin) == sizeof(Tout)),
          SIMD_ENABLE_IF(std::is_floating_point<Tin>::value !=
                         std::is_floating_point<Tout>::value)>
static SIMD_INLINE void extend(const Vec<Tin, 32> &vIn, Vec<Tout, 32> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Tout>());
}

// ---------------------------------------------------------------------------
// srai
// ---------------------------------------------------------------------------

#ifdef __AVX2__
// 16. Oct 22 (Jonas Keller): added missing Byte and SignedByte versions

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 32> srai(const Vec<Byte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    const __m256i odd  = _mm256_srai_epi16(a, COUNT);
    const __m256i even = _mm256_srai_epi16(_mm256_slli_epi16(a, 8), COUNT + 8);
    return _mm256_blendv_epi8(even, odd, _mm256_set1_epi16((int16_t) 0xff00));
  } else {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 32> srai(const Vec<SignedByte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    const __m256i odd  = _mm256_srai_epi16(a, COUNT);
    const __m256i even = _mm256_srai_epi16(_mm256_slli_epi16(a, 8), COUNT + 8);
    return _mm256_blendv_epi8(even, odd, _mm256_set1_epi16((int16_t) 0xff00));
  } else {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 32> srai(const Vec<Word, 32> &a)
{
  return _mm256_srai_epi16(a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 32> srai(const Vec<Short, 32> &a)
{
  return _mm256_srai_epi16(a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 32> srai(const Vec<Int, 32> &a)
{
  return _mm256_srai_epi32(a, vec::min(COUNT, 31ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 32> srai(const Vec<Long, 32> &a)
{ // workaround from Hacker's Delight, 2–17 Double-Length Shifts, Shift right
  // double signed:
  const __m256i odd = _mm256_srai_epi32(a, vec::min(COUNT, 31ul));
  __m256i even;
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    even =
      _mm256_or_si256(_mm256_srli_epi32(a, COUNT),
                      _mm256_slli_epi32(_mm256_srli_si256(a, 4), 32 - COUNT));
  } else {
    even =
      _mm256_srai_epi32(_mm256_srli_si256(a, 4), vec::min(COUNT - 32, 31ul));
  }
  return _mm256_blend_epi16(even, odd, 0xcc);
}

#else

// non-avx2 workaround
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> srai(const Vec<T, 32> &a)
{
  return Vec<T, 32>(srai<COUNT>(a.lo()), srai<COUNT>(a.hi()));
}

#endif

// ---------------------------------------------------------------------------
// srli
// ---------------------------------------------------------------------------

#ifdef __AVX2__

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 32> srli(const Vec<Byte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm256_and_si256(_mm256_set1_epi8((int8_t) (0xff >> COUNT)),
                            _mm256_srli_epi32(a, COUNT));
  } else {
    return _mm256_setzero_si256();
  }
}

// https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
// License: not specified
template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 32> srli(const Vec<SignedByte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm256_and_si256(_mm256_set1_epi8((int8_t) (0xff >> COUNT)),
                            _mm256_srli_epi32(a, COUNT));
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 32> srli(const Vec<Word, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm256_srli_epi16(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 32> srli(const Vec<Short, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm256_srli_epi16(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 32> srli(const Vec<Int, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm256_srli_epi32(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 32> srli(const Vec<Long, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm256_srli_epi64(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

#else

// non-avx2 workaround
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> srli(const Vec<T, 32> &a)
{
  return Vec<T, 32>(srli<COUNT>(a.lo()), srli<COUNT>(a.hi()));
}

#endif

// ---------------------------------------------------------------------------
// slli
// ---------------------------------------------------------------------------

#ifdef __AVX2__

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 32> slli(const Vec<Byte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm256_and_si256(
      _mm256_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm256_slli_epi32(a, COUNT));
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 32> slli(const Vec<SignedByte, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm256_and_si256(
      _mm256_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm256_slli_epi32(a, COUNT));
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 32> slli(const Vec<Word, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm256_slli_epi16(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 32> slli(const Vec<Short, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm256_slli_epi16(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 32> slli(const Vec<Int, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm256_slli_epi32(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 32> slli(const Vec<Long, 32> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm256_slli_epi64(a, COUNT);
  } else {
    return _mm256_setzero_si256();
  }
}

#else

// non-avx2 workaround
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> slli(const Vec<T, 32> &a)
{
  return Vec<T, 32>(slli<COUNT>(a.lo()), slli<COUNT>(a.hi()));
}

#endif

// 19. Dec 22 (Jonas Keller): added sra, srl and sll functions

// ---------------------------------------------------------------------------
// sra
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> sra(const Vec<Byte, 32> &a,
                                     const uint8_t count)
{
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);
  }
  const __m256i odd = _mm256_sra_epi16(a, _mm_cvtsi32_si128(count));
  const __m256i even =
    _mm256_sra_epi16(_mm256_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  return _mm256_blendv_epi8(even, odd, _mm256_set1_epi16((int16_t) 0xff00));
}

static SIMD_INLINE Vec<SignedByte, 32> sra(const Vec<SignedByte, 32> &a,
                                           const uint8_t count)
{
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm256_cmpgt_epi8(_mm256_setzero_si256(), a);
  }
  const __m256i odd = _mm256_sra_epi16(a, _mm_cvtsi32_si128(count));
  const __m256i even =
    _mm256_sra_epi16(_mm256_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  return _mm256_blendv_epi8(even, odd, _mm256_set1_epi16((int16_t) 0xff00));
}

static SIMD_INLINE Vec<Word, 32> sra(const Vec<Word, 32> &a,
                                     const uint8_t count)
{
  return _mm256_sra_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 32> sra(const Vec<Short, 32> &a,
                                      const uint8_t count)
{
  return _mm256_sra_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 32> sra(const Vec<Int, 32> &a, const uint8_t count)
{
  return _mm256_sra_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 32> sra(const Vec<Long, 32> &a,
                                     const uint8_t count)
{
  // workaround from Hacker's Delight, 2–17 Double-Length Shifts, Shift right
  // double signed:
  const __m256i odd = _mm256_sra_epi32(a, _mm_cvtsi32_si128(count));
  __m256i even;
  if (count < 32) {
    even = _mm256_or_si256(
      _mm256_srl_epi32(a, _mm_cvtsi32_si128(count)),
      _mm256_sll_epi32(_mm256_srli_si256(a, 4), _mm_cvtsi32_si128(32 - count)));
  } else {
    even =
      _mm256_sra_epi32(_mm256_srli_si256(a, 4), _mm_cvtsi32_si128(count - 32));
  }
  return _mm256_blend_epi16(even, odd, 0xcc);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> sra(const Vec<T, 32> &a, const uint8_t count)
{
  return Vec<T, 32>(sra(a.lo(), count), sra(a.hi(), count));
}

#endif

// ---------------------------------------------------------------------------
// srl
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> srl(const Vec<Byte, 32> &a,
                                     const uint8_t count)
{
  return _mm256_and_si256(_mm256_srl_epi16(a, _mm_cvtsi32_si128(count)),
                          _mm256_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<SignedByte, 32> srl(const Vec<SignedByte, 32> &a,
                                           const uint8_t count)
{
  return _mm256_and_si256(_mm256_srl_epi16(a, _mm_cvtsi32_si128(count)),
                          _mm256_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<Word, 32> srl(const Vec<Word, 32> &a,
                                     const uint8_t count)
{
  return _mm256_srl_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 32> srl(const Vec<Short, 32> &a,
                                      const uint8_t count)
{
  return _mm256_srl_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 32> srl(const Vec<Int, 32> &a, const uint8_t count)
{
  return _mm256_srl_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 32> srl(const Vec<Long, 32> &a,
                                     const uint8_t count)
{
  return _mm256_srl_epi64(a, _mm_cvtsi32_si128(count));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> srl(const Vec<T, 32> &a, const uint8_t count)
{
  return Vec<T, 32>(srl(a.lo(), count), srl(a.hi(), count));
}

#endif

// ---------------------------------------------------------------------------
// sll
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> sll(const Vec<Byte, 32> &a,
                                     const uint8_t count)
{
  return _mm256_and_si256(
    _mm256_sll_epi16(a, _mm_cvtsi32_si128(count)),
    _mm256_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<SignedByte, 32> sll(const Vec<SignedByte, 32> &a,
                                           const uint8_t count)
{
  return _mm256_and_si256(
    _mm256_sll_epi16(a, _mm_cvtsi32_si128(count)),
    _mm256_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<Word, 32> sll(const Vec<Word, 32> &a,
                                     const uint8_t count)
{
  return _mm256_sll_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 32> sll(const Vec<Short, 32> &a,
                                      const uint8_t count)
{
  return _mm256_sll_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Int, 32> sll(const Vec<Int, 32> &a, const uint8_t count)
{
  return _mm256_sll_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 32> sll(const Vec<Long, 32> &a,
                                     const uint8_t count)
{
  return _mm256_sll_epi64(a, _mm_cvtsi32_si128(count));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> sll(const Vec<T, 32> &a, const uint8_t count)
{
  return Vec<T, 32>(sll(a.lo(), count), sll(a.hi(), count));
}

#endif

// 19. Sep 22 (Jonas Keller):
// added Byte and SignedByte versions of hadd, hadds, hsub and hsubs
// added Word version of hadds and hsubs

// ---------------------------------------------------------------------------
// hadd
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 32> hadd(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  Vec<T, 32> x, y;
  unzip<1>(a, b, x, y);
  return add(x, y);
}

static SIMD_INLINE Vec<Word, 32> hadd(const Vec<Word, 32> &a,
                                      const Vec<Word, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hadd_epi16(a, b));
}

static SIMD_INLINE Vec<Short, 32> hadd(const Vec<Short, 32> &a,
                                       const Vec<Short, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hadd_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 32> hadd(const Vec<Int, 32> &a,
                                     const Vec<Int, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hadd_epi32(a, b));
}

static SIMD_INLINE Vec<Float, 32> hadd(const Vec<Float, 32> &a,
                                       const Vec<Float, 32> &b)
{
  return x_mm256_transpose4x64_ps(_mm256_hadd_ps(a, b));
}

static SIMD_INLINE Vec<Double, 32> hadd(const Vec<Double, 32> &a,
                                        const Vec<Double, 32> &b)
{
  return x_mm256_transpose4x64_pd(_mm256_hadd_pd(a, b));
}

// ---------------------------------------------------------------------------
// hadds
// ---------------------------------------------------------------------------

// 09. Mar 23 (Jonas Keller): made Int version of hadds saturating

template <typename T>
static SIMD_INLINE Vec<T, 32> hadds(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  Vec<T, 32> x, y;
  unzip<1>(a, b, x, y);
  return adds(x, y);
}

static SIMD_INLINE Vec<Short, 32> hadds(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hadds_epi16(a, b));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 32> hadds(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  return x_mm256_transpose4x64_ps(_mm256_hadd_ps(a, b));
}

// Double not saturated
static SIMD_INLINE Vec<Double, 32> hadds(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return x_mm256_transpose4x64_pd(_mm256_hadd_pd(a, b));
}

// ---------------------------------------------------------------------------
// hsub
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 32> hsub(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  Vec<T, 32> x, y;
  unzip<1>(a, b, x, y);
  return sub(x, y);
}

static SIMD_INLINE Vec<Word, 32> hsub(const Vec<Word, 32> &a,
                                      const Vec<Word, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hsub_epi16(a, b));
}

static SIMD_INLINE Vec<Short, 32> hsub(const Vec<Short, 32> &a,
                                       const Vec<Short, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hsub_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 32> hsub(const Vec<Int, 32> &a,
                                     const Vec<Int, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hsub_epi32(a, b));
}

static SIMD_INLINE Vec<Float, 32> hsub(const Vec<Float, 32> &a,
                                       const Vec<Float, 32> &b)
{
  return x_mm256_transpose4x64_ps(_mm256_hsub_ps(a, b));
}

static SIMD_INLINE Vec<Double, 32> hsub(const Vec<Double, 32> &a,
                                        const Vec<Double, 32> &b)
{
  return x_mm256_transpose4x64_pd(_mm256_hsub_pd(a, b));
}

// ---------------------------------------------------------------------------
// hsubs
// ---------------------------------------------------------------------------

// 09. Mar 23 (Jonas Keller): made Int version of hsubs saturating

template <typename T>
static SIMD_INLINE Vec<T, 32> hsubs(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  Vec<T, 32> x, y;
  unzip<1>(a, b, x, y);
  return subs(x, y);
}

static SIMD_INLINE Vec<Short, 32> hsubs(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return x_mm256_transpose4x64_epi64(x_mm256_hsubs_epi16(a, b));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 32> hsubs(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  return x_mm256_transpose4x64_ps(_mm256_hsub_ps(a, b));
}

// Double not saturated
static SIMD_INLINE Vec<Double, 32> hsubs(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return x_mm256_transpose4x64_pd(_mm256_hsub_pd(a, b));
}

// ---------------------------------------------------------------------------
// element-wise shift right
// ---------------------------------------------------------------------------

template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> srle(const Vec<T, 32> &a)
{
  const __m256i aInt          = reinterpret(a, OutputType<Int>());
  const Vec<Int, 32> aShifted = x_mm256_srli256_si256<COUNT * sizeof(T)>(aInt);
  return reinterpret(aShifted, OutputType<T>());
}

// ---------------------------------------------------------------------------
// element-wise shift left
// ---------------------------------------------------------------------------

template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> slle(const Vec<T, 32> &a)
{
  const __m256i aInt          = reinterpret(a, OutputType<Int>());
  const Vec<Int, 32> aShifted = x_mm256_slli256_si256<COUNT * sizeof(T)>(aInt);
  return reinterpret(aShifted, OutputType<T>());
}

// ---------------------------------------------------------------------------
// alignre
// ---------------------------------------------------------------------------

// all integer versions
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 32> alignre(const Vec<T, 32> &h, const Vec<T, 32> &l)
{
  const auto intH = reinterpret(h, OutputType<Int>());
  const auto intL = reinterpret(l, OutputType<Int>());
  const Vec<Int, 32> intRes =
    x_mm256_alignr256_epi8<COUNT * sizeof(T)>(intH, intL);
  return reinterpret(intRes, OutputType<T>());
}

// ---------------------------------------------------------------------------
// swizzle
// ---------------------------------------------------------------------------

// ---------- swizzle aux functions -----------

// alignoff is the element-wise offset (relates to size of byte)
template <size_t ALIGNOFF>
static SIMD_INLINE __m256i align_shuffle_256(__m256i lo, __m256i hi,
                                             __m256i mask)
{
  static_assert(ALIGNOFF < 32, "");
  return x_mm256_shuffle_epi8(x_mm256_alignr_epi8<ALIGNOFF>(hi, lo), mask);
}

// ---------- swizzle (AoS to SoA) ----------

// 01. Apr 23 (Jonas Keller): switched from using tag dispatching to using
// enable_if SFINAE, which allows more cases with the same implementation
// to be combined

// -------------------- n = 1 --------------------

// all types
template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 32>[1], Integer<1>)
{
  // v remains unchanged
}

// -------------------- n = 2 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) <= 2 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 32> v[2], Integer<2>)
{
  Vec<T, 32> vs[2];
  swizzle_32_16<2>(v, vs);
  const __m256i mask = x_mm256_duplicate_si128(get_swizzle_mask<2, T>());
  const __m256i s[2] = {
    x_mm256_shuffle_epi8(vs[0], mask),
    x_mm256_shuffle_epi8(vs[1], mask),
  };
  v[0] = x_mm256_unpacklo_epi64(s[0], s[1]);
  v[1] = x_mm256_unpackhi_epi64(s[0], s[1]);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[2], Integer<2>)
{
  const Vec<Float, 32> vFloat[2] = {
    reinterpret(v[0], OutputType<Float>()),
    reinterpret(v[1], OutputType<Float>()),
  };
  Vec<Float, 32> vs[2];
  swizzle_32_16<2>(vFloat, vs);
  const Vec<Float, 32> vOut[2] = {
    _mm256_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(2, 0, 2, 0)),
    _mm256_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(3, 1, 3, 1)),
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[2], Integer<2>)
{
  const Vec<Double, 32> vDouble[2] = {
    reinterpret(v[0], OutputType<Double>()),
    reinterpret(v[1], OutputType<Double>()),
  };
  Vec<Double, 32> vs[2];
  swizzle_32_16<2>(vDouble, vs);
  const Vec<Double, 32> vOut[2] = {
    _mm256_shuffle_pd(vs[0], vs[1], 0),
    _mm256_shuffle_pd(vs[0], vs[1], 0xf),
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
}

// -------------------- n = 3 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) <= 2 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 32> v[3], Integer<3>)
{
  Vec<T, 32> vs[3];
  swizzle_32_16<3>(v, vs);
  const __m256i mask = x_mm256_duplicate_si128(get_swizzle_mask<3, T>());
  const __m256i s0   = align_shuffle_256<0>(vs[0], vs[1], mask);
  const __m256i s1   = align_shuffle_256<12>(vs[0], vs[1], mask);
  const __m256i s2   = align_shuffle_256<8>(vs[1], vs[2], mask);
  const __m256i s3 =
    align_shuffle_256<4>(vs[2], _mm256_undefined_si256(), mask);
  const __m256i l01 = x_mm256_unpacklo_epi32(s0, s1);
  const __m256i h01 = x_mm256_unpackhi_epi32(s0, s1);
  const __m256i l23 = x_mm256_unpacklo_epi32(s2, s3);
  const __m256i h23 = x_mm256_unpackhi_epi32(s2, s3);
  v[0]              = x_mm256_unpacklo_epi64(l01, l23);
  v[1]              = x_mm256_unpackhi_epi64(l01, l23);
  v[2]              = x_mm256_unpacklo_epi64(h01, h23);
}

// 32 bit types
// from Stan Melax: "3D Vector Normalization..."
// https://software.intel.com/en-us/articles/3d-vector-normalization-using-256-bit-intel-advanced-vector-extensions-intel-avx
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[3], Integer<3>)
{
  const Vec<Float, 32> vFloat[3] = {
    reinterpret(v[0], OutputType<Float>()),
    reinterpret(v[1], OutputType<Float>()),
    reinterpret(v[2], OutputType<Float>()),
  };
  Vec<Float, 32> vs[3];
  swizzle_32_16<3>(vFloat, vs);
  // x0y0z0x1 = v[0]
  // y1z1x2y2 = v[1]
  // z2x3y3z3 = v[2]
  const Vec<Float, 32> x2y2x3y3 =
    _mm256_shuffle_ps(vs[1], vs[2], _MM_SHUFFLE(2, 1, 3, 2));
  const Vec<Float, 32> y0z0y1z1 =
    _mm256_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(1, 0, 2, 1));
  const Vec<Float, 32> x0x1x2x3 =
    _mm256_shuffle_ps(vs[0], x2y2x3y3, _MM_SHUFFLE(2, 0, 3, 0));
  const Vec<Float, 32> y0y1y2y3 =
    _mm256_shuffle_ps(y0z0y1z1, x2y2x3y3, _MM_SHUFFLE(3, 1, 2, 0));
  const Vec<Float, 32> z0z1z2z3 =
    _mm256_shuffle_ps(y0z0y1z1, vs[2], _MM_SHUFFLE(3, 0, 3, 1));
  v[0] = reinterpret(x0x1x2x3, OutputType<T>());
  v[1] = reinterpret(y0y1y2y3, OutputType<T>());
  v[2] = reinterpret(z0z1z2z3, OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[3], Integer<3>)
{
  const Vec<Double, 32> vDouble[3] = {
    reinterpret(v[0], OutputType<Double>()), // x0y0z0x1
    reinterpret(v[1], OutputType<Double>()), // y1z1x2y2
    reinterpret(v[2], OutputType<Double>()), // z2x3y3z3
  };
  Vec<Double, 32> vs[3];
  swizzle_32_16<3>(vDouble, vs);
  // vs[0] = x0y0x2y2
  // vs[1] = z0x1z2x3
  // vs[2] = y1z1y3z3
  const Vec<Double, 32> vOut[3] = {
    // x0x1x2x3
    _mm256_shuffle_pd(vs[0], vs[1], 0xa), // 0b1010
    // y0y1y2y3
    _mm256_shuffle_pd(vs[0], vs[2], 0x5), // 0b0101
    // z0z1z2z3
    _mm256_shuffle_pd(vs[1], vs[2], 0xa), // 0b1010
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
  v[2] = reinterpret(vOut[2], OutputType<T>());
}

// -------------------- n = 4 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF((sizeof(T) <= 2 && std::is_integral<T>::value))>
static SIMD_INLINE void swizzle(Vec<T, 32> v[4], Integer<4>)
{
  Vec<T, 32> vs[4];
  swizzle_32_16<4>(v, vs);
  const __m256i mask = x_mm256_duplicate_si128(get_swizzle_mask<4, T>());
  const __m256i s[4] = {
    x_mm256_shuffle_epi8(vs[0], mask),
    x_mm256_shuffle_epi8(vs[1], mask),
    x_mm256_shuffle_epi8(vs[2], mask),
    x_mm256_shuffle_epi8(vs[3], mask),
  };
  const __m256i l01 = x_mm256_unpacklo_epi32(s[0], s[1]);
  const __m256i h01 = x_mm256_unpackhi_epi32(s[0], s[1]);
  const __m256i l23 = x_mm256_unpacklo_epi32(s[2], s[3]);
  const __m256i h23 = x_mm256_unpackhi_epi32(s[2], s[3]);
  v[0]              = x_mm256_unpacklo_epi64(l01, l23);
  v[1]              = x_mm256_unpackhi_epi64(l01, l23);
  v[2]              = x_mm256_unpacklo_epi64(h01, h23);
  v[3]              = x_mm256_unpackhi_epi64(h01, h23);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[4], Integer<4>)
{
  Vec<Float, 32> vInt[4];
  for (size_t i = 0; i < 4; ++i) {
    vInt[i] = reinterpret(v[i], OutputType<Float>());
  }
  Vec<Float, 32> vs[4];
  swizzle_32_16<4>(vInt, vs);
  const __m256 s[4] = {
    _mm256_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(1, 0, 1, 0)),
    _mm256_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(3, 2, 3, 2)),
    _mm256_shuffle_ps(vs[2], vs[3], _MM_SHUFFLE(1, 0, 1, 0)),
    _mm256_shuffle_ps(vs[2], vs[3], _MM_SHUFFLE(3, 2, 3, 2)),
  };
  const Vec<Float, 32> vOut[4] = {
    _mm256_shuffle_ps(s[0], s[2], _MM_SHUFFLE(2, 0, 2, 0)),
    _mm256_shuffle_ps(s[0], s[2], _MM_SHUFFLE(3, 1, 3, 1)),
    _mm256_shuffle_ps(s[1], s[3], _MM_SHUFFLE(2, 0, 2, 0)),
    _mm256_shuffle_ps(s[1], s[3], _MM_SHUFFLE(3, 1, 3, 1)),
  };
  for (size_t i = 0; i < 4; ++i) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[4], Integer<4>)
{
  const Vec<Double, 32> vInt[4] = {
    reinterpret(v[0], OutputType<Double>()), // x0y0z0w0
    reinterpret(v[1], OutputType<Double>()), // x1y1z1w1
    reinterpret(v[2], OutputType<Double>()), // x2y2z2w2
    reinterpret(v[3], OutputType<Double>()), // x3y3z3w3
  };
  Vec<Double, 32> vs[4];
  swizzle_32_16<4>(vInt, vs);
  // vs[0] = x0y0x2y2
  // vs[1] = z0w0z2w2
  // vs[2] = x1y1x3y3
  // vs[3] = z1w1z3w3
  const Vec<Double, 32> vOut[4] = {
    // x0x1x2x3
    _mm256_shuffle_pd(vs[0], vs[2], 0x0), // 0b0000
    // y0y1y2y3
    _mm256_shuffle_pd(vs[0], vs[2], 0xF), // 0b1111
    // z0z1z2z3
    _mm256_shuffle_pd(vs[1], vs[3], 0x0), // 0b0000
    // w0w1w2w3
    _mm256_shuffle_pd(vs[1], vs[3], 0xF), // 0b1111
  };
  for (size_t i = 0; i < 4; ++i) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// -------------------- n = 5 --------------------

// 8 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 1 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 32> v[5], Integer<5>)
{
  Vec<T, 32> vs[5];
  swizzle_32_16<5>(v, vs);
  const __m256i mask = x_mm256_duplicate_si128(get_swizzle_mask<5, T>());
  const __m256i s0   = align_shuffle_256<0>(vs[0], vs[1], mask);
  const __m256i s1   = align_shuffle_256<10>(vs[0], vs[1], mask);
  const __m256i s2   = align_shuffle_256<4>(vs[1], vs[2], mask);
  const __m256i s3   = align_shuffle_256<14>(vs[1], vs[2], mask);
  const __m256i s4   = align_shuffle_256<8>(vs[2], vs[3], mask);
  const __m256i s5   = align_shuffle_256<2>(vs[3], vs[4], mask);
  const __m256i s6   = align_shuffle_256<12>(vs[3], vs[4], mask);
  const __m256i s7 =
    align_shuffle_256<6>(vs[4], _mm256_undefined_si256(), mask);
  const __m256i l01     = x_mm256_unpacklo_epi16(s0, s1);
  const __m256i h01     = x_mm256_unpackhi_epi16(s0, s1);
  const __m256i l23     = x_mm256_unpacklo_epi16(s2, s3);
  const __m256i h23     = x_mm256_unpackhi_epi16(s2, s3);
  const __m256i l45     = x_mm256_unpacklo_epi16(s4, s5);
  const __m256i h45     = x_mm256_unpackhi_epi16(s4, s5);
  const __m256i l67     = x_mm256_unpacklo_epi16(s6, s7);
  const __m256i h67     = x_mm256_unpackhi_epi16(s6, s7);
  const __m256i ll01l23 = x_mm256_unpacklo_epi32(l01, l23);
  const __m256i hl01l23 = x_mm256_unpackhi_epi32(l01, l23);
  const __m256i ll45l67 = x_mm256_unpacklo_epi32(l45, l67);
  const __m256i hl45l67 = x_mm256_unpackhi_epi32(l45, l67);
  const __m256i lh01h23 = x_mm256_unpacklo_epi32(h01, h23);
  const __m256i lh45h67 = x_mm256_unpacklo_epi32(h45, h67);
  v[0]                  = x_mm256_unpacklo_epi64(ll01l23, ll45l67);
  v[1]                  = x_mm256_unpackhi_epi64(ll01l23, ll45l67);
  v[2]                  = x_mm256_unpacklo_epi64(hl01l23, hl45l67);
  v[3]                  = x_mm256_unpackhi_epi64(hl01l23, hl45l67);
  v[4]                  = x_mm256_unpacklo_epi64(lh01h23, lh45h67);
}

// 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 2 && std::is_integral<T>::value),
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[5], Integer<5>)
{
  Vec<T, 32> vs[5];
  swizzle_32_16<5>(v, vs);
  const __m256i mask = x_mm256_duplicate_si128(get_swizzle_mask<5, T>());
  const __m256i s0   = align_shuffle_256<0>(vs[0], vs[1], mask);
  const __m256i s1   = align_shuffle_256<6>(vs[0], vs[1], mask);
  const __m256i s2   = align_shuffle_256<4>(vs[1], vs[2], mask);
  const __m256i s3   = align_shuffle_256<10>(vs[1], vs[2], mask);
  const __m256i s4   = align_shuffle_256<8>(vs[2], vs[3], mask);
  const __m256i s5   = align_shuffle_256<14>(vs[2], vs[3], mask);
  const __m256i s6   = align_shuffle_256<12>(vs[3], vs[4], mask);
  const __m256i s7 =
    align_shuffle_256<2>(vs[4], _mm256_undefined_si256(), mask);
  const __m256i l02 = x_mm256_unpacklo_epi32(s0, s2);
  const __m256i h02 = x_mm256_unpackhi_epi32(s0, s2);
  const __m256i l13 = x_mm256_unpacklo_epi32(s1, s3);
  const __m256i l46 = x_mm256_unpacklo_epi32(s4, s6);
  const __m256i h46 = x_mm256_unpackhi_epi32(s4, s6);
  const __m256i l57 = x_mm256_unpacklo_epi32(s5, s7);
  v[0]              = x_mm256_unpacklo_epi64(l02, l46);
  v[1]              = x_mm256_unpackhi_epi64(l02, l46);
  v[2]              = x_mm256_unpacklo_epi64(h02, h46);
  v[3]              = x_mm256_unpacklo_epi64(l13, l57);
  v[4]              = x_mm256_unpackhi_epi64(l13, l57);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[5], Integer<5>)
{
  Vec<Int, 32> vInt[5];
  for (size_t i = 0; i < 5; i++) {
    vInt[i] = reinterpret(v[i], OutputType<Int>());
  }
  Vec<Int, 32> vs[5];
  swizzle_32_16<5>(vInt, vs);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  // v[0]: 0 1 2 3
  // v[1]: 4 x x x
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                   x x x   x
  // 5 6 7 8
  const __m256i s2 = x_mm256_alignr_epi8<4>(vs[2], vs[1]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                             x  x  x    x
  // 9 x x x
  const __m256i s3 = x_mm256_alignr_epi8<4>(vs[3], vs[2]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                x  x    x  x
  // 10 11 12 13
  const __m256i s4 = x_mm256_alignr_epi8<8>(vs[3], vs[2]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                              x  x    x  x
  // 14 x x x
  const __m256i s5 = x_mm256_alignr_epi8<8>(vs[4], vs[3]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                 X    X  X  X
  // 15 16 17 18
  const __m256i s6 = x_mm256_alignr_epi8<12>(vs[4], vs[3]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                               X X X X
  // 19 x x x
  const __m256i s7 = x_mm256_alignr_epi8<12>(vs[0], vs[4]);
  // 0 1 2 3 / 5 6 7 8 -> 0 5 1 6 / 2 7 3 8
  const __m256i l02 = x_mm256_unpacklo_epi32(vs[0], s2);
  const __m256i h02 = x_mm256_unpackhi_epi32(vs[0], s2);
  // 4 x x x / 9 x x x -> 4 9 x x
  const __m256i l13 = x_mm256_unpacklo_epi32(vs[1], s3);
  // 10 11 12 13 / 15 16 17 18 -> 10 15 11 13 / 12 17 13 18
  const __m256i l46 = x_mm256_unpacklo_epi32(s4, s6);
  const __m256i h46 = x_mm256_unpackhi_epi32(s4, s6);
  // 14 x x x / 19 x x x -> 14 19 x x
  const __m256i l57          = x_mm256_unpacklo_epi32(s5, s7);
  const Vec<Int, 32> vOut[5] = {
    // 0 5 1 6 / 10 15 11 13 -> 0 5 10 15 / 1 6 11 16
    x_mm256_unpacklo_epi64(l02, l46),
    x_mm256_unpackhi_epi64(l02, l46),
    // 2 7 3 8 / 12 17 13 18 -> 2 7 12 17 / 3 8 13 18
    x_mm256_unpacklo_epi64(h02, h46),
    x_mm256_unpackhi_epi64(h02, h46),
    // 4 9 x x / 14 19 x x -> 4 9 14 19
    x_mm256_unpacklo_epi64(l13, l57),
  };
  for (size_t i = 0; i < 5; i++) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void, typename = void>
static SIMD_INLINE void swizzle(Vec<T, 32> v[5], Integer<5>)
{
  const Vec<Double, 32> vDouble[5] = {
    reinterpret(v[0], OutputType<Double>()), // x0y0z0w0
    reinterpret(v[1], OutputType<Double>()), // v0x1y1z1
    reinterpret(v[2], OutputType<Double>()), // w1v1x2y2
    reinterpret(v[3], OutputType<Double>()), // z2w2v2x3
    reinterpret(v[4], OutputType<Double>()), // y3z3w3v3
  };
  Vec<Double, 32> vs[5];
  swizzle_32_16<5>(vDouble, vs);
  // vs[0] = x0y0x2y2
  // vs[1] = z0w0z2w2
  // vs[2] = v0x1v2x3
  // vs[3] = y1z1y3z3
  // vs[4] = w1v1w3v3
  const Vec<Double, 32> vOut[5] = {
    // x0x1x2x3
    _mm256_shuffle_pd(vs[0], vs[2], 0xa), // 0b1010
    // y0y1y2y3
    _mm256_shuffle_pd(vs[0], vs[3], 0x5), // 0b0101
    // z0z1z2z3
    _mm256_shuffle_pd(vs[1], vs[3], 0xa), // 0b1010
    // w0w1w2w3
    _mm256_shuffle_pd(vs[1], vs[4], 0x5), // 0b0101
    // v0v1v2v3
    _mm256_shuffle_pd(vs[2], vs[4], 0xa), // 0b1010
  };
  for (size_t i = 0; i < 5; i++) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// ---------------------------------------------------------------------------
// comparison functions
// ---------------------------------------------------------------------------

// 28. Mar 23 (Jonas Keller): checked the constants for _mm256_cmp_ps in the
// Float comparison functions, they match the implementation of the SSE versions
// (see cmpps in Intel manual) and added corresponding comments

// ---------------------------------------------------------------------------
// compare <
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/
//   sse-compare-packed-unsigned-bytes

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> cmplt(const Vec<Byte, 32> &a,
                                       const Vec<Byte, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80808080);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x80
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x80
  return _mm256_cmpgt_epi8(b1, a1);
}

static SIMD_INLINE Vec<SignedByte, 32> cmplt(const Vec<SignedByte, 32> &a,
                                             const Vec<SignedByte, 32> &b)
{
  return _mm256_cmpgt_epi8(b, a);
}

static SIMD_INLINE Vec<Word, 32> cmplt(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80008000);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x8000
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x8000
  return _mm256_cmpgt_epi16(b1, a1);
}

static SIMD_INLINE Vec<Short, 32> cmplt(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return _mm256_cmpgt_epi16(b, a);
}

static SIMD_INLINE Vec<Int, 32> cmplt(const Vec<Int, 32> &a,
                                      const Vec<Int, 32> &b)
{
  return _mm256_cmpgt_epi32(b, a);
}

static SIMD_INLINE Vec<Long, 32> cmplt(const Vec<Long, 32> &a,
                                       const Vec<Long, 32> &b)
{
  return _mm256_cmpgt_epi64(b, a);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmplt(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmplt(a.lo(), b.lo()), cmplt(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmplt(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmplt_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(a, b, _CMP_LT_OS);
}

static SIMD_INLINE Vec<Double, 32> cmplt(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(a, b, _CMP_LT_OS);
}

// ---------------------------------------------------------------------------
// compare <=
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/
//   sse-compare-packed-unsigned-bytes

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> cmple(const Vec<Byte, 32> &a,
                                       const Vec<Byte, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80808080);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x80
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x80
  return _mm256_or_si256(_mm256_cmpgt_epi8(b1, a1), _mm256_cmpeq_epi8(a1, b1));
}

static SIMD_INLINE Vec<SignedByte, 32> cmple(const Vec<SignedByte, 32> &a,
                                             const Vec<SignedByte, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi8(b, a), _mm256_cmpeq_epi8(a, b));
}

static SIMD_INLINE Vec<Word, 32> cmple(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80008000);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x8000
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x8000
  return _mm256_or_si256(_mm256_cmpgt_epi16(b1, a1),
                         _mm256_cmpeq_epi16(a1, b1));
}

static SIMD_INLINE Vec<Short, 32> cmple(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi16(b, a), _mm256_cmpeq_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 32> cmple(const Vec<Int, 32> &a,
                                      const Vec<Int, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi32(b, a), _mm256_cmpeq_epi32(a, b));
}

static SIMD_INLINE Vec<Long, 32> cmple(const Vec<Long, 32> &a,
                                       const Vec<Long, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi64(b, a), _mm256_cmpeq_epi64(a, b));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmple(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmple(a.lo(), b.lo()), cmple(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmple(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmple_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(a, b, _CMP_LE_OS);
}

static SIMD_INLINE Vec<Double, 32> cmple(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(a, b, _CMP_LE_OS);
}

// ---------------------------------------------------------------------------
// compare ==
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> cmpeq(const Vec<Byte, 32> &a,
                                       const Vec<Byte, 32> &b)
{
  return _mm256_cmpeq_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 32> cmpeq(const Vec<SignedByte, 32> &a,
                                             const Vec<SignedByte, 32> &b)
{
  return _mm256_cmpeq_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> cmpeq(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b)
{
  return _mm256_cmpeq_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 32> cmpeq(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return _mm256_cmpeq_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> cmpeq(const Vec<Int, 32> &a,
                                      const Vec<Int, 32> &b)
{
  return _mm256_cmpeq_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 32> cmpeq(const Vec<Long, 32> &a,
                                       const Vec<Long, 32> &b)
{
  return _mm256_cmpeq_epi64(a, b);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmpeq(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmpeq(a.lo(), b.lo()), cmpeq(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmpeq(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmpeq_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(a, b, _CMP_EQ_OQ);
}

static SIMD_INLINE Vec<Double, 32> cmpeq(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(a, b, _CMP_EQ_OQ);
}

// ---------------------------------------------------------------------------
// compare >
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/
//   sse-compare-packed-unsigned-bytes

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> cmpgt(const Vec<Byte, 32> &a,
                                       const Vec<Byte, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80808080);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x80
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x80
  return _mm256_cmpgt_epi8(a1, b1);
}

static SIMD_INLINE Vec<SignedByte, 32> cmpgt(const Vec<SignedByte, 32> &a,
                                             const Vec<SignedByte, 32> &b)
{
  return _mm256_cmpgt_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 32> cmpgt(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80008000);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x8000
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x8000
  return _mm256_cmpgt_epi16(a1, b1);
}

static SIMD_INLINE Vec<Short, 32> cmpgt(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return _mm256_cmpgt_epi16(a, b);
}

static SIMD_INLINE Vec<Int, 32> cmpgt(const Vec<Int, 32> &a,
                                      const Vec<Int, 32> &b)
{
  return _mm256_cmpgt_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 32> cmpgt(const Vec<Long, 32> &a,
                                       const Vec<Long, 32> &b)
{
  return _mm256_cmpgt_epi64(a, b);
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmpgt(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmpgt(a.lo(), b.lo()), cmpgt(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmpgt(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmplt_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(b, a, _CMP_LT_OS);
}

static SIMD_INLINE Vec<Double, 32> cmpgt(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(b, a, _CMP_LT_OS);
}

// ---------------------------------------------------------------------------
// compare >=
// ---------------------------------------------------------------------------

// http://stackoverflow.com/questions/16204663/
//  sse-compare-packed-unsigned-bytes

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> cmpge(const Vec<Byte, 32> &a,
                                       const Vec<Byte, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80808080);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x80
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x80
  return _mm256_or_si256(_mm256_cmpgt_epi8(a1, b1), _mm256_cmpeq_epi8(a1, b1));
}

static SIMD_INLINE Vec<SignedByte, 32> cmpge(const Vec<SignedByte, 32> &a,
                                             const Vec<SignedByte, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi8(a, b), _mm256_cmpeq_epi8(a, b));
}

static SIMD_INLINE Vec<Word, 32> cmpge(const Vec<Word, 32> &a,
                                       const Vec<Word, 32> &b)
{
  const __m256i signbit = _mm256_set1_epi32(0x80008000);
  const __m256i a1      = _mm256_xor_si256(a, signbit); // sub 0x8000
  const __m256i b1      = _mm256_xor_si256(b, signbit); // sub 0x8000
  return _mm256_or_si256(_mm256_cmpgt_epi16(a1, b1),
                         _mm256_cmpeq_epi16(a1, b1));
}

static SIMD_INLINE Vec<Short, 32> cmpge(const Vec<Short, 32> &a,
                                        const Vec<Short, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi16(a, b), _mm256_cmpeq_epi16(a, b));
}

static SIMD_INLINE Vec<Int, 32> cmpge(const Vec<Int, 32> &a,
                                      const Vec<Int, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi32(a, b), _mm256_cmpeq_epi32(a, b));
}

static SIMD_INLINE Vec<Long, 32> cmpge(const Vec<Long, 32> &a,
                                       const Vec<Long, 32> &b)
{
  return _mm256_or_si256(_mm256_cmpgt_epi64(a, b), _mm256_cmpeq_epi64(a, b));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmpge(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmpge(a.lo(), b.lo()), cmpge(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmpge(const Vec<Float, 32> &a,
                                        const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmple_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(b, a, _CMP_LE_OS);
}

static SIMD_INLINE Vec<Double, 32> cmpge(const Vec<Double, 32> &a,
                                         const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(b, a, _CMP_LE_OS);
}

// ---------------------------------------------------------------------------
// compare !=
// ---------------------------------------------------------------------------

#ifdef __AVX2__

// there is no cmpneq for integers and no not, so use cmpeq and xor with all
// ones to invert the result

static SIMD_INLINE Vec<Byte, 32> cmpneq(const Vec<Byte, 32> &a,
                                        const Vec<Byte, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi8(a, b), _mm256_set1_epi32(-1));
}

static SIMD_INLINE Vec<SignedByte, 32> cmpneq(const Vec<SignedByte, 32> &a,
                                              const Vec<SignedByte, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi8(a, b), _mm256_set1_epi32(-1));
}

static SIMD_INLINE Vec<Word, 32> cmpneq(const Vec<Word, 32> &a,
                                        const Vec<Word, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi16(a, b), _mm256_set1_epi32(-1));
}

static SIMD_INLINE Vec<Short, 32> cmpneq(const Vec<Short, 32> &a,
                                         const Vec<Short, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi16(a, b), _mm256_set1_epi32(-1));
}

static SIMD_INLINE Vec<Int, 32> cmpneq(const Vec<Int, 32> &a,
                                       const Vec<Int, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi32(a, b), _mm256_set1_epi32(-1));
}

static SIMD_INLINE Vec<Long, 32> cmpneq(const Vec<Long, 32> &a,
                                        const Vec<Long, 32> &b)
{
  return _mm256_xor_si256(_mm256_cmpeq_epi64(a, b), _mm256_set1_epi32(-1));
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> cmpneq(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(cmpneq(a.lo(), b.lo()), cmpneq(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Float, 32> cmpneq(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b)
{
  // same constant as in implementation of _mm_cmpneq_ps (see cmpps instruction
  // in Intel manual)
  return _mm256_cmp_ps(a, b, _CMP_NEQ_UQ);
}

static SIMD_INLINE Vec<Double, 32> cmpneq(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b)
{
  return _mm256_cmp_pd(a, b, _CMP_NEQ_UQ);
}

// ---------------------------------------------------------------------------
// ifelse
// ---------------------------------------------------------------------------

// 10. Apr 23 (Jonas Keller): made two versions of ifelse, one for 8 and 16 bit
// data types, and one for 32 and larger data types, so that for the latter
// the blendv instruction can be used even if avx2 is not available

// version for 8 and 16 bit data types
template <typename T, SIMD_ENABLE_IF(sizeof(T) <= 2)>
static SIMD_INLINE Vec<T, 32> ifelse(const Vec<T, 32> &cond,
                                     const Vec<T, 32> &trueVal,
                                     const Vec<T, 32> &falseVal)
{
#ifdef __AVX2__
  const Vec<Byte, 32> res =
    _mm256_blendv_epi8(reinterpret(falseVal, OutputType<Byte>()),
                       reinterpret(trueVal, OutputType<Byte>()),
                       reinterpret(cond, OutputType<Byte>()));
#else
  // non-avx2 workaround
  const Vec<Float, 32> res =
    _mm256_or_ps(_mm256_and_ps(reinterpret(cond, OutputType<Float>()),
                               reinterpret(trueVal, OutputType<Float>())),
                 _mm256_andnot_ps(reinterpret(cond, OutputType<Float>()),
                                  reinterpret(falseVal, OutputType<Float>())));
#endif
  return reinterpret(res, OutputType<T>());
}

// version for 32 bit or larger data types
template <typename T, SIMD_ENABLE_IF(sizeof(T) > 2), typename = void>
static SIMD_INLINE Vec<T, 32> ifelse(const Vec<T, 32> &cond,
                                     const Vec<T, 32> &trueVal,
                                     const Vec<T, 32> &falseVal)
{
  const Vec<Float, 32> res =
    _mm256_blendv_ps(reinterpret(falseVal, OutputType<Float>()),
                     reinterpret(trueVal, OutputType<Float>()),
                     reinterpret(cond, OutputType<Float>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_and
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> bit_and(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
#ifdef __AVX2__
  return _mm256_and_si256(a, b);
#else
  // non-avx2 workaround
  return _mm256_castps_si256(
    _mm256_and_ps(_mm256_castsi256_ps(a), _mm256_castsi256_ps(b)));
#endif
}

// float version
static SIMD_INLINE Vec<Float, 32> bit_and(const Vec<Float, 32> &a,
                                          const Vec<Float, 32> &b)
{
  return _mm256_and_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> bit_and(const Vec<Double, 32> &a,
                                           const Vec<Double, 32> &b)
{
  return _mm256_and_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_or
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> bit_or(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
#ifdef __AVX2__
  return _mm256_or_si256(a, b);
#else
  // non-avx2 workaround
  return _mm256_castps_si256(
    _mm256_or_ps(_mm256_castsi256_ps(a), _mm256_castsi256_ps(b)));
#endif
}

// float version
static SIMD_INLINE Vec<Float, 32> bit_or(const Vec<Float, 32> &a,
                                         const Vec<Float, 32> &b)
{
  return _mm256_or_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> bit_or(const Vec<Double, 32> &a,
                                          const Vec<Double, 32> &b)
{
  return _mm256_or_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_andnot
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> bit_andnot(const Vec<T, 32> &a,
                                         const Vec<T, 32> &b)
{
#ifdef __AVX2__
  return _mm256_andnot_si256(a, b);
#else
  // non-avx2 workaround
  return _mm256_castps_si256(
    _mm256_andnot_ps(_mm256_castsi256_ps(a), _mm256_castsi256_ps(b)));
#endif
}

// float version
static SIMD_INLINE Vec<Float, 32> bit_andnot(const Vec<Float, 32> &a,
                                             const Vec<Float, 32> &b)
{
  return _mm256_andnot_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> bit_andnot(const Vec<Double, 32> &a,
                                              const Vec<Double, 32> &b)
{
  return _mm256_andnot_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_xor
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> bit_xor(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
#ifdef __AVX2__
  return _mm256_xor_si256(a, b);
#else
  // non-avx2 workaround
  return _mm256_castps_si256(
    _mm256_xor_ps(_mm256_castsi256_ps(a), _mm256_castsi256_ps(b)));
#endif
}

// float version
static SIMD_INLINE Vec<Float, 32> bit_xor(const Vec<Float, 32> &a,
                                          const Vec<Float, 32> &b)
{
  return _mm256_xor_ps(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 32> bit_xor(const Vec<Double, 32> &a,
                                           const Vec<Double, 32> &b)
{
  return _mm256_xor_pd(a, b);
}

// ---------------------------------------------------------------------------
// bit_not
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 32> bit_not(const Vec<T, 32> &a)
{
#ifdef __AVX2__
  return _mm256_xor_si256(a, _mm256_set1_epi32(-1));
#else
  // non-avx2 workaround
  return _mm256_castps_si256(_mm256_xor_ps(
    _mm256_castsi256_ps(a), _mm256_castsi256_ps(_mm256_set1_epi32(-1))));
#endif
}

// float version
static SIMD_INLINE Vec<Float, 32> bit_not(const Vec<Float, 32> &a)
{
  return _mm256_xor_ps(a, _mm256_castsi256_ps(_mm256_set1_epi32(-1)));
}

// double version
static SIMD_INLINE Vec<Double, 32> bit_not(const Vec<Double, 32> &a)
{
  return _mm256_xor_pd(a, _mm256_castsi256_pd(_mm256_set1_epi32(-1)));
}

// ---------------------------------------------------------------------------
// avg: average with rounding down
// ---------------------------------------------------------------------------

#ifdef __AVX2__

static SIMD_INLINE Vec<Byte, 32> avg(const Vec<Byte, 32> &a,
                                     const Vec<Byte, 32> &b)
{
  return _mm256_avg_epu8(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<SignedByte, 32> avg(const Vec<SignedByte, 32> &a,
                                           const Vec<SignedByte, 32> &b)
{
  // from Agner Fog's VCL vectori128.h
  const __m256i signbit = _mm256_set1_epi8(int8_t(0x80));
  const __m256i a1      = _mm256_xor_si256(a, signbit); // add 0x80
  const __m256i b1      = _mm256_xor_si256(b, signbit); // add 0x80
  const __m256i m1      = _mm256_avg_epu8(a1, b1);      // unsigned avg
  return _mm256_xor_si256(m1, signbit);                 // sub 0x80
}

static SIMD_INLINE Vec<Word, 32> avg(const Vec<Word, 32> &a,
                                     const Vec<Word, 32> &b)
{
  return _mm256_avg_epu16(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Short, 32> avg(const Vec<Short, 32> &a,
                                      const Vec<Short, 32> &b)
{
  // from Agner Fog's VCL vectori128.h
  const __m256i signbit = _mm256_set1_epi16(int16_t(0x8000));
  const __m256i a1      = _mm256_xor_si256(a, signbit); // add 0x8000
  const __m256i b1      = _mm256_xor_si256(b, signbit); // add 0x8000
  const __m256i m1      = _mm256_avg_epu16(a1, b1);     // unsigned avg
  return _mm256_xor_si256(m1, signbit);                 // sub 0x8000
}

#else

// non-avx2 workaround
template <typename T>
static SIMD_INLINE Vec<T, 32> avg(const Vec<T, 32> &a, const Vec<T, 32> &b)
{
  return Vec<T, 32>(avg(a.lo(), b.lo()), avg(a.hi(), b.hi()));
}

#endif

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Int, 32> avg(const Vec<Int, 32> &a,
                                    const Vec<Int, 32> &b)
{
  const auto halfA = srai<1>(a);
  const auto halfB = srai<1>(b);
  const auto sum   = add(halfA, halfB);
  const auto lsb   = bit_and(bit_or(a, b), set1(Int(1), Integer<32>()));
  return add(sum, lsb);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Long, 32> avg(const Vec<Long, 32> &a,
                                     const Vec<Long, 32> &b)
{
  const auto halfA = srai<1>(a);
  const auto halfB = srai<1>(b);
  const auto sum   = add(halfA, halfB);
  const auto lsb   = bit_and(bit_or(a, b), set1(Long(1), Integer<32>()));
  return add(sum, lsb);
}

// NOTE: Float version doesn't round!
static SIMD_INLINE Vec<Float, 32> avg(const Vec<Float, 32> &a,
                                      const Vec<Float, 32> &b)
{
  return _mm256_mul_ps(_mm256_add_ps(a, b), _mm256_set1_ps(0.5f));
}

// NOTE: Double version doesn't round!
static SIMD_INLINE Vec<Double, 32> avg(const Vec<Double, 32> &a,
                                       const Vec<Double, 32> &b)
{
  return _mm256_mul_pd(_mm256_add_pd(a, b), _mm256_set1_pd(0.5));
}

// ---------------------------------------------------------------------------
// test_all_zeros
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_zeros(const Vec<T, 32> &a)
{
  const auto intA = reinterpret(a, OutputType<Int>());
  return _mm256_testz_si256(intA, intA);
}

// ---------------------------------------------------------------------------
// test_all_ones
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_ones(const Vec<T, 32> &a)
{
  const auto intA = reinterpret(a, OutputType<Int>());
  return _mm256_testc_si256(intA, _mm256_set1_epi32(-1));
}

// ---------------------------------------------------------------------------
// reverse
// ---------------------------------------------------------------------------

// All reverse operations below are courtesy of Yannick Sander
// modified

static SIMD_INLINE Vec<Byte, 32> reverse(const Vec<Byte, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask =
    _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1,
                    2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
  // _mm256_shuffle_epi8 reverses the upper and lower lane of a individually the
  // two lanes have to be swapped as well to perform a full reverse
  const __m256i shuffled_lanes = _mm256_shuffle_epi8(a, mask);
  // swap lanes
  return _mm256_permute4x64_epi64(shuffled_lanes, _MM_SHUFFLE(1, 0, 3, 2));
#else // AVX fallback
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<SignedByte, 32> reverse(const Vec<SignedByte, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask =
    _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1,
                    2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
  // _mm256_shuffle_epi8 reverses the upper and lower lane of a individually the
  // two lanes have to be swapped as well to perform a full reverse
  const __m256i shuffled_lanes = _mm256_shuffle_epi8(a, mask);
  // swap lanes
  return _mm256_permute4x64_epi64(shuffled_lanes, _MM_SHUFFLE(1, 0, 3, 2));
#else
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Short, 32> reverse(const Vec<Short, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask =
    _mm256_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17,
                    16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28, 31, 30);
  const __m256i shuffled_lanes = _mm256_shuffle_epi8(a, mask);
  // swap lanes
  return _mm256_permute4x64_epi64(shuffled_lanes, _MM_SHUFFLE(1, 0, 3, 2));
#else
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Word, 32> reverse(const Vec<Word, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask =
    _mm256_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17,
                    16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29, 28, 31, 30);
  const __m256i shuffled_lanes = _mm256_shuffle_epi8(a, mask);
  // swap lanes
  return _mm256_permute4x64_epi64(shuffled_lanes, _MM_SHUFFLE(1, 0, 3, 2));
#else
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Int, 32> reverse(const Vec<Int, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);
  return _mm256_permutevar8x32_epi32(a, mask);
#else
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Long, 32> reverse(const Vec<Long, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask = _mm256_set_epi32(1, 0, 3, 2, 5, 4, 7, 6);
  return _mm256_permutevar8x32_epi32(a, mask);
#else
  return _mm256_set_m128i(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Float, 32> reverse(const Vec<Float, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);
  return _mm256_permutevar8x32_ps(a, mask);
#else
  return _mm256_set_m128(reverse(a.lo()), reverse(a.hi()));
#endif
}

static SIMD_INLINE Vec<Double, 32> reverse(const Vec<Double, 32> &a)
{
#ifdef __AVX2__
  const __m256i mask = _mm256_set_epi32(1, 0, 3, 2, 5, 4, 7, 6);
  return _mm256_castps_pd(_mm256_permutevar8x32_ps(_mm256_castpd_ps(a), mask));
#else
  return _mm256_set_m128d(reverse(a.lo()), reverse(a.hi()));
#endif
}

// ---------------------------------------------------------------------------
// msb2int
// ---------------------------------------------------------------------------

// 26. Aug 22 (Jonas Keller): added msb2int functions

// 16. Aug 23 (Jonas Keller): fixed bug in msb2int for Byte and SignedByte
// caused by trying to cast an int to uint64_t which internally first casts to
// int64_t and then to uint64_t, which causes sign extension

template <typename T,
          SIMD_ENABLE_IF(std::is_integral<T>::value && sizeof(T) == 1)>
static SIMD_INLINE uint64_t msb2int(const Vec<T, 32> &a)
{
  // first convert to uint32_t to avoid sign extension
#ifdef __AVX2__
  const auto res = _mm256_movemask_epi8(a);
#else
  const auto res =
    _mm_movemask_epi8(a.lo()) | (_mm_movemask_epi8(a.hi()) << 16);
#endif
  // prevent sign extension when casting to uint64_t by first casting to uint
  return uint64_t(uint(res));
}

template <typename T,
          SIMD_ENABLE_IF(std::is_integral<T>::value && sizeof(T) == 2),
          typename = void>
static SIMD_INLINE uint64_t msb2int(const Vec<T, 32> &a)
{
  // there is no _mm256_movemask_epi16, so use _mm256_movemask_epi8
  // and discard the even bits
  // discarding the even bytes in a with a shuffle does not work,
  // since shuffle shuffles within 128 bit lanes
  // TODO: better way to do this?
#ifdef __AVX2__
  uint64_t x = _mm256_movemask_epi8(a);
#else
  uint64_t x = _mm_movemask_epi8(a.lo()) | (_mm_movemask_epi8(a.hi()) << 16);
#endif
  // idea from: https://stackoverflow.com/a/45695465/8461272
  // x = 0b a.b. c.d. e.f. g.h. i.j. k.l. m.n. o.p.
  // where a,b,c,d,... are the bits we care about and . represents
  // the bits we don't care about

  x >>= 1;
  // x = 0b .a.b .c.d .e.f .g.h .i.j .k.l .m.n .o.p

  x = ((x & 0x44444444) >> 1) | (x & 0x11111111);
  // x = 0b ..ab ..cd ..ef ..gh ..ij ..kl ..mn ..op

  x = ((x & 0x30303030) >> 2) | (x & 0x03030303);
  // x = 0b .... abcd .... efgh .... ijkl .... mnop

  x = ((x & 0x0F000F00) >> 4) | (x & 0x000F000F);
  // x = 0b .... .... abcd efgh .... .... ijkl mnop

  x = ((x & 0x00FF0000) >> 8) | (x & 0x000000FF);
  // x = 0b .... .... .... .... abcd efgh ijkl mnop

  return x;
}

static SIMD_INLINE uint64_t msb2int(const Vec<Int, 32> &a)
{
  return _mm256_movemask_ps(_mm256_castsi256_ps(a));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Long, 32> &a)
{
  return _mm256_movemask_pd(_mm256_castsi256_pd(a));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Float, 32> &a)
{
  return _mm256_movemask_ps(a);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Double, 32> &a)
{
  return _mm256_movemask_pd(a);
}

// ---------------------------------------------------------------------------
// int2msb
// ---------------------------------------------------------------------------

// 06. Oct 22 (Jonas Keller): added int2msb functions

static SIMD_INLINE Vec<Byte, 32> int2msb(const uint64_t a, OutputType<Byte>,
                                         Integer<32>)
{
#ifdef __AVX2__
  const __m256i shuffleIndeces = _mm256_set_epi64x(
    0x0303030303030303, 0x0202020202020202, 0x0101010101010101, 0);
  const __m256i aVec =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a), shuffleIndeces);
  const __m256i sel      = _mm256_set1_epi64x(0x8040201008040201);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  const __m256i result   = _mm256_cmpeq_epi8(selected, sel);
  return _mm256_and_si256(result, _mm256_set1_epi8((int8_t) 0x80));
#else
  const __m128i shuffleIndeces = _mm_set_epi64x(0x0101010101010101, 0);
  const __m128i aVecLo = _mm_shuffle_epi8(_mm_cvtsi32_si128(a), shuffleIndeces);
  const __m128i aVecHi =
    _mm_shuffle_epi8(_mm_cvtsi32_si128(a >> 16), shuffleIndeces);
  const __m128i sel        = _mm_set1_epi64x(0x8040201008040201);
  const __m128i selectedLo = _mm_and_si128(aVecLo, sel);
  const __m128i selectedHi = _mm_and_si128(aVecHi, sel);
  const __m128i resultLo   = _mm_cmpeq_epi8(selectedLo, sel);
  const __m128i resultHi   = _mm_cmpeq_epi8(selectedHi, sel);
  const __m256i result     = _mm256_set_m128i(resultHi, resultLo);
  return _mm256_castps_si256(
    _mm256_and_ps(_mm256_castsi256_ps(result),
                  _mm256_castsi256_ps(_mm256_set1_epi8((int8_t) 0x80))));
#endif
}

static SIMD_INLINE Vec<SignedByte, 32> int2msb(const uint64_t a,
                                               OutputType<SignedByte>,
                                               Integer<32>)
{
  return reinterpret(int2msb(a, OutputType<Byte>(), Integer<32>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 32> int2msb(const uint64_t a, OutputType<Short>,
                                          Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec = _mm256_set1_epi16(a);
  const __m256i sel  = _mm256_set_epi16(
    (int16_t) 0x8000, 0x4000, 0x2000, 0x1000, 0x0800, 0x0400, 0x0200, 0x0100,
    0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004, 0x0002, 0x0001);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  const __m256i result   = _mm256_cmpeq_epi16(selected, sel);
  return _mm256_and_si256(result, _mm256_set1_epi16((int16_t) 0x8000));
#else
  const __m128i aVec  = _mm_set1_epi16(a);
  const __m128i selLo = _mm_set_epi16(0x0080, 0x0040, 0x0020, 0x0010, 0x0008,
                                      0x0004, 0x0002, 0x0001);
  const __m128i selHi = _mm_set_epi16((int16_t) 0x8000, 0x4000, 0x2000, 0x1000,
                                      0x0800, 0x0400, 0x0200, 0x0100);
  const __m128i selectedLo = _mm_and_si128(aVec, selLo);
  const __m128i selectedHi = _mm_and_si128(aVec, selHi);
  const __m128i resultLo   = _mm_cmpeq_epi16(selectedLo, selLo);
  const __m128i resultHi   = _mm_cmpeq_epi16(selectedHi, selHi);
  const __m256i result     = _mm256_set_m128i(resultHi, resultLo);
  return _mm256_castps_si256(
    _mm256_and_ps(_mm256_castsi256_ps(result),
                  _mm256_castsi256_ps(_mm256_set1_epi16((int16_t) 0x8000))));
#endif
}

static SIMD_INLINE Vec<Word, 32> int2msb(const uint64_t a, OutputType<Word>,
                                         Integer<32>)
{
  return reinterpret(int2msb(a, OutputType<Short>(), Integer<32>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 32> int2msb(const uint64_t a, OutputType<Int>,
                                        Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec = _mm256_set1_epi32(a);
  const __m256i sel =
    _mm256_set_epi32(0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  const __m256i result   = _mm256_cmpeq_epi32(selected, sel);
  return _mm256_and_si256(result, _mm256_set1_epi32(0x80000000));
#else
  const __m128i aVec       = _mm_set1_epi32(a);
  const __m128i selLo      = _mm_set_epi32(0x08, 0x04, 0x02, 0x01);
  const __m128i selHi      = _mm_set_epi32(0x80, 0x40, 0x20, 0x10);
  const __m128i selectedLo = _mm_and_si128(aVec, selLo);
  const __m128i selectedHi = _mm_and_si128(aVec, selHi);
  const __m256i result = _mm256_set_m128i(_mm_cmpeq_epi32(selectedHi, selHi),
                                          _mm_cmpeq_epi32(selectedLo, selLo));
  return _mm256_castps_si256(
    _mm256_and_ps(_mm256_castsi256_ps(result),
                  _mm256_castsi256_ps(_mm256_set1_epi32(0x80000000))));
#endif
}

static SIMD_INLINE Vec<Long, 32> int2msb(const uint64_t a, OutputType<Long>,
                                         Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec     = _mm256_set1_epi64x(a);
  const __m256i sel      = _mm256_set_epi64x(8, 4, 2, 1);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  const __m256i result   = _mm256_cmpeq_epi64(selected, sel);
  return _mm256_and_si256(result, _mm256_set1_epi64x(0x8000000000000000));
#else
  const __m128i aVec       = _mm_set1_epi64x(a);
  const __m128i selLo      = _mm_set_epi64x(2, 1);
  const __m128i selHi      = _mm_set_epi64x(8, 4);
  const __m128i selectedLo = _mm_and_si128(aVec, selLo);
  const __m128i selectedHi = _mm_and_si128(aVec, selHi);
  const __m256i result = _mm256_set_m128i(_mm_cmpeq_epi64(selectedHi, selHi),
                                          _mm_cmpeq_epi64(selectedLo, selLo));
  return _mm256_castpd_si256(
    _mm256_and_pd(_mm256_castsi256_pd(result),
                  _mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000))));
#endif
}

static SIMD_INLINE Vec<Float, 32> int2msb(const uint64_t a, OutputType<Float>,
                                          Integer<32>)
{
  return reinterpret(int2msb(a, OutputType<Int>(), Integer<32>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 32> int2msb(const uint64_t a, OutputType<Double>,
                                           Integer<32>)
{
  return reinterpret(int2msb(a, OutputType<Long>(), Integer<32>()),
                     OutputType<Double>());
}

// ---------------------------------------------------------------------------
// int2bits
// ---------------------------------------------------------------------------

// 09. Oct 22 (Jonas Keller): added int2bits

static SIMD_INLINE Vec<Byte, 32> int2bits(const uint64_t a, OutputType<Byte>,
                                          Integer<32>)
{
#ifdef __AVX2__
  const __m256i shuffleIndeces = _mm256_set_epi64x(
    0x0303030303030303, 0x0202020202020202, 0x0101010101010101, 0);
  const __m256i aVec =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a), shuffleIndeces);
  const __m256i sel      = _mm256_set1_epi64x(0x8040201008040201);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  return _mm256_cmpeq_epi8(selected, sel);
#else
  return _mm256_set_m128i(int2bits(a >> 16, OutputType<Byte>(), Integer<16>()),
                          int2bits(a, OutputType<Byte>(), Integer<16>()));
#endif
}

static SIMD_INLINE Vec<SignedByte, 32> int2bits(const uint64_t a,
                                                OutputType<SignedByte>,
                                                Integer<32>)
{
  return reinterpret(int2bits(a, OutputType<Byte>(), Integer<32>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 32> int2bits(const uint64_t a, OutputType<Short>,
                                           Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec = _mm256_set1_epi16(a);
  const __m256i sel  = _mm256_set_epi16(
    (int16_t) 0x8000, 0x4000, 0x2000, 0x1000, 0x0800, 0x0400, 0x0200, 0x0100,
    0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004, 0x0002, 0x0001);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  return _mm256_cmpeq_epi16(selected, sel);
#else
  return _mm256_set_m128i(int2bits(a >> 8, OutputType<Short>(), Integer<16>()),
                          int2bits(a, OutputType<Short>(), Integer<16>()));
#endif
}

static SIMD_INLINE Vec<Word, 32> int2bits(const uint64_t a, OutputType<Word>,
                                          Integer<32>)
{
  return reinterpret(int2bits(a, OutputType<Short>(), Integer<32>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 32> int2bits(const uint64_t a, OutputType<Int>,
                                         Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec = _mm256_set1_epi32(a);
  const __m256i sel =
    _mm256_set_epi32(0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  return _mm256_cmpeq_epi32(selected, sel);
#else
  return _mm256_set_m128i(int2bits(a >> 4, OutputType<Int>(), Integer<16>()),
                          int2bits(a, OutputType<Int>(), Integer<16>()));
#endif
}

static SIMD_INLINE Vec<Long, 32> int2bits(const uint64_t a, OutputType<Long>,
                                          Integer<32>)
{
#ifdef __AVX2__
  const __m256i aVec     = _mm256_set1_epi64x(a);
  const __m256i sel      = _mm256_set_epi64x(8, 4, 2, 1);
  const __m256i selected = _mm256_and_si256(aVec, sel);
  return _mm256_cmpeq_epi64(selected, sel);
#else
  const __m128i aVec       = _mm_set1_epi64x(a);
  const __m128i selLo      = _mm_set_epi64x(2, 1);
  const __m128i selHi      = _mm_set_epi64x(8, 4);
  const __m128i selectedLo = _mm_and_si128(aVec, selLo);
  const __m128i selectedHi = _mm_and_si128(aVec, selHi);
  return _mm256_set_m128i(_mm_cmpeq_epi64(selectedHi, selHi),
                          _mm_cmpeq_epi64(selectedLo, selLo));
#endif
}

static SIMD_INLINE Vec<Float, 32> int2bits(const uint64_t a, OutputType<Float>,
                                           Integer<32>)
{
  return reinterpret(int2bits(a, OutputType<Int>(), Integer<32>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 32> int2bits(const uint64_t a,
                                            OutputType<Double>, Integer<32>)
{
  return reinterpret(int2bits(a, OutputType<Long>(), Integer<32>()),
                     OutputType<Double>());
}

// ---------------------------------------------------------------------------
// iota
// ---------------------------------------------------------------------------

// 30. Jan 23 (Jonas Keller): added iota

static SIMD_INLINE Vec<Byte, 32> iota(OutputType<Byte>, Integer<32>)
{
  return _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18,
                         17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,
                         1, 0);
}

static SIMD_INLINE Vec<SignedByte, 32> iota(OutputType<SignedByte>, Integer<32>)
{
  return _mm256_set_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18,
                         17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,
                         1, 0);
}

static SIMD_INLINE Vec<Short, 32> iota(OutputType<Short>, Integer<32>)
{
  return _mm256_set_epi16(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Word, 32> iota(OutputType<Word>, Integer<32>)
{
  return _mm256_set_epi16(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Int, 32> iota(OutputType<Int>, Integer<32>)
{
  return _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Long, 32> iota(OutputType<Long>, Integer<32>)
{
  return _mm256_set_epi64x(3, 2, 1, 0);
}

static SIMD_INLINE Vec<Float, 32> iota(OutputType<Float>, Integer<32>)
{
  return _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);
}

static SIMD_INLINE Vec<Double, 32> iota(OutputType<Double>, Integer<32>)
{
  return _mm256_set_pd(3.0, 2.0, 1.0, 0.0);
}

} // namespace base
} // namespace internal
} // namespace simd

#endif

#endif // SIMD_VEC_BASE_IMPL_INTEL_32_H_

// ===========================================================================
//
// SIMDVecBaseImplIntel64.H --
// encapsulation for AVX512 Intel vector extensions
// inspired by Agner Fog's C++ Vector Class Library
// http://www.agner.org/optimize/#vectorclass
// (VCL License: GNU General Public License Version 3,
//  http://www.gnu.org/licenses/gpl-3.0.en.html)
//
// Changes to unpack, zip and unzip functions in 2022 by
//   Jan-Lukas Wolf (jawolf@techfak.uni-bielefeld.de)
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace
// 13. May 23 (Jonas Keller): added Double support

#ifndef SIMD_VEC_BASE_IMPL_INTEL_64_H_
#define SIMD_VEC_BASE_IMPL_INTEL_64_H_

#include <cstddef>
#include <cstdint>
#include <limits>
#include <type_traits>

#if defined(SIMDVEC_INTEL_ENABLE) && defined(_SIMD_VEC_64_AVAIL_) &&           \
  !defined(SIMDVEC_SANDBOX)

namespace simd {

// ===========================================================================
// NOTES:
//
// - setting zero inside the function is not inefficient, see:
//   http://stackoverflow.com/questions/26807285/...
//   ...are-static-static-local-sse-avx-variables-blocking-a-xmm-ymm-register
//
// - for some data types (Int, Float) there are no saturated versions
//   of add/sub instructions; in this case we use the unsaturated version;
//   the user is responsible to avoid overflows
// ===========================================================================

// ===========================================================================
// Vec integer specialization for AVX512 v
// ===========================================================================

// partial specialization for SIMD_WIDTH = 64
template <typename T>
class Vec<T, 64>
{
  __m512i zmm = _mm512_setzero_si512();

public:
  using Type                       = T;
  static constexpr size_t elements = 64 / sizeof(T);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 64;

  Vec() = default;
  Vec(const __m512i &x) { zmm = x; }
  Vec &operator=(const __m512i &x)
  {
    zmm = x;
    return *this;
  }
  operator __m512i() const { return zmm; }
  // for avx512bw emulation
  Vec(const Vec<T, 32> &lo, const Vec<T, 32> &hi)
  {
    zmm = _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
  }
  SIMD_INLINE Vec<T, 32> lo() const { return _mm512_castsi512_si256(zmm); }
  SIMD_INLINE Vec<T, 32> hi() const
  {
    return _mm512_extracti64x4_epi64(zmm, 1);
  }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<T, bytes>, bytes>;
};

// ===========================================================================
// Vec float specialization for AVX512 v
// ===========================================================================

template <>
class Vec<Float, 64>
{
  __m512 zmm = _mm512_setzero_ps();

public:
  using Type                       = Float;
  static constexpr size_t elements = 64 / sizeof(Float);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 64;

  Vec() = default;
  Vec(const __m512 &x) { zmm = x; }
  Vec &operator=(const __m512 &x)
  {
    zmm = x;
    return *this;
  }
  operator __m512() const { return zmm; }
  // for avx512bw emulation
  Vec(const Vec<Float, 32> &lo, const Vec<Float, 32> &hi)
  {
    zmm = _mm512_castpd_ps(_mm512_insertf64x4(
      _mm512_castps_pd(_mm512_castps256_ps512(lo)), _mm256_castps_pd(hi), 1));
  }
  SIMD_INLINE Vec<Float, 32> lo() const { return _mm512_castps512_ps256(zmm); }
  // _mm512_extractf32x8_ps only in AVX512DQ
  SIMD_INLINE Vec<Float, 32> hi() const
  {
    return _mm256_castpd_ps(_mm512_extractf64x4_pd(_mm512_castps_pd(zmm), 1));
  }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<Float, bytes>, bytes>;
};

// ===========================================================================
// Vec double specialization for AVX512 v
// ===========================================================================

template <>
class Vec<Double, 64>
{
  __m512d zmm;

public:
  using Type                       = Double;
  static constexpr size_t elements = 64 / sizeof(Double);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 64;

  Vec() = default;
  Vec(const __m512d &x) { zmm = x; }
  Vec &operator=(const __m512d &x)
  {
    zmm = x;
    return *this;
  }
  operator __m512d() const { return zmm; }
  // for avx512bw emulation
  Vec(const Vec<Double, 32> &lo, const Vec<Double, 32> &hi)
  {
    zmm = _mm512_insertf64x4(_mm512_castpd256_pd512(lo), hi, 1);
  }
  SIMD_INLINE Vec<Double, 32> lo() const { return _mm512_castpd512_pd256(zmm); }
  SIMD_INLINE Vec<Double, 32> hi() const
  {
    return _mm512_extractf64x4_pd(zmm, 1);
  }
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  using allocator = simd_aligned_allocator<Vec<Double, bytes>, bytes>;
};

namespace internal {
namespace base {

// ===========================================================================
// auxiliary functions
// ===========================================================================

// These functions either wrap  intrinsics (e.g. to handle
// immediate arguments as template parameter), or switch between
// implementations with different SSE* extensions, or provide
// altered or additional functionality.
// Only for use in wrapper functions!

// 01. Apr 23 (Jonas Keller): removed some not really necessary internal
// wrapper functions and inlined them directly into where they were used

// ---------------------------------------------------------------------------
// alignr v
// ---------------------------------------------------------------------------

// 21. Apr 23 (Jonas Keller): replaced IMM range handling via tag dispatch
// with static_assert, since we don't need the range handling anymore,
// we just assert that IMM is in range

template <size_t COUNT>
static SIMD_INLINE __m512i x_mm512_alignr_epi8(__m512i h, __m512i l)
{
  static_assert(COUNT < 32, "");
#ifdef __AVX512BW__
  return _mm512_alignr_epi8(h, l, COUNT);
#else
  // non-avx512bw workarounds
  // (easy since AVX512BW instructions operate on lanes anyhow)
  const __m256i lo = _mm256_alignr_epi8(_mm512_castsi512_si256(h),
                                        _mm512_castsi512_si256(l), COUNT);
  const __m256i hi = _mm256_alignr_epi8(_mm512_extracti64x4_epi64(h, 1),
                                        _mm512_extracti64x4_epi64(l, 1), COUNT);
  return _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
#endif
}

// ---------------------------------------------------------------------------
// transpose8x64 v
// ---------------------------------------------------------------------------

static SIMD_INLINE __m512i x_mm512_transpose8x64_epi64(__m512i a)
{
  return _mm512_permutexvar_epi64(_mm512_set_epi64(7, 3, 6, 2, 5, 1, 4, 0), a);
}

// ---------------------------------------------------------------------------
// evenodd8x64 v
// ---------------------------------------------------------------------------

static SIMD_INLINE __m512i x_mm512_evenodd8x64_epi64(__m512i a)
{
  return _mm512_permutexvar_epi64(_mm512_set_epi64(7, 5, 3, 1, 6, 4, 2, 0), a);
}

// ---------------------------------------------------------------------------
// binary functions with non-avx512bw workarounds v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
// avx512bw is available
#define SIMD_X_BW_INT_BINFCT_64(INTRIN)                                        \
  static SIMD_INLINE __m512i x_mm512_##INTRIN(__m512i a, __m512i b)            \
  {                                                                            \
    return _mm512_##INTRIN(a, b);                                              \
  }
#else
// non-avx512bw workaround
#define SIMD_X_BW_INT_BINFCT_64(INTRIN)                                        \
  static SIMD_INLINE __m512i x_mm512_##INTRIN(__m512i a, __m512i b)            \
  {                                                                            \
    const __m256i lo =                                                         \
      _mm256_##INTRIN(_mm512_castsi512_si256(a), _mm512_castsi512_si256(b));   \
    const __m256i hi = _mm256_##INTRIN(_mm512_extracti64x4_epi64(a, 1),        \
                                       _mm512_extracti64x4_epi64(b, 1));       \
    return _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);              \
  }
#endif

SIMD_X_BW_INT_BINFCT_64(unpacklo_epi8)
SIMD_X_BW_INT_BINFCT_64(unpackhi_epi8)
SIMD_X_BW_INT_BINFCT_64(unpacklo_epi16)
SIMD_X_BW_INT_BINFCT_64(unpackhi_epi16)
SIMD_X_BW_INT_BINFCT_64(shuffle_epi8)
SIMD_X_BW_INT_BINFCT_64(packs_epi16)
SIMD_X_BW_INT_BINFCT_64(packs_epi32)
SIMD_X_BW_INT_BINFCT_64(packus_epi16)
SIMD_X_BW_INT_BINFCT_64(packus_epi32)

// ---------------------------------------------------------------------------
// non-existing avx512 functions emulated via avx v
// ---------------------------------------------------------------------------

// ---------------------------------------------------------------------------
// x_mm512_movm_epi32 v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

static SIMD_INLINE __m512i x_mm512_movm_epi32(__mmask16 k)
{
#ifdef __AVX512DQ__
  return _mm512_movm_epi32(k);
#else
  return _mm512_maskz_mov_epi32(k, _mm512_set1_epi32(-1));
#endif
}

// ---------------------------------------------------------------------------
// x_mm512_movm_epi64 v
// ---------------------------------------------------------------------------

static SIMD_INLINE __m512i x_mm512_movm_epi64(__mmask8 k)
{
#ifdef __AVX512DQ__
  return _mm512_movm_epi64(k);
#else
  return _mm512_maskz_mov_epi64(k, _mm512_set1_epi64(-1));
#endif
}

// ###########################################################################
// ###########################################################################
// ###########################################################################

// ===========================================================================
// Vec template function specializations or overloading for AVX
// ===========================================================================

// ---------------------------------------------------------------------------
// reinterpretation casts v
// ---------------------------------------------------------------------------

// 08. Apr 23 (Jonas Keller): used enable_if for cleaner implementation

// between all integer types
template <typename Tdst, typename Tsrc,
          SIMD_ENABLE_IF((!std::is_same<Tdst, Tsrc>::value &&
                          std::is_integral<Tdst>::value &&
                          std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Tdst, 64> reinterpret(const Vec<Tsrc, 64> &vec,
                                             OutputType<Tdst>)
{
  // 26. Nov 22 (Jonas Keller): reinterpret_cast is technically undefined
  // behavior, so just rewrapping the vector register in a new Vec instead
  // return reinterpret_cast<const Vec<Tdst,64>&>(vec);
  return Vec<Tdst, 64>(__m512i(vec));
}

// from float to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 64> reinterpret(const Vec<Float, 64> &vec,
                                             OutputType<Tdst>)
{
  return _mm512_castps_si512(vec);
}

// from any integer type to float
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Float, 64> reinterpret(const Vec<Tsrc, 64> &vec,
                                              OutputType<Float>)
{
  return _mm512_castsi512_ps(vec);
}

// from double to any integer type
template <typename Tdst, SIMD_ENABLE_IF((std::is_integral<Tdst>::value))>
static SIMD_INLINE Vec<Tdst, 64> reinterpret(const Vec<Double, 64> &vec,
                                             OutputType<Tdst>)
{
  return _mm512_castpd_si512(vec);
}

// from any integer type to double
template <typename Tsrc, SIMD_ENABLE_IF((std::is_integral<Tsrc>::value))>
static SIMD_INLINE Vec<Double, 64> reinterpret(const Vec<Tsrc, 64> &vec,
                                               OutputType<Double>)
{
  return _mm512_castsi512_pd(vec);
}

// from float to double
static SIMD_INLINE Vec<Double, 64> reinterpret(const Vec<Float, 64> &vec,
                                               OutputType<Double>)
{
  return _mm512_castps_pd(vec);
}

// from double to float
static SIMD_INLINE Vec<Float, 64> reinterpret(const Vec<Double, 64> &vec,
                                              OutputType<Float>)
{
  return _mm512_castpd_ps(vec);
}

// between identical types
template <typename T>
static SIMD_INLINE Vec<T, 64> reinterpret(const Vec<T, 64> &vec, OutputType<T>)
{
  return vec;
}

// ---------------------------------------------------------------------------
// convert (without changes in the number of of elements) v
// ---------------------------------------------------------------------------

// conversion with saturation; we wanted to have a fast solution that
// doesn't trigger the overflow which results in a negative two's
// complement result ("invalid int32": 0x80000000); therefore we clamp
// the positive values at the maximal positive float which is
// convertible to int32 without overflow (0x7fffffbf = 2147483520);
// negative values cannot overflow (they are clamped to invalid int
// which is the most negative int32)
static SIMD_INLINE Vec<Int, 64> cvts(const Vec<Float, 64> &a, OutputType<Int>)
{
  // TODO: analyze much more complex solution for cvts at
  // TODO: http://stackoverflow.com/questions/9157373/
  // TODO: most-efficient-way-to-convert-vector-of-float-to-vector-of-uint32
  __m512 clip = _mm512_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT32);
  return _mm512_cvtps_epi32(_mm512_min_ps(clip, a));
}

// saturation is not necessary in this case
static SIMD_INLINE Vec<Float, 64> cvts(const Vec<Int, 64> &a, OutputType<Float>)
{
  return _mm512_cvtepi32_ps(a);
}

static SIMD_INLINE Vec<Long, 64> cvts(const Vec<Double, 64> &a,
                                      OutputType<Long>)
{
  const auto clip    = _mm512_set1_pd(MAX_POS_DOUBLE_CONVERTIBLE_TO_INT64);
  const auto clipped = _mm512_min_pd(clip, a);
#ifdef __AVX512DQ__
  return _mm512_cvtpd_epi64(clipped);
#else
  // workaround from https://stackoverflow.com/a/41148578 only works for
  // values in range [-2^52, 2^52]
  // using serial workaround instead
  // TODO: serial workaround is slow, find parallel workaround
  Double tmpD[8] SIMD_ATTR_ALIGNED(64);
  _mm512_store_pd(tmpD, clipped);
  Long tmpL[8] SIMD_ATTR_ALIGNED(64);
  for (size_t i = 0; i < 8; ++i) {
    tmpL[i] = static_cast<Long>(std::rint(tmpD[i]));
  }
  return _mm512_load_si512((__m512i *) tmpL);
#endif
}

static SIMD_INLINE Vec<Double, 64> cvts(const Vec<Long, 64> &a,
                                        OutputType<Double>)
{
#ifdef __AVX512DQ__
  return _mm512_cvtepi64_pd(a);
#else
#if 0
  // workaround from https://stackoverflow.com/a/41148578 (int64_t -> double) (modified)
  __m512i xH = _mm512_srai_epi32(a, 16);
  xH         = _mm512_and_si512(xH, _mm512_set1_epi32(0xffff0000));
  xH         = _mm512_add_epi64(
    xH, _mm512_castpd_si512(_mm512_set1_pd(442721857769029238784.))); // 3*2^67
  __m512i xL = _mm512_or_si512(
    _mm512_and_si512(a, _mm512_set1_epi64(0x0000ffffffffffff)),
    _mm512_castpd_si512(_mm512_set1_pd(0x0010000000000000))); //  2^52
  __m512d f =
    _mm512_sub_pd(_mm512_castsi512_pd(xH),
                  _mm512_set1_pd(442726361368656609280.)); //  3*2^67 + 2^52
  return _mm512_add_pd(f, _mm512_castsi512_pd(xL));
#else
  // the workaround above does not work
  // TODO: why?
  // so we use a serial workaround instead
  Long tmpL[8] SIMD_ATTR_ALIGNED(64);
  _mm512_store_si512((__m512i *) tmpL, a);
  Double tmpD[8] SIMD_ATTR_ALIGNED(64);
  for (size_t i = 0; i < 8; ++i) { tmpD[i] = static_cast<Double>(tmpL[i]); }
  return _mm512_load_pd(tmpD);
#endif
#endif
}

// ---------------------------------------------------------------------------
// setzero v
// ---------------------------------------------------------------------------

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> setzero(OutputType<T>, Integer<64>)
{
  return _mm512_setzero_si512();
}

static SIMD_INLINE Vec<Float, 64> setzero(OutputType<Float>, Integer<64>)
{
  return _mm512_setzero_ps();
}

static SIMD_INLINE Vec<Double, 64> setzero(OutputType<Double>, Integer<64>)
{
  return _mm512_setzero_pd();
}

// ---------------------------------------------------------------------------
// set1 v
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 64> set1(Byte a, Integer<64>)
{
  return _mm512_set1_epi8(a);
}

static SIMD_INLINE Vec<SignedByte, 64> set1(SignedByte a, Integer<64>)
{
  return _mm512_set1_epi8(a);
}

static SIMD_INLINE Vec<Word, 64> set1(Word a, Integer<64>)
{
  return _mm512_set1_epi16(a);
}

static SIMD_INLINE Vec<Short, 64> set1(Short a, Integer<64>)
{
  return _mm512_set1_epi16(a);
}

static SIMD_INLINE Vec<Int, 64> set1(Int a, Integer<64>)
{
  return _mm512_set1_epi32(a);
}

static SIMD_INLINE Vec<Long, 64> set1(Long a, Integer<64>)
{
  return _mm512_set1_epi64(a);
}

static SIMD_INLINE Vec<Float, 64> set1(Float a, Integer<64>)
{
  return _mm512_set1_ps(a);
}

static SIMD_INLINE Vec<Double, 64> set1(Double a, Integer<64>)
{
  return _mm512_set1_pd(a);
}

// ---------------------------------------------------------------------------
// load v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> load(const T *const p, Integer<64>)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  return _mm512_load_si512((__m512i *) p);
}

static SIMD_INLINE Vec<Float, 64> load(const Float *const p, Integer<64>)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  return _mm512_load_ps(p);
}

static SIMD_INLINE Vec<Double, 64> load(const Double *const p, Integer<64>)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  return _mm512_load_pd(p);
}

// ---------------------------------------------------------------------------
// loadu v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> loadu(const T *const p, Integer<64>)
{
  return _mm512_loadu_si512((__m512i *) p);
}

static SIMD_INLINE Vec<Float, 64> loadu(const Float *const p, Integer<64>)
{
  return _mm512_loadu_ps(p);
}

static SIMD_INLINE Vec<Double, 64> loadu(const Double *const p, Integer<64>)
{
  return _mm512_loadu_pd(p);
}

// ---------------------------------------------------------------------------
// store v
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void store(T *const p, const Vec<T, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_store_si512((__m512i *) p, a);
}

// float version
static SIMD_INLINE void store(Float *const p, const Vec<Float, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_store_ps(p, a);
}

// double version
static SIMD_INLINE void store(Double *const p, const Vec<Double, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_store_pd(p, a);
}

// ---------------------------------------------------------------------------
// storeu v
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void storeu(T *const p, const Vec<T, 64> &a)
{
  _mm512_storeu_si512((__m512i *) p, a);
}

// float version
static SIMD_INLINE void storeu(Float *const p, const Vec<Float, 64> &a)
{
  _mm512_storeu_ps(p, a);
}

// double version
static SIMD_INLINE void storeu(Double *const p, const Vec<Double, 64> &a)
{
  _mm512_storeu_pd(p, a);
}

// ---------------------------------------------------------------------------
// stream_store v
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE void stream_store(T *const p, const Vec<T, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_stream_si512((__m512i *) p, a);
}

// float version
static SIMD_INLINE void stream_store(Float *const p, const Vec<Float, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_stream_ps(p, a);
}

// double version
static SIMD_INLINE void stream_store(Double *const p, const Vec<Double, 64> &a)
{
  // AVX load and store instructions need alignment to 64 byte
  // (lower 6 bit need to be zero)
  SIMD_CHECK_ALIGNMENT(p, 64);
  _mm512_stream_pd(p, a);
}

// ---------------------------------------------------------------------------
// extract v
// ---------------------------------------------------------------------------

template <size_t COUNT>
static SIMD_INLINE Byte extract(const Vec<Byte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm_extract_epi8(_mm512_extracti32x4_epi32(a, COUNT >> 4),
                            COUNT % 16);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE SignedByte extract(const Vec<SignedByte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm_extract_epi8(_mm512_extracti32x4_epi32(a, COUNT >> 4),
                            COUNT % 16);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Word extract(const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm_extract_epi16(_mm512_extracti32x4_epi32(a, COUNT >> 3),
                             COUNT % 8);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Short extract(const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm_extract_epi16(_mm512_extracti32x4_epi32(a, COUNT >> 3),
                             COUNT % 8);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Int extract(const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm_extract_epi32(_mm512_extracti32x4_epi32(a, COUNT >> 2),
                             COUNT % 4);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Long extract(const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return _mm_extract_epi64(_mm512_extracti32x4_epi32(a, COUNT >> 1),
                             COUNT % 2);
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Float extract(const Vec<Float, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return ::simd::internal::bit_cast<Float>(
      _mm_extract_ps(_mm512_extractf32x4_ps(a, COUNT >> 2), COUNT % 4));
  } else {
    return 0;
  }
}

template <size_t COUNT>
static SIMD_INLINE Double extract(const Vec<Double, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    return ::simd::internal::bit_cast<Double>(_mm_extract_epi64(
      _mm512_extracti32x4_epi32(_mm512_castpd_si512(a), COUNT >> 1),
      COUNT % 2));
  } else {
    return 0;
  }
}

// ---------------------------------------------------------------------------
// extract 128-bit-lane as Vec<T, 16>
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// generalized extract of 128-bit-lanes (LANE_INDEX = 0..3)
template <size_t LANE_INDEX, typename T>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, 64> &a)
{
  const auto intA           = reinterpret(a, OutputType<Int>());
  const Vec<Int, 16> intRes = _mm512_extracti32x4_epi32(intA, LANE_INDEX);
  return reinterpret(intRes, OutputType<T>());
}

// ---------------------------------------------------------------------------
// add v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> add(const Vec<Byte, 64> &a,
                                     const Vec<Byte, 64> &b)
{
  return _mm512_add_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> add(const Vec<SignedByte, 64> &a,
                                           const Vec<SignedByte, 64> &b)
{
  return _mm512_add_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> add(const Vec<Word, 64> &a,
                                     const Vec<Word, 64> &b)
{
  return _mm512_add_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 64> add(const Vec<Short, 64> &a,
                                      const Vec<Short, 64> &b)
{
  return _mm512_add_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> add(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(add(a.lo(), b.lo()), add(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> add(const Vec<Int, 64> &a,
                                    const Vec<Int, 64> &b)
{
  return _mm512_add_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 64> add(const Vec<Long, 64> &a,
                                     const Vec<Long, 64> &b)
{
  return _mm512_add_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 64> add(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_add_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> add(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// adds
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> adds(const Vec<Byte, 64> &a,
                                      const Vec<Byte, 64> &b)
{
  return _mm512_adds_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> adds(const Vec<SignedByte, 64> &a,
                                            const Vec<SignedByte, 64> &b)
{
  return _mm512_adds_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> adds(const Vec<Word, 64> &a,
                                      const Vec<Word, 64> &b)
{
  return _mm512_adds_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 64> adds(const Vec<Short, 64> &a,
                                       const Vec<Short, 64> &b)
{
  return _mm512_adds_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> adds(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(adds(a.lo(), b.lo()), adds(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> adds(const Vec<Int, 64> &a,
                                     const Vec<Int, 64> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm512_adds_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  const __m512i sum             = _mm512_add_epi32(a, b);
  const __m512i opsHaveDiffSign = _mm512_xor_si512(a, b);
  const __m512i sumHasDiffSign  = _mm512_xor_si512(a, sum);
  // indicates when an overflow has occurred
  const __m512i overflow =
    _mm512_srai_epi32(_mm512_andnot_si512(opsHaveDiffSign, sumHasDiffSign), 31);
  // saturated sum for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  const __m512i saturatedSum =
    _mm512_xor_si512(_mm512_srai_epi32(a, 31), _mm512_set1_epi32(0x7FFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return _mm512_or_si512(_mm512_andnot_si512(overflow, sum),
                         _mm512_and_si512(overflow, saturatedSum));
}

static SIMD_INLINE Vec<Long, 64> adds(const Vec<Long, 64> &a,
                                      const Vec<Long, 64> &b)
{
  // _mm512_adds_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "Signed integer overflow of
  // addition occurs if and only if the operands have the same sign and the
  // sum has a sign opposite to that of the operands."
  const __m512i sum             = _mm512_add_epi64(a, b);
  const __m512i opsHaveDiffSign = _mm512_xor_si512(a, b);
  const __m512i sumHasDiffSign  = _mm512_xor_si512(a, sum);
  // indicates when an overflow has occurred
  const __m512i overflow =
    _mm512_srai_epi64(_mm512_andnot_si512(opsHaveDiffSign, sumHasDiffSign), 63);
  // saturated sum for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0, 0x8000000000000000=min
  // negative long, when sign of a (and thus b as well) is 1)
  const __m512i saturatedSum = _mm512_xor_si512(
    _mm512_srai_epi64(a, 63), _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF));
  // return saturated sum if overflow occurred, otherwise return sum
  return _mm512_or_si512(_mm512_andnot_si512(overflow, sum),
                         _mm512_and_si512(overflow, saturatedSum));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 64> adds(const Vec<Float, 64> &a,
                                       const Vec<Float, 64> &b)
{
  return _mm512_add_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 64> adds(const Vec<Double, 64> &a,
                                        const Vec<Double, 64> &b)
{
  return _mm512_add_pd(a, b);
}

// ---------------------------------------------------------------------------
// sub v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> sub(const Vec<Byte, 64> &a,
                                     const Vec<Byte, 64> &b)
{
  return _mm512_sub_epi8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> sub(const Vec<SignedByte, 64> &a,
                                           const Vec<SignedByte, 64> &b)
{
  return _mm512_sub_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> sub(const Vec<Word, 64> &a,
                                     const Vec<Word, 64> &b)
{
  return _mm512_sub_epi16(a, b);
}

static SIMD_INLINE Vec<Short, 64> sub(const Vec<Short, 64> &a,
                                      const Vec<Short, 64> &b)
{
  return _mm512_sub_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> sub(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(sub(a.lo(), b.lo()), sub(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> sub(const Vec<Int, 64> &a,
                                    const Vec<Int, 64> &b)
{
  return _mm512_sub_epi32(a, b);
}

static SIMD_INLINE Vec<Long, 64> sub(const Vec<Long, 64> &a,
                                     const Vec<Long, 64> &b)
{
  return _mm512_sub_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 64> sub(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_sub_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> sub(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// subs
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> subs(const Vec<Byte, 64> &a,
                                      const Vec<Byte, 64> &b)
{
  return _mm512_subs_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> subs(const Vec<SignedByte, 64> &a,
                                            const Vec<SignedByte, 64> &b)
{
  return _mm512_subs_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> subs(const Vec<Word, 64> &a,
                                      const Vec<Word, 64> &b)
{
  return _mm512_subs_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 64> subs(const Vec<Short, 64> &a,
                                       const Vec<Short, 64> &b)
{
  return _mm512_subs_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> subs(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(subs(a.lo(), b.lo()), subs(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> subs(const Vec<Int, 64> &a,
                                     const Vec<Int, 64> &b)
{
  // 09. Mar 23 (Jonas Keller): added workaround so that this function is
  // saturated

  // _mm512_subs_epi32 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  const __m512i diff            = _mm512_sub_epi32(a, b);
  const __m512i opsHaveDiffSign = _mm512_xor_si512(a, b);
  const __m512i diffHasDiffSign = _mm512_xor_si512(a, diff);
  // indicates when an overflow has occurred
  const __m512i overflow =
    _mm512_srai_epi32(_mm512_and_si512(opsHaveDiffSign, diffHasDiffSign), 31);
  // saturated diff for if overflow occurred (0x7FFFFFFF=max positive int, when
  // sign of a (and thus b as well) is 0, 0x80000000=min negative int, when sign
  // of a (and thus b as well) is 1)
  const __m512i saturatedDiff =
    _mm512_xor_si512(_mm512_srai_epi32(a, 31), _mm512_set1_epi32(0x7FFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return _mm512_or_si512(_mm512_andnot_si512(overflow, diff),
                         _mm512_and_si512(overflow, saturatedDiff));
}

static SIMD_INLINE Vec<Long, 64> subs(const Vec<Long, 64> &a,
                                      const Vec<Long, 64> &b)
{
  // _mm512_subs_epi64 does not exist, workaround:
  // Hacker's Delight, 2-13 Overflow Detection: "[...] overflow in the final
  // value of x−y [...] occurs if and only if x and y have opposite signs and
  // the sign of x−y [...] is opposite to that of x [...]"
  const __m512i diff            = _mm512_sub_epi64(a, b);
  const __m512i opsHaveDiffSign = _mm512_xor_si512(a, b);
  const __m512i diffHasDiffSign = _mm512_xor_si512(a, diff);
  // indicates when an overflow has occurred
  const __m512i overflow =
    _mm512_srai_epi64(_mm512_and_si512(opsHaveDiffSign, diffHasDiffSign), 63);
  // saturated diff for if overflow occurred (0x7FFFFFFFFFFFFFFF=max positive
  // long, when sign of a (and thus b as well) is 0, 0x8000000000000000=min
  // negative long, when sign of a (and thus b as well) is 1)
  const __m512i saturatedDiff = _mm512_xor_si512(
    _mm512_srai_epi64(a, 63), _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF));
  // return saturated diff if overflow occurred, otherwise return diff
  return _mm512_or_si512(_mm512_andnot_si512(overflow, diff),
                         _mm512_and_si512(overflow, saturatedDiff));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 64> subs(const Vec<Float, 64> &a,
                                       const Vec<Float, 64> &b)
{
  return _mm512_sub_ps(a, b);
}

// Double not saturated
static SIMD_INLINE Vec<Double, 64> subs(const Vec<Double, 64> &a,
                                        const Vec<Double, 64> &b)
{
  return _mm512_sub_pd(a, b);
}

// ---------------------------------------------------------------------------
// neg (negate = two's complement or unary minus), only signed types v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<SignedByte, 64> neg(const Vec<SignedByte, 64> &a)
{
  return _mm512_sub_epi8(_mm512_setzero_si512(), a);
}

static SIMD_INLINE Vec<Short, 64> neg(const Vec<Short, 64> &a)
{
  return _mm512_sub_epi16(_mm512_setzero_si512(), a);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> neg(const Vec<T, 64> &a)
{
  return Vec<T, 64>(neg(a.lo()), neg(a.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> neg(const Vec<Int, 64> &a)
{
  return _mm512_sub_epi32(_mm512_setzero_si512(), a);
}

static SIMD_INLINE Vec<Long, 64> neg(const Vec<Long, 64> &a)
{
  return _mm512_sub_epi64(_mm512_setzero_si512(), a);
}

static SIMD_INLINE Vec<Float, 64> neg(const Vec<Float, 64> &a)
{
  // xor has better latency than sub
  return _mm512_castsi512_ps(
    _mm512_xor_si512(_mm512_set1_epi32(0x80000000), _mm512_castps_si512(a)));
}

static SIMD_INLINE Vec<Double, 64> neg(const Vec<Double, 64> &a)
{
  // xor has better latency than sub
  return _mm512_castsi512_pd(_mm512_xor_si512(
    _mm512_set1_epi64(0x8000000000000000), _mm512_castpd_si512(a)));
}

// ---------------------------------------------------------------------------
// min v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> min(const Vec<Byte, 64> &a,
                                     const Vec<Byte, 64> &b)
{
  return _mm512_min_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> min(const Vec<SignedByte, 64> &a,
                                           const Vec<SignedByte, 64> &b)
{
  return _mm512_min_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> min(const Vec<Word, 64> &a,
                                     const Vec<Word, 64> &b)
{
  return _mm512_min_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 64> min(const Vec<Short, 64> &a,
                                      const Vec<Short, 64> &b)
{
  return _mm512_min_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> min(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(min(a.lo(), b.lo()), min(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> min(const Vec<Int, 64> &a,
                                    const Vec<Int, 64> &b)
{
  return _mm512_min_epi32(a, b);
}

// there is an unsigned version of min for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 64> min(const Vec<Long, 64> &a,
                                     const Vec<Long, 64> &b)
{
  return _mm512_min_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 64> min(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_min_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> min(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_min_pd(a, b);
}

// ---------------------------------------------------------------------------
// max v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> max(const Vec<Byte, 64> &a,
                                     const Vec<Byte, 64> &b)
{
  return _mm512_max_epu8(a, b);
}

static SIMD_INLINE Vec<SignedByte, 64> max(const Vec<SignedByte, 64> &a,
                                           const Vec<SignedByte, 64> &b)
{
  return _mm512_max_epi8(a, b);
}

static SIMD_INLINE Vec<Word, 64> max(const Vec<Word, 64> &a,
                                     const Vec<Word, 64> &b)
{
  return _mm512_max_epu16(a, b);
}

static SIMD_INLINE Vec<Short, 64> max(const Vec<Short, 64> &a,
                                      const Vec<Short, 64> &b)
{
  return _mm512_max_epi16(a, b);
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> max(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(max(a.lo(), b.lo()), max(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> max(const Vec<Int, 64> &a,
                                    const Vec<Int, 64> &b)
{
  return _mm512_max_epi32(a, b);
}

// there is an unsigned version of max for 32 bit but we currently
// don't have an element type for it

static SIMD_INLINE Vec<Long, 64> max(const Vec<Long, 64> &a,
                                     const Vec<Long, 64> &b)
{
  return _mm512_max_epi64(a, b);
}

static SIMD_INLINE Vec<Float, 64> max(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_max_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> max(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_max_pd(a, b);
}

// ---------------------------------------------------------------------------
// mul, div v
// ---------------------------------------------------------------------------

// TODO: add mul/div versions for int types? or make special versions of mul
// TODO: and div where the result is scaled?

static SIMD_INLINE Vec<Float, 64> mul(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_mul_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> mul(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_mul_pd(a, b);
}

static SIMD_INLINE Vec<Float, 64> div(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_div_ps(a, b);
}

static SIMD_INLINE Vec<Double, 64> div(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_div_pd(a, b);
}

// ---------------------------------------------------------------------------
// ceil, floor, round, truncate v
// ---------------------------------------------------------------------------

// 25. Mar 23 (Jonas Keller): added versions for integer types

// versions for integer types do nothing:

template <typename T>
static SIMD_INLINE Vec<T, 64> ceil(const Vec<T, 64> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 64> floor(const Vec<T, 64> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 64> round(const Vec<T, 64> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 64> truncate(const Vec<T, 64> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

// see Peter Cordes at https://stackoverflow.com/questions/50854991
// _mm512_roundscale_ps:
// imm[7:4] = fraction bits = here 0, imm[0:1] = rounding mode

static SIMD_INLINE Vec<Float, 64> ceil(const Vec<Float, 64> &a)
{
  return _mm512_roundscale_ps(a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> ceil(const Vec<Double, 64> &a)
{
  return _mm512_roundscale_pd(a, _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> floor(const Vec<Float, 64> &a)
{
  return _mm512_roundscale_ps(a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> floor(const Vec<Double, 64> &a)
{
  return _mm512_roundscale_pd(a, _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> round(const Vec<Float, 64> &a)
{
  return _mm512_roundscale_ps(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> round(const Vec<Double, 64> &a)
{
  return _mm512_roundscale_pd(a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> truncate(const Vec<Float, 64> &a)
{
  return _mm512_roundscale_ps(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> truncate(const Vec<Double, 64> &a)
{
  return _mm512_roundscale_pd(a, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// ---------------------------------------------------------------------------
// elementary mathematical functions v
// ---------------------------------------------------------------------------

// estimate of a reciprocal
// NOTE: this has better precision than SSE and AVX versions!

// float version
static SIMD_INLINE Vec<Float, 64> rcp(const Vec<Float, 64> &a)
{
  // 20. Mar 23 (Jonas Keller):
  // use _mm512_rcp28_ps if available, which has even better precision
  // and does not seem to be any slower (at least according to this:
  // https://github.com/tanakamura/instruction-bench/blob/master/knl.log)
#ifdef __AVX512ER__
  return _mm512_rcp28_ps(a);
#else
  return _mm512_rcp14_ps(a);
#endif
}

// double version
static SIMD_INLINE Vec<Double, 64> rcp(const Vec<Double, 64> &a)
{
  // use _mm512_rcp28_pd if available, which has even better precision
  // and does not seem to be any slower (at least according to this:
  // https://github.com/tanakamura/instruction-bench/blob/master/knl.log)
#ifdef __AVX512ER__
  return _mm512_rcp28_pd(a);
#else
  return _mm512_rcp14_pd(a);
#endif
}

// estimate of reverse square root
// NOTE: this has better precision than SSE and AVX versions!

// float version
static SIMD_INLINE Vec<Float, 64> rsqrt(const Vec<Float, 64> &a)
{
  // 20. Mar 23 (Jonas Keller):
  // use _mm512_rsqrt28_ps if available, which has even better precision
  // and does not seem to be any slower (probably)
#ifdef __AVX512ER__
  return _mm512_rsqrt28_ps(a);
#else
  return _mm512_rsqrt14_ps(a);
#endif
}

// double version
static SIMD_INLINE Vec<Double, 64> rsqrt(const Vec<Double, 64> &a)
{
  // use _mm512_rsqrt28_pd if available, which has even better precision
  // and does not seem to be any slower (probably)
#ifdef __AVX512ER__
  return _mm512_rsqrt28_pd(a);
#else
  return _mm512_rsqrt14_pd(a);
#endif
}

// square root

// float version
static SIMD_INLINE Vec<Float, 64> sqrt(const Vec<Float, 64> &a)
{
  return _mm512_sqrt_ps(a);
}

// double version
static SIMD_INLINE Vec<Double, 64> sqrt(const Vec<Double, 64> &a)
{
  return _mm512_sqrt_pd(a);
}

// ---------------------------------------------------------------------------
// abs v
// ---------------------------------------------------------------------------

// 25. Mar 25 (Jonas Keller): added abs for unsigned integers

// unsigned integers
template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> abs(const Vec<T, 64> &a)
{
  return a;
}

static SIMD_INLINE Vec<SignedByte, 64> abs(const Vec<SignedByte, 64> &a)
{
#ifdef __AVX512BW__
  return _mm512_abs_epi8(a);
#else
  // non-avx512bw workaround
  return Vec<SignedByte, 64>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Short, 64> abs(const Vec<Short, 64> &a)
{
#ifdef __AVX512BW__
  return _mm512_abs_epi16(a);
#else
  // non-avx512bw workaround
  return Vec<Short, 64>(abs(a.lo()), abs(a.hi()));
#endif
}

static SIMD_INLINE Vec<Int, 64> abs(const Vec<Int, 64> &a)
{
  return _mm512_abs_epi32(a);
}

static SIMD_INLINE Vec<Long, 64> abs(const Vec<Long, 64> &a)
{
  return _mm512_abs_epi64(a);
}

static SIMD_INLINE Vec<Float, 64> abs(const Vec<Float, 64> &a)
{
  return _mm512_abs_ps(a);
}

static SIMD_INLINE Vec<Double, 64> abs(const Vec<Double, 64> &a)
{
  return _mm512_abs_pd(a);
}

// ---------------------------------------------------------------------------
// unpacklo v (with permutex2var)
// ---------------------------------------------------------------------------

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<1>)
{
#ifdef __AVX512VBMI__
  // element order high to low for idx
  __m512i idx = _mm512_set_epi8(
    95, 31, 94, 30, 93, 29, 92, 28, 91, 27, 90, 26, 89, 25, 88, 24, 87, 23, 86,
    22, 85, 21, 84, 20, 83, 19, 82, 18, 81, 17, 80, 16, 79, 15, 78, 14, 77, 13,
    76, 12, 75, 11, 74, 10, 73, 9, 72, 8, 71, 7, 70, 6, 69, 5, 68, 4, 67, 3, 66,
    2, 65, 1, 64, 0);
  return _mm512_permutex2var_epi8(a, idx, b);
#else
  return x_mm512_unpacklo_epi8(x_mm512_transpose8x64_epi64(a),
                               x_mm512_transpose8x64_epi64(b));
#endif
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<2>)
{
#ifdef __AVX512BW__
  // element order high to low for idx
  __m512i idx =
    _mm512_set_epi16(47, 15, 46, 14, 45, 13, 44, 12, 43, 11, 42, 10, 41, 9, 40,
                     8, 39, 7, 38, 6, 37, 5, 36, 4, 35, 3, 34, 2, 33, 1, 32, 0);
  return _mm512_permutex2var_epi16(a, idx, b);
#else
  return x_mm512_unpacklo_epi16(x_mm512_transpose8x64_epi64(a),
                                x_mm512_transpose8x64_epi64(b));
#endif
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<4>)
{
  __m512i idx =
    _mm512_set_epi32(23, 7, 22, 6, 21, 5, 20, 4, 19, 3, 18, 2, 17, 1, 16, 0);
  return _mm512_permutex2var_epi32(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(11, 3, 10, 2, 9, 1, 8, 0);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 3, 2, 9, 8, 1, 0);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<0>, Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 9, 8, 3, 2, 1, 0);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<0>,
                                         Bytes<4>)
{
  __m512i idx =
    _mm512_set_epi32(23, 7, 22, 6, 21, 5, 20, 4, 19, 3, 18, 2, 17, 1, 16, 0);
  return _mm512_permutex2var_ps(a, idx, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<0>,
                                         Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(11, 3, 10, 2, 9, 1, 8, 0);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<0>,
                                         Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 3, 2, 9, 8, 1, 0);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<0>,
                                         Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 9, 8, 3, 2, 1, 0);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<0>,
                                          Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(11, 3, 10, 2, 9, 1, 8, 0);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<0>,
                                          Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 3, 2, 9, 8, 1, 0);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<0>,
                                          Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(11, 10, 9, 8, 3, 2, 1, 0);
  return _mm512_permutex2var_pd(a, idx, b);
}

// ---------------------------------------------------------------------------
// unpackhi v (with permutex2var)
// ---------------------------------------------------------------------------

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<1>)
{
#ifdef __AVX512VBMI__
  // element order high to low for idx
  __m512i idx = _mm512_set_epi8(
    127, 63, 126, 62, 125, 61, 124, 60, 123, 59, 122, 58, 121, 57, 120, 56, 119,
    55, 118, 54, 117, 53, 116, 52, 115, 51, 114, 50, 113, 49, 112, 48, 111, 47,
    110, 46, 109, 45, 108, 44, 107, 43, 106, 42, 105, 41, 104, 40, 103, 39, 102,
    38, 101, 37, 100, 36, 99, 35, 98, 34, 97, 33, 96, 32);
  return _mm512_permutex2var_epi8(a, idx, b);
#else
  return x_mm512_unpackhi_epi8(x_mm512_transpose8x64_epi64(a),
                               x_mm512_transpose8x64_epi64(b));
#endif
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<2>)
{
#ifdef __AVX512BW__
  // element order high to low for idx
  __m512i idx = _mm512_set_epi16(63, 31, 62, 30, 61, 29, 60, 28, 59, 27, 58, 26,
                                 57, 25, 56, 24, 55, 23, 54, 22, 53, 21, 52, 20,
                                 51, 19, 50, 18, 49, 17, 48, 16);
  return _mm512_permutex2var_epi16(a, idx, b);
#else
  return x_mm512_unpackhi_epi16(x_mm512_transpose8x64_epi64(a),
                                x_mm512_transpose8x64_epi64(b));
#endif
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<4>)
{
  __m512i idx = _mm512_set_epi32(31, 15, 30, 14, 29, 13, 28, 12, 27, 11, 26, 10,
                                 25, 9, 24, 8);
  return _mm512_permutex2var_epi32(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(15, 7, 14, 6, 13, 5, 12, 4);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 13, 12, 5, 4);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                     Part<1>, Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 13, 12, 7, 6, 5, 4);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<1>,
                                         Bytes<4>)
{
  __m512i idx = _mm512_set_epi32(31, 15, 30, 14, 29, 13, 28, 12, 27, 11, 26, 10,
                                 25, 9, 24, 8);
  return _mm512_permutex2var_ps(a, idx, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<1>,
                                         Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(15, 7, 14, 6, 13, 5, 12, 4);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<1>,
                                         Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 13, 12, 5, 4);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b, Part<1>,
                                         Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 13, 12, 7, 6, 5, 4);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<1>,
                                          Bytes<8>)
{
  __m512i idx = _mm512_set_epi64(15, 7, 14, 6, 13, 5, 12, 4);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<1>,
                                          Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 13, 12, 5, 4);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b, Part<1>,
                                          Bytes<32>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 13, 12, 7, 6, 5, 4);
  return _mm512_permutex2var_pd(a, idx, b);
}

// ---------------------------------------------------------------------------
// 128-bit-lane oriented unpacklo (with direct intrinsic calls)
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<1>)
{
  return x_mm512_unpacklo_epi8(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<2>)
{
  return x_mm512_unpacklo_epi16(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<4>)
{
  return _mm512_unpacklo_epi32(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<8>)
{
  return _mm512_unpacklo_epi64(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(13, 12, 5, 4, 9, 8, 1, 0);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<0>, Bytes<32>)
{
  return _mm512_shuffle_i32x4(a, b, _MM_SHUFFLE(1, 0, 1, 0));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<0>,
                                           Bytes<4>)
{
  return _mm512_unpacklo_ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<0>,
                                           Bytes<8>)
{
  return _mm512_castpd_ps(
    _mm512_unpacklo_pd(_mm512_castps_pd(a), _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<0>,
                                           Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(13, 12, 5, 4, 9, 8, 1, 0);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<0>,
                                           Bytes<32>)
{
  return _mm512_shuffle_f32x4(a, b, _MM_SHUFFLE(1, 0, 1, 0));
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<0>,
                                            Bytes<8>)
{
  return _mm512_unpacklo_pd(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<0>,
                                            Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(13, 12, 5, 4, 9, 8, 1, 0);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<0>,
                                            Bytes<32>)
{
  return _mm512_shuffle_f64x2(a, b, _MM_SHUFFLE(1, 0, 1, 0));
}

// ---------------------------------------------------------------------------
// 128-bit-lane oriented unpackhi v
// ---------------------------------------------------------------------------

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<1>)
{
  return x_mm512_unpackhi_epi8(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<2>)
{
  return x_mm512_unpackhi_epi16(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<4>)
{
  return _mm512_unpackhi_epi32(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<8>)
{
  return _mm512_unpackhi_epi64(a, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 11, 10, 3, 2);
  return _mm512_permutex2var_epi64(a, idx, b);
}

// integer version
template <typename T>
static SIMD_INLINE Vec<T, 64> unpack16(const Vec<T, 64> &a, const Vec<T, 64> &b,
                                       Part<1>, Bytes<32>)
{
  return _mm512_shuffle_i32x4(a, b, _MM_SHUFFLE(3, 2, 3, 2));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<1>,
                                           Bytes<4>)
{
  return _mm512_unpackhi_ps(a, b);
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<1>,
                                           Bytes<8>)
{
  return _mm512_castpd_ps(
    _mm512_unpackhi_pd(_mm512_castps_pd(a), _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<1>,
                                           Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 11, 10, 3, 2);
  return _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idx, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE Vec<Float, 64> unpack16(const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b, Part<1>,
                                           Bytes<32>)
{
  return _mm512_shuffle_f32x4(a, b, _MM_SHUFFLE(3, 2, 3, 2));
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<1>,
                                            Bytes<8>)
{
  return _mm512_unpackhi_pd(a, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<1>,
                                            Bytes<16>)
{
  __m512i idx = _mm512_set_epi64(15, 14, 7, 6, 11, 10, 3, 2);
  return _mm512_permutex2var_pd(a, idx, b);
}

// double version
static SIMD_INLINE Vec<Double, 64> unpack16(const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b, Part<1>,
                                            Bytes<32>)
{
  return _mm512_shuffle_f64x2(a, b, _MM_SHUFFLE(3, 2, 3, 2));
}

// ---------------------------------------------------------------------------
// zip v
// ---------------------------------------------------------------------------

// 25. Aug 23 (Jonas Keller): Simplified zip implementation by using a single
// template function instead of multiple specializations.

// a, b are passed by-value to avoid problems with identical in/out args.

// zips blocks of NUM_ELEMS elements of type T
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip(const Vec<T, 64> a, const Vec<T, 64> b,
                            Vec<T, 64> &l, Vec<T, 64> &h)
{
  l = unpack(a, b, Part<0>(), Bytes<NUM_ELEMS * sizeof(T)>());
  h = unpack(a, b, Part<1>(), Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// zip16 (16-byte-lane oriented zip)
// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// zips blocks of NUM_ELEMS elements of type T
template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip16(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h)
{
  l = unpack16(a, b, Part<0>(), Bytes<NUM_ELEMS * sizeof(T)>());
  h = unpack16(a, b, Part<1>(), Bytes<NUM_ELEMS * sizeof(T)>());
}

// ---------------------------------------------------------------------------
// unzip v
// ---------------------------------------------------------------------------

// a, b are passed by-value to avoid problems with identical
// input/output args.

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<1>)
{
#ifdef __AVX512VBMI__
  const __m512i idxL = _mm512_set_epi8(
    126, 124, 122, 120, 118, 116, 114, 112, 110, 108, 106, 104, 102, 100, 98,
    96, 94, 92, 90, 88, 86, 84, 82, 80, 78, 76, 74, 72, 70, 68, 66, 64, 62, 60,
    58, 56, 54, 52, 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22,
    20, 18, 16, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH = _mm512_set_epi8(
    127, 125, 123, 121, 119, 117, 115, 113, 111, 109, 107, 105, 103, 101, 99,
    97, 95, 93, 91, 89, 87, 85, 83, 81, 79, 77, 75, 73, 71, 69, 67, 65, 63, 61,
    59, 57, 55, 53, 51, 49, 47, 45, 43, 41, 39, 37, 35, 33, 31, 29, 27, 25, 23,
    21, 19, 17, 15, 13, 11, 9, 7, 5, 3, 1);
  l = _mm512_permutex2var_epi8(a, idxL, b);
  h = _mm512_permutex2var_epi8(a, idxH, b);
#else
  const __m512i mask = _mm512_set_epi8(
    15, 13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8, 6, 4, 2, 0, 15, 13, 11, 9, 7, 5,
    3, 1, 14, 12, 10, 8, 6, 4, 2, 0, 15, 13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8,
    6, 4, 2, 0, 15, 13, 11, 9, 7, 5, 3, 1, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i atmp = x_mm512_shuffle_epi8(a, mask);
  const __m512i btmp = x_mm512_shuffle_epi8(b, mask);
  l                  = _mm512_permutex2var_epi64(
    atmp, _mm512_set_epi64(14, 12, 10, 8, 6, 4, 2, 0), btmp);
  h = _mm512_permutex2var_epi64(
    atmp, _mm512_set_epi64(15, 13, 11, 9, 7, 5, 3, 1), btmp);
#endif
}

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<2>)
{
#ifdef __AVX512BW__
  const __m512i idxL = _mm512_set_epi16(
    62, 60, 58, 56, 54, 52, 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26,
    24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH = _mm512_set_epi16(
    63, 61, 59, 57, 55, 53, 51, 49, 47, 45, 43, 41, 39, 37, 35, 33, 31, 29, 27,
    25, 23, 21, 19, 17, 15, 13, 11, 9, 7, 5, 3, 1);
  l = _mm512_permutex2var_epi16(a, idxL, b);
  h = _mm512_permutex2var_epi16(a, idxH, b);
#else
  const __m512i mask = _mm512_set_epi8(
    15, 14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5, 4, 1, 0, 15, 14, 11, 10, 7, 6,
    3, 2, 13, 12, 9, 8, 5, 4, 1, 0, 15, 14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5,
    4, 1, 0, 15, 14, 11, 10, 7, 6, 3, 2, 13, 12, 9, 8, 5, 4, 1, 0);
  const __m512i atmp = x_mm512_shuffle_epi8(a, mask);
  const __m512i btmp = x_mm512_shuffle_epi8(b, mask);
  l                  = _mm512_permutex2var_epi64(
    atmp, _mm512_set_epi64(14, 12, 10, 8, 6, 4, 2, 0), btmp);
  h = _mm512_permutex2var_epi64(
    atmp, _mm512_set_epi64(15, 13, 11, 9, 7, 5, 3, 1), btmp);
#endif
}

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<4>)
{
  const __m512i idxL =
    _mm512_set_epi32(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH =
    _mm512_set_epi32(31, 29, 27, 25, 23, 21, 19, 17, 15, 13, 11, 9, 7, 5, 3, 1);
  l = _mm512_permutex2var_epi32(a, idxL, b);
  h = _mm512_permutex2var_epi32(a, idxH, b);
}

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<8>)
{
  const __m512i idxL = _mm512_set_epi64(14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH = _mm512_set_epi64(15, 13, 11, 9, 7, 5, 3, 1);
  l                  = _mm512_permutex2var_epi64(a, idxL, b);
  h                  = _mm512_permutex2var_epi64(a, idxH, b);
}

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<16>)
{
  const __m512i idxL = _mm512_set_epi64(13, 12, 9, 8, 5, 4, 1, 0);
  const __m512i idxH = _mm512_set_epi64(15, 14, 11, 10, 7, 6, 3, 2);
  l                  = _mm512_permutex2var_epi64(a, idxL, b);
  h                  = _mm512_permutex2var_epi64(a, idxH, b);
}

// integer version
template <typename T>
static SIMD_INLINE void unzip(const Vec<T, 64> a, const Vec<T, 64> b,
                              Vec<T, 64> &l, Vec<T, 64> &h, Bytes<32>)
{
  l = unpack(a, b, Part<0>(), Bytes<32>());
  h = unpack(a, b, Part<1>(), Bytes<32>());
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 64> a, const Vec<Float, 64> b,
                              Vec<Float, 64> &l, Vec<Float, 64> &h, Bytes<4>)
{
  const __m512i idxL =
    _mm512_set_epi32(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH =
    _mm512_set_epi32(31, 29, 27, 25, 23, 21, 19, 17, 15, 13, 11, 9, 7, 5, 3, 1);
  l = _mm512_permutex2var_ps(a, idxL, b);
  h = _mm512_permutex2var_ps(a, idxH, b);
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 64> a, const Vec<Float, 64> b,
                              Vec<Float, 64> &l, Vec<Float, 64> &h, Bytes<8>)
{
  const __m512i idxL = _mm512_set_epi64(14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH = _mm512_set_epi64(15, 13, 11, 9, 7, 5, 3, 1);
  l                  = _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idxL, _mm512_castps_pd(b)));
  h = _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idxH, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 64> a, const Vec<Float, 64> b,
                              Vec<Float, 64> &l, Vec<Float, 64> &h, Bytes<16>)
{
  const __m512i idxL = _mm512_set_epi64(13, 12, 9, 8, 5, 4, 1, 0);
  const __m512i idxH = _mm512_set_epi64(15, 14, 11, 10, 7, 6, 3, 2);
  l                  = _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idxL, _mm512_castps_pd(b)));
  h = _mm512_castpd_ps(
    _mm512_permutex2var_pd(_mm512_castps_pd(a), idxH, _mm512_castps_pd(b)));
}

// float version
static SIMD_INLINE void unzip(const Vec<Float, 64> a, const Vec<Float, 64> b,
                              Vec<Float, 64> &l, Vec<Float, 64> &h, Bytes<32>)
{
  l = unpack(a, b, Part<0>(), Bytes<32>());
  h = unpack(a, b, Part<1>(), Bytes<32>());
}

// double version
static SIMD_INLINE void unzip(const Vec<Double, 64> a, const Vec<Double, 64> b,
                              Vec<Double, 64> &l, Vec<Double, 64> &h, Bytes<8>)
{
  const __m512i idxL = _mm512_set_epi64(14, 12, 10, 8, 6, 4, 2, 0);
  const __m512i idxH = _mm512_set_epi64(15, 13, 11, 9, 7, 5, 3, 1);
  l                  = _mm512_permutex2var_pd(a, idxL, b);
  h                  = _mm512_permutex2var_pd(a, idxH, b);
}

// double version
static SIMD_INLINE void unzip(const Vec<Double, 64> a, const Vec<Double, 64> b,
                              Vec<Double, 64> &l, Vec<Double, 64> &h, Bytes<16>)
{
  const __m512i idxL = _mm512_set_epi64(13, 12, 9, 8, 5, 4, 1, 0);
  const __m512i idxH = _mm512_set_epi64(15, 14, 11, 10, 7, 6, 3, 2);
  l                  = _mm512_permutex2var_pd(a, idxL, b);
  h                  = _mm512_permutex2var_pd(a, idxH, b);
}

// ---------------------------------------------------------------------------
// packs v
// ---------------------------------------------------------------------------

// ========== signed -> signed ==========

static SIMD_INLINE Vec<SignedByte, 64> packs(const Vec<Short, 64> &a,
                                             const Vec<Short, 64> &b,
                                             OutputType<SignedByte>)
{
  return x_mm512_evenodd8x64_epi64(x_mm512_packs_epi16(a, b));
}

static SIMD_INLINE Vec<Short, 64> packs(const Vec<Int, 64> &a,
                                        const Vec<Int, 64> &b,
                                        OutputType<Short>)
{
  return x_mm512_evenodd8x64_epi64(x_mm512_packs_epi32(a, b));
}

static SIMD_INLINE Vec<Short, 64> packs(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b,
                                        OutputType<Short>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Short>());
}

static SIMD_INLINE Vec<Int, 64> packs(const Vec<Long, 64> &a,
                                      const Vec<Long, 64> &b, OutputType<Int>)
{
  return _mm512_inserti64x4(_mm512_castsi256_si512(_mm512_cvtsepi64_epi32(a)),
                            _mm512_cvtsepi64_epi32(b), 1);
}

static SIMD_INLINE Vec<Float, 64> packs(const Vec<Long, 64> &a,
                                        const Vec<Long, 64> &b,
                                        OutputType<Float>)
{
#ifdef __AVX512DQ__
  const __m256d low  = _mm256_castps_pd(_mm512_cvtepi64_ps(a));
  const __m256d high = _mm256_castps_pd(_mm512_cvtepi64_ps(b));
#else
  const __m256d low =
    _mm256_castps_pd(_mm512_cvtpd_ps(cvts(a, OutputType<Double>())));
  const __m256d high =
    _mm256_castps_pd(_mm512_cvtpd_ps(cvts(b, OutputType<Double>())));
#endif
  return _mm512_castpd_ps(
    _mm512_insertf64x4(_mm512_castpd256_pd512(low), high, 1));
}

static SIMD_INLINE Vec<Float, 64> packs(const Vec<Double, 64> &a,
                                        const Vec<Double, 64> &b,
                                        OutputType<Float>)
{
  const __m256d low  = _mm256_castps_pd(_mm512_cvtpd_ps(a));
  const __m256d high = _mm256_castps_pd(_mm512_cvtpd_ps(b));
  return _mm512_castpd_ps(
    _mm512_insertf64x4(_mm512_castpd256_pd512(low), high, 1));
}

static SIMD_INLINE Vec<Int, 64> packs(const Vec<Double, 64> &a,
                                      const Vec<Double, 64> &b, OutputType<Int>)
{
  const __m512d clip = _mm512_set1_pd(std::numeric_limits<Int>::max());
  const __m256i low  = _mm512_cvtpd_epi32(_mm512_min_pd(clip, a));
  const __m256i high = _mm512_cvtpd_epi32(_mm512_min_pd(clip, b));
  return _mm512_inserti64x4(_mm512_castsi256_si512(low), high, 1);
}

// ========== unsigned -> unsigned ==========

// non-avx512bw workaround
static SIMD_INLINE Vec<Byte, 64> packs(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b, OutputType<Byte>)
{
  const auto aSaturated = min(a, Vec<Word, 64>(_mm512_set1_epi16(0xff)));
  const auto bSaturated = min(b, Vec<Word, 64>(_mm512_set1_epi16(0xff)));
  return x_mm512_evenodd8x64_epi64(
    x_mm512_packus_epi16(aSaturated, bSaturated));
}

// ========== signed -> unsigned ==========

// non-avx512bw workaround
static SIMD_INLINE Vec<Byte, 64> packs(const Vec<Short, 64> &a,
                                       const Vec<Short, 64> &b,
                                       OutputType<Byte>)
{
  return x_mm512_evenodd8x64_epi64(x_mm512_packus_epi16(a, b));
}

// non-avx512bw workaround
static SIMD_INLINE Vec<Word, 64> packs(const Vec<Int, 64> &a,
                                       const Vec<Int, 64> &b, OutputType<Word>)
{
  return x_mm512_evenodd8x64_epi64(x_mm512_packus_epi32(a, b));
}

static SIMD_INLINE Vec<Word, 64> packs(const Vec<Float, 64> &a,
                                       const Vec<Float, 64> &b,
                                       OutputType<Word>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Word>());
}

// ========== unsigned -> signed ==========

// non-avx512bw workaround
static SIMD_INLINE Vec<SignedByte, 64> packs(const Vec<Word, 64> &a,
                                             const Vec<Word, 64> &b,
                                             OutputType<SignedByte>)
{
  return x_mm512_evenodd8x64_epi64(
    x_mm512_packs_epi16(min(a, Vec<Word, 64>(_mm512_set1_epi16(0x7f))),
                        min(b, Vec<Word, 64>(_mm512_set1_epi16(0x7f)))));
}

// ---------------------------------------------------------------------------
// generalized extend: no stage v
// ---------------------------------------------------------------------------

// combinations:
// - signed   -> extended signed (sign extension)
// - unsigned -> extended unsigned (zero extension)
// - unsigned -> extended signed (zero extension)
// - signed   -> extended unsigned (saturation and zero extension)

// same types
template <typename T>
static SIMD_INLINE void extend(const Vec<T, 64> &vIn, Vec<T, 64> vOut[1])
{
  vOut[0] = vIn;
}

// same size, different types

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Byte, 64> vOut[1])
{
  vOut[0] = max(vIn, Vec<SignedByte, 64>(_mm512_setzero_si512()));
}

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn,
                               Vec<SignedByte, 64> vOut[1])
{
  vOut[0] = min(vIn, Vec<Byte, 64>(_mm512_set1_epi8(0x7f)));
}

static SIMD_INLINE void extend(const Vec<Short, 64> &vIn, Vec<Word, 64> vOut[1])
{
  vOut[0] = max(vIn, Vec<Short, 64>(_mm512_setzero_si512()));
}

static SIMD_INLINE void extend(const Vec<Word, 64> &vIn, Vec<Short, 64> vOut[1])
{
  vOut[0] = min(vIn, Vec<Word, 64>(_mm512_set1_epi16(0x7fff)));
}

// ---------------------------------------------------------------------------
// generalized extend: single stage v
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Short, 64> vOut[2])
{
#ifdef __AVX512BW__
  vOut[0] = _mm512_cvtepi8_epi16(vIn.lo());
  vOut[1] = _mm512_cvtepi8_epi16(vIn.hi());
#else
  {
    const __m256i lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vIn.lo()));
    const __m256i hi =
      _mm256_cvtepi8_epi16(_mm256_extractf128_si256(vIn.lo(), 1));
    vOut[0] = _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
  }
  {
    const __m256i lo = _mm256_cvtepi8_epi16(_mm256_castsi256_si128(vIn.hi()));
    const __m256i hi =
      _mm256_cvtepi8_epi16(_mm256_extractf128_si256(vIn.hi(), 1));
    vOut[1] = _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
  }
#endif
}

static SIMD_INLINE void extend(const Vec<Short, 64> &vIn, Vec<Int, 64> vOut[2])
{
  vOut[0] = _mm512_cvtepi16_epi32(vIn.lo());
  vOut[1] = _mm512_cvtepi16_epi32(vIn.hi());
}

static SIMD_INLINE void extend(const Vec<Short, 64> &vIn,
                               Vec<Float, 64> vOut[2])
{
  vOut[0] = _mm512_cvtepi32_ps(_mm512_cvtepi16_epi32(vIn.lo()));
  vOut[1] = _mm512_cvtepi32_ps(_mm512_cvtepi16_epi32(vIn.hi()));
}

static SIMD_INLINE void extend(const Vec<Int, 64> &vIn, Vec<Long, 64> vecOut[2])
{
  vecOut[0] = _mm512_cvtepi32_epi64(vIn.lo());
  vecOut[1] = _mm512_cvtepi32_epi64(vIn.hi());
}

static SIMD_INLINE void extend(const Vec<Int, 64> &vIn,
                               Vec<Double, 64> vecOut[2])
{
  vecOut[0] = _mm512_cvtepi32_pd(vIn.lo());
  vecOut[1] = _mm512_cvtepi32_pd(vIn.hi());
}

static SIMD_INLINE void extend(const Vec<Float, 64> &vIn,
                               Vec<Long, 64> vecOut[2])
{
  const Vec<Float, 64> clipped =
    _mm512_min_ps(_mm512_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT64), vIn);
#ifdef __AVX512DQ__
  vecOut[0] = _mm512_cvtps_epi64(clipped.lo());
  vecOut[1] = _mm512_cvtps_epi64(clipped.hi());
#else
  vecOut[0] = cvts(_mm512_cvtps_pd(clipped.lo()), OutputType<Long>());
  vecOut[1] = cvts(_mm512_cvtps_pd(clipped.hi()), OutputType<Long>());
#endif
}

static SIMD_INLINE void extend(const Vec<Float, 64> &vIn,
                               Vec<Double, 64> vecOut[2])
{
  vecOut[0] = _mm512_cvtps_pd(vIn.lo());
  vecOut[1] = _mm512_cvtps_pd(vIn.hi());
}

// unsigned -> unsigned

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn, Vec<Word, 64> vOut[2])
{
  // there's no _mm512_cvtepu8_epu16()
  vOut[0] = unpack(vIn, setzero(OutputType<Byte>(), Integer<64>()), Part<0>(),
                   Bytes<1>());
  vOut[1] = unpack(vIn, setzero(OutputType<Byte>(), Integer<64>()), Part<1>(),
                   Bytes<1>());
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn, Vec<Short, 64> vOut[2])
{
#ifdef __AVX512BW__
  vOut[0] = _mm512_cvtepu8_epi16(vIn.lo());
  vOut[1] = _mm512_cvtepu8_epi16(vIn.hi());
#else
  {
    const __m256i lo = _mm256_cvtepu8_epi16(_mm256_castsi256_si128(vIn.lo()));
    const __m256i hi =
      _mm256_cvtepu8_epi16(_mm256_extractf128_si256(vIn.lo(), 1));
    vOut[0] = _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
  }
  {
    const __m256i lo = _mm256_cvtepu8_epi16(_mm256_castsi256_si128(vIn.hi()));
    const __m256i hi =
      _mm256_cvtepu8_epi16(_mm256_extractf128_si256(vIn.hi(), 1));
    vOut[1] = _mm512_inserti64x4(_mm512_castsi256_si512(lo), hi, 1);
  }
#endif
}

static SIMD_INLINE void extend(const Vec<Word, 64> &vIn, Vec<Int, 64> vOut[2])
{
  vOut[0] = _mm512_cvtepu16_epi32(vIn.lo());
  vOut[1] = _mm512_cvtepu16_epi32(vIn.hi());
}

static SIMD_INLINE void extend(const Vec<Word, 64> &vIn, Vec<Float, 64> vOut[2])
{
  vOut[0] = _mm512_cvtepi32_ps(_mm512_cvtepu16_epi32(vIn.lo()));
  vOut[1] = _mm512_cvtepi32_ps(_mm512_cvtepu16_epi32(vIn.hi()));
}

// signed -> unsigned

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Word, 64> vOut[2])
{
  const Vec<SignedByte, 64> saturated =
    max(vIn, Vec<SignedByte, 64>(_mm512_setzero_si512()));
  vOut[0] = unpack(saturated, setzero(OutputType<SignedByte>(), Integer<64>()),
                   Part<0>(), Bytes<1>());
  vOut[1] = unpack(saturated, setzero(OutputType<SignedByte>(), Integer<64>()),
                   Part<1>(), Bytes<1>());
} // namespace base

// ---------------------------------------------------------------------------
// generalized extend: two stages v
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Int, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepi8_epi32(_mm256_castsi256_si128(vIn.lo()));
  vOut[1] = _mm512_cvtepi8_epi32(_mm256_extractf128_si256(vIn.lo(), 1));
  vOut[2] = _mm512_cvtepi8_epi32(_mm256_castsi256_si128(vIn.hi()));
  vOut[3] = _mm512_cvtepi8_epi32(_mm256_extractf128_si256(vIn.hi(), 1));
}

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Float, 64> vOut[4])
{
  Vec<Int, 64> vTmp[4];
  extend(vIn, vTmp);
  for (size_t i = 0; i < 4; i++) vOut[i] = cvts(vTmp[i], OutputType<Float>());
}

static SIMD_INLINE void extend(const Vec<Short, 64> &vIn, Vec<Long, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepi16_epi64(_mm512_extracti32x4_epi32(vIn, 0));
  vOut[1] = _mm512_cvtepi16_epi64(_mm512_extracti32x4_epi32(vIn, 1));
  vOut[2] = _mm512_cvtepi16_epi64(_mm512_extracti32x4_epi32(vIn, 2));
  vOut[3] = _mm512_cvtepi16_epi64(_mm512_extracti32x4_epi32(vIn, 3));
}

static SIMD_INLINE void extend(const Vec<Short, 64> &vIn,
                               Vec<Double, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepi32_pd(
    _mm256_cvtepi16_epi32(_mm512_extracti32x4_epi32(vIn, 0)));
  vOut[1] = _mm512_cvtepi32_pd(
    _mm256_cvtepi16_epi32(_mm512_extracti32x4_epi32(vIn, 1)));
  vOut[2] = _mm512_cvtepi32_pd(
    _mm256_cvtepi16_epi32(_mm512_extracti32x4_epi32(vIn, 2)));
  vOut[3] = _mm512_cvtepi32_pd(
    _mm256_cvtepi16_epi32(_mm512_extracti32x4_epi32(vIn, 3)));
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn, Vec<Int, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(vIn.lo()));
  vOut[1] = _mm512_cvtepu8_epi32(_mm256_extractf128_si256(vIn.lo(), 1));
  vOut[2] = _mm512_cvtepu8_epi32(_mm256_castsi256_si128(vIn.hi()));
  vOut[3] = _mm512_cvtepu8_epi32(_mm256_extractf128_si256(vIn.hi(), 1));
}

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn, Vec<Float, 64> vOut[4])
{
  Vec<Int, 64> vTmp[4];
  extend(vIn, vTmp);
  for (size_t i = 0; i < 4; i++) vOut[i] = cvts(vTmp[i], OutputType<Float>());
}

static SIMD_INLINE void extend(const Vec<Word, 64> &vIn, Vec<Long, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepu16_epi64(_mm512_extracti32x4_epi32(vIn, 0));
  vOut[1] = _mm512_cvtepu16_epi64(_mm512_extracti32x4_epi32(vIn, 1));
  vOut[2] = _mm512_cvtepu16_epi64(_mm512_extracti32x4_epi32(vIn, 2));
  vOut[3] = _mm512_cvtepu16_epi64(_mm512_extracti32x4_epi32(vIn, 3));
}

static SIMD_INLINE void extend(const Vec<Word, 64> &vIn,
                               Vec<Double, 64> vOut[4])
{
  vOut[0] = _mm512_cvtepi32_pd(
    _mm256_cvtepu16_epi32(_mm512_extracti32x4_epi32(vIn, 0)));
  vOut[1] = _mm512_cvtepi32_pd(
    _mm256_cvtepu16_epi32(_mm512_extracti32x4_epi32(vIn, 1)));
  vOut[2] = _mm512_cvtepi32_pd(
    _mm256_cvtepu16_epi32(_mm512_extracti32x4_epi32(vIn, 2)));
  vOut[3] = _mm512_cvtepi32_pd(
    _mm256_cvtepu16_epi32(_mm512_extracti32x4_epi32(vIn, 3)));
}

// ---------------------------------------------------------------------------
// generalized extend: three stages
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Long, 64> vOut[8])
{
  vOut[0] = _mm512_cvtepi8_epi64(_mm512_castsi512_si128(vIn));
  vOut[1] =
    _mm512_cvtepi8_epi64(_mm_srli_si128(_mm512_castsi512_si128(vIn), 8));
  vOut[2] = _mm512_cvtepi8_epi64(_mm512_extracti32x4_epi32(vIn, 1));
  vOut[3] =
    _mm512_cvtepi8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 1), 8));
  vOut[4] = _mm512_cvtepi8_epi64(_mm512_extracti32x4_epi32(vIn, 2));
  vOut[5] =
    _mm512_cvtepi8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 2), 8));
  vOut[6] = _mm512_cvtepi8_epi64(_mm512_extracti32x4_epi32(vIn, 3));
  vOut[7] =
    _mm512_cvtepi8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 3), 8));
}

static SIMD_INLINE void extend(const Vec<SignedByte, 64> &vIn,
                               Vec<Double, 64> vOut[8])
{
  const __m128i vIn128[4] = {
    _mm512_extracti32x4_epi32(vIn, 0),
    _mm512_extracti32x4_epi32(vIn, 1),
    _mm512_extracti32x4_epi32(vIn, 2),
    _mm512_extracti32x4_epi32(vIn, 3),
  };

  for (size_t i = 0; i < 4; i++) {
    vOut[i * 2 + 0] = _mm512_cvtepi32_pd(_mm256_cvtepi8_epi32(vIn128[i]));
    vOut[i * 2 + 1] =
      _mm512_cvtepi32_pd(_mm256_cvtepi8_epi32(_mm_srli_si128(vIn128[i], 8)));
  }
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn, Vec<Long, 64> vOut[8])
{
  vOut[0] = _mm512_cvtepu8_epi64(_mm512_castsi512_si128(vIn));
  vOut[1] =
    _mm512_cvtepu8_epi64(_mm_srli_si128(_mm512_castsi512_si128(vIn), 8));
  vOut[2] = _mm512_cvtepu8_epi64(_mm512_extracti32x4_epi32(vIn, 1));
  vOut[3] =
    _mm512_cvtepu8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 1), 8));
  vOut[4] = _mm512_cvtepu8_epi64(_mm512_extracti32x4_epi32(vIn, 2));
  vOut[5] =
    _mm512_cvtepu8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 2), 8));
  vOut[6] = _mm512_cvtepu8_epi64(_mm512_extracti32x4_epi32(vIn, 3));
  vOut[7] =
    _mm512_cvtepu8_epi64(_mm_srli_si128(_mm512_extracti32x4_epi32(vIn, 3), 8));
}

static SIMD_INLINE void extend(const Vec<Byte, 64> &vIn,
                               Vec<Double, 64> vOut[8])
{
  const __m128i vIn128[4] = {
    _mm512_extracti32x4_epi32(vIn, 0),
    _mm512_extracti32x4_epi32(vIn, 1),
    _mm512_extracti32x4_epi32(vIn, 2),
    _mm512_extracti32x4_epi32(vIn, 3),
  };

  for (size_t i = 0; i < 4; i++) {
    vOut[i * 2 + 0] = _mm512_cvtepi32_pd(_mm256_cvtepu8_epi32(vIn128[i]));
    vOut[i * 2 + 1] =
      _mm512_cvtepi32_pd(_mm256_cvtepu8_epi32(_mm_srli_si128(vIn128[i], 8)));
  }
}

// ---------------------------------------------------------------------------
// generalized extend: special case int <-> float, long <-> double
// ---------------------------------------------------------------------------

template <typename Tout, typename Tin,
          SIMD_ENABLE_IF(sizeof(Tin) == sizeof(Tout)),
          SIMD_ENABLE_IF(std::is_floating_point<Tin>::value !=
                         std::is_floating_point<Tout>::value)>
static SIMD_INLINE void extend(const Vec<Tin, 64> &vIn, Vec<Tout, 64> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Tout>());
}

// ---------------------------------------------------------------------------
// srai v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
// 16. Oct 22 (Jonas Keller): added missing Byte and SignedByte versions

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 64> srai(const Vec<Byte, 64> &a)
{
  const __m512i odd = _mm512_srai_epi16(a, vec::min(COUNT, 7ul));
  const __m512i even =
    _mm512_srai_epi16(_mm512_slli_epi16(a, 8), vec::min(COUNT, 7ul) + 8);
  const __mmask64 mask = __mmask64(0x5555555555555555);
  return _mm512_mask_blend_epi8(mask, odd, even);
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 64> srai(const Vec<SignedByte, 64> &a)
{
  const __m512i odd = _mm512_srai_epi16(a, vec::min(COUNT, 7ul));
  const __m512i even =
    _mm512_srai_epi16(_mm512_slli_epi16(a, 8), vec::min(COUNT, 7ul) + 8);
  const __mmask64 mask = __mmask64(0x5555555555555555);
  return _mm512_mask_blend_epi8(mask, odd, even);
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> srai(const Vec<Word, 64> &a)
{
  return _mm512_srai_epi16(a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> srai(const Vec<Short, 64> &a)
{
  return _mm512_srai_epi16(a, vec::min(COUNT, 15ul));
}

#else

// non-avx512bw workaround
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 64> srai(const Vec<T, 64> &a)
{
  return Vec<T, 64>(srai<COUNT>(a.lo()), srai<COUNT>(a.hi()));
}

#endif

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> srai(const Vec<Int, 64> &a)
{
  return _mm512_srai_epi32(a, vec::min(COUNT, 31ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> srai(const Vec<Long, 64> &a)
{
  return _mm512_srai_epi64(a, vec::min(COUNT, 63ul));
}

// ---------------------------------------------------------------------------
// srli v
// ---------------------------------------------------------------------------

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 64> srli(const Vec<Byte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm512_and_si512(_mm512_set1_epi8((int8_t) (0xff >> COUNT)),
                            _mm512_srli_epi32(a, COUNT));
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 64> srli(const Vec<SignedByte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm512_and_si512(_mm512_set1_epi8((int8_t) (0xff >> COUNT)),
                            _mm512_srli_epi32(a, COUNT));
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> srli(const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
#ifdef __AVX512BW__
    return _mm512_srli_epi16(a, COUNT);
#else
    return _mm512_and_si512(_mm512_set1_epi16((int16_t) (0xffff >> COUNT)),
                            _mm512_srli_epi32(a, COUNT));
#endif
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> srli(const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
#ifdef __AVX512BW__
    return _mm512_srli_epi16(a, COUNT);
#else
    return _mm512_and_si512(_mm512_set1_epi16((int16_t) (0xffff >> COUNT)),
                            _mm512_srli_epi32(a, COUNT));
#endif
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> srli(const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_srli_epi32(a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> srli(const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_srli_epi64(a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

// ---------------------------------------------------------------------------
// slli v
// ---------------------------------------------------------------------------

template <size_t COUNT>
static SIMD_INLINE Vec<Byte, 64> slli(const Vec<Byte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm512_and_si512(
      _mm512_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm512_slli_epi32(a, COUNT));
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<SignedByte, 64> slli(const Vec<SignedByte, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 8) {
    // https://github.com/grumpos/spu_intrin/blob/master/src/sse_extensions.h
    // License: not specified
    return _mm512_and_si512(
      _mm512_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << COUNT))),
      _mm512_slli_epi32(a, COUNT));
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> slli(const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
#ifdef __AVX512BW__
    return _mm512_slli_epi16(a, COUNT);
#else
    return _mm512_and_si512(
      _mm512_set1_epi16((int16_t) (uint16_t) (0xffff & (0xffff << COUNT))),
      _mm512_slli_epi32(a, COUNT));
#endif
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> slli(const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
#ifdef __AVX512BW__
    return _mm512_slli_epi16(a, COUNT);
#else
    return _mm512_and_si512(
      _mm512_set1_epi16((int16_t) (uint16_t) (0xffff & (0xffff << COUNT))),
      _mm512_slli_epi32(a, COUNT));
#endif
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> slli(const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_slli_epi32(a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> slli(const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_slli_epi64(a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

// 19. Dec 22 (Jonas Keller): added sra, srl and sll functions

// ---------------------------------------------------------------------------
// sra
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> sra(const Vec<Byte, 64> &a,
                                     const uint8_t count)
{
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm512_movm_epi8(_mm512_cmplt_epi8_mask(a, _mm512_setzero_si512()));
  }
  __m512i odd = _mm512_sra_epi16(a, _mm_cvtsi32_si128(count));
  __m512i even =
    _mm512_sra_epi16(_mm512_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  __mmask64 mask = __mmask64(0x5555555555555555);
  return _mm512_mask_blend_epi8(mask, odd, even);
}

static SIMD_INLINE Vec<SignedByte, 64> sra(const Vec<SignedByte, 64> &a,
                                           const uint8_t count)
{
  if (count >= 8) {
    // result should be all ones if a is negative, all zeros otherwise
    return _mm512_movm_epi8(_mm512_cmplt_epi8_mask(a, _mm512_setzero_si512()));
  }
  __m512i odd = _mm512_sra_epi16(a, _mm_cvtsi32_si128(count));
  __m512i even =
    _mm512_sra_epi16(_mm512_slli_epi16(a, 8), _mm_cvtsi32_si128(count + 8));
  __mmask64 mask = __mmask64(0x5555555555555555);
  return _mm512_mask_blend_epi8(mask, odd, even);
}

static SIMD_INLINE Vec<Word, 64> sra(const Vec<Word, 64> &a,
                                     const uint8_t count)
{
  return _mm512_sra_epi16(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Short, 64> sra(const Vec<Short, 64> &a,
                                      const uint8_t count)
{
  return _mm512_sra_epi16(a, _mm_cvtsi32_si128(count));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> sra(const Vec<T, 64> &a, const uint8_t count)
{
  return Vec<T, 64>(sra(a.lo(), count), sra(a.hi(), count));
}

#endif

static SIMD_INLINE Vec<Int, 64> sra(const Vec<Int, 64> &a, const uint8_t count)
{
  return _mm512_sra_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 64> sra(const Vec<Long, 64> &a,
                                     const uint8_t count)
{
  return _mm512_sra_epi64(a, _mm_cvtsi32_si128(count));
}

// ---------------------------------------------------------------------------
// srl
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 64> srl(const Vec<Byte, 64> &a,
                                     const uint8_t count)
{
  return _mm512_and_si512(_mm512_srl_epi32(a, _mm_cvtsi32_si128(count)),
                          _mm512_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<SignedByte, 64> srl(const Vec<SignedByte, 64> &a,
                                           const uint8_t count)
{
  return _mm512_and_si512(_mm512_srl_epi32(a, _mm_cvtsi32_si128(count)),
                          _mm512_set1_epi8((int8_t) (uint8_t) (0xff >> count)));
}

static SIMD_INLINE Vec<Word, 64> srl(const Vec<Word, 64> &a,
                                     const uint8_t count)
{
#ifdef __AVX512BW__
  return _mm512_srl_epi16(a, _mm_cvtsi32_si128(count));
#else
  return _mm512_and_si512(
    _mm512_srl_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi16((int16_t) (uint16_t) (0xffff >> count)));
#endif
}

static SIMD_INLINE Vec<Short, 64> srl(const Vec<Short, 64> &a,
                                      const uint8_t count)
{
#ifdef __AVX512BW__
  return _mm512_srl_epi16(a, _mm_cvtsi32_si128(count));
#else
  return _mm512_and_si512(
    _mm512_srl_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi16((int16_t) (uint16_t) (0xffff >> count)));
#endif
}

static SIMD_INLINE Vec<Int, 64> srl(const Vec<Int, 64> &a, const uint8_t count)
{
  return _mm512_srl_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 64> srl(const Vec<Long, 64> &a,
                                     const uint8_t count)
{
  return _mm512_srl_epi64(a, _mm_cvtsi32_si128(count));
}

// ---------------------------------------------------------------------------
// sll
// ---------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 64> sll(const Vec<Byte, 64> &a,
                                     const uint8_t count)
{
  return _mm512_and_si512(
    _mm512_sll_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<SignedByte, 64> sll(const Vec<SignedByte, 64> &a,
                                           const uint8_t count)
{
  return _mm512_and_si512(
    _mm512_sll_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi8((int8_t) (uint8_t) (0xff & (0xff << count))));
}

static SIMD_INLINE Vec<Word, 64> sll(const Vec<Word, 64> &a,
                                     const uint8_t count)
{
#ifdef __AVX512BW__
  return _mm512_sll_epi16(a, _mm_cvtsi32_si128(count));
#else
  return _mm512_and_si512(
    _mm512_sll_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi16((int16_t) (uint16_t) (0xffff & (0xffff << count))));
#endif
}

static SIMD_INLINE Vec<Short, 64> sll(const Vec<Short, 64> &a,
                                      const uint8_t count)
{
#ifdef __AVX512BW__
  return _mm512_sll_epi16(a, _mm_cvtsi32_si128(count));
#else
  return _mm512_and_si512(
    _mm512_sll_epi32(a, _mm_cvtsi32_si128(count)),
    _mm512_set1_epi16((int16_t) (uint16_t) (0xffff & (0xffff << count))));
#endif
}

static SIMD_INLINE Vec<Int, 64> sll(const Vec<Int, 64> &a, const uint8_t count)
{
  return _mm512_sll_epi32(a, _mm_cvtsi32_si128(count));
}

static SIMD_INLINE Vec<Long, 64> sll(const Vec<Long, 64> &a,
                                     const uint8_t count)
{
  return _mm512_sll_epi64(a, _mm_cvtsi32_si128(count));
}

// 05. Aug 22 (Jonas Keller):
// Improved implementation of hadd, hadds, hsub and hsubs,
// implementation does not use emulation via AVX anymore.
// Byte and SignedByte are now supported as well.
// The new implementation is faster for Int and Float, but
// slower for Word and Short for some reason.

// ---------------------------------------------------------------------------
// hadd v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> hadd(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return add(x, y);
}

// ---------------------------------------------------------------------------
// hadds v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> hadds(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return adds(x, y);
}

// ---------------------------------------------------------------------------
// hsub v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> hsub(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return sub(x, y);
}

// ---------------------------------------------------------------------------
// hsubs v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> hsubs(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return subs(x, y);
}

// ---------------------------------------------------------------------------
// permute_64_16: permutation of 128-bit lanes, two sources v
// ---------------------------------------------------------------------------

// template parameter:
// - ABi (0/1): select lane i from a (0) or from b (1)
// - Ii (0..3): select lane i from index Ii in either a or b

template <size_t AB0, size_t I0, size_t AB1, size_t I1, size_t AB2, size_t I2,
          size_t AB3, size_t I3, typename T>
static SIMD_INLINE Vec<T, 64> permute_64_16(const Vec<T, 64> &a,
                                            const Vec<T, 64> &b)

{
  const __m512i mask = _mm512_set_epi64(
    (AB3 << 3) | (2 * I3 + 1), (AB3 << 3) | (2 * I3), (AB2 << 3) | (2 * I2 + 1),
    (AB2 << 3) | (2 * I2), (AB1 << 3) | (2 * I1 + 1), (AB1 << 3) | (2 * I1),
    (AB0 << 3) | (2 * I0 + 1), (AB0 << 3) | (2 * I0));
  // reinterpret as Int in case T is not an integer type
  const Vec<Int, 64> res = _mm512_permutex2var_epi64(
    reinterpret(a, OutputType<Int>()), mask, reinterpret(b, OutputType<Int>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// alignre v
// ---------------------------------------------------------------------------

// Li, Hi: lanes
// n = IMM * sizeof(T) [#bytes]
//
// input: H0 H1 H2 H3
//        L0 L1 L2 L3         NB
// ==================
// n<16:  L1 L2 L3 H0   L,H   1
//        L0 L1 L2 L3   L,H   0
// ------------------
// n<32:  L2 L3 H0 H1   L,H   2
//        L1 L2 L3 H0   L,H   1
// ------------------
// n<48:  L3 H0 H1 H2   L,H   3
//        L2 L3 H0 H1   L,H   2
// ------------------
// n<64:  H0 H1 H2 H3   L,H   4
//        L3 H0 H1 H2   L,H   3
// ------------------
// n<80:  H1 H2 H3 0    H,0   1
//        H0 H1 H2 H3   H,0   0
// ------------------
// n<96:  H2 H3 0  0    H,0   2
//        H1 H2 H3 0    H,0   1
// ------------------
// n<112: H3 0  0  0    H,0   3
//        H2 H3 0  0    H,0   2
// ------------------
// n<128: 0  0  0  0    H,0   4
//        H3 0  0  0    H,0   3

// align_64_16 v (helper for alignre)

// 16-byte lanes:          AB0 I0 AB1 I1 AB2 I2 AB3 I3
// NB=0: a0 a1 a2 a3         0 0    0 1    0 2    0 3
// NB=1: a1 a2 a3 b0         0 1    0 2    0 3    1 0
// NB=2: a2 a3 b0 b1         0 2    0 3    1 0    1 1
// NB=3: a3 b0 b1 b2         0 3    1 0    1 1    1 2
// NB=4: b0 b1 b2 b3         1 0    1 1    1 2    1 3

template <size_t NB, typename T>
static SIMD_INLINE Vec<T, 64> align_64_16(const Vec<T, 64> &a,
                                          const Vec<T, 64> &b)
{
  SIMD_IF_CONSTEXPR (NB == 0) {
    return a;
  } else SIMD_IF_CONSTEXPR (NB == 4) {
    return b;
  } else {
    return permute_64_16<(NB > 3), (NB % 4), (NB > 2), (NB + 1) % 4, (NB > 1),
                         (NB + 2) % 4, (NB > 0), (NB + 3) % 4>(a, b);
  }
}

// COUNT: in elements
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 64> alignre(const Vec<T, 64> &h, const Vec<T, 64> &l)
{
  const auto byteShift = COUNT * sizeof(T);
  SIMD_IF_CONSTEXPR (byteShift < 128) {
    const auto laneShift = byteShift / 16;
    const Vec<T, 64> L   = (byteShift < 64) ? l : h;
    const Vec<T, 64> H =
      (byteShift < 64) ? h : setzero(OutputType<T>(), Integer<64>());
    const Vec<T, 64> ll = align_64_16<laneShift % 4>(L, H);
    const Vec<T, 64> hh = align_64_16<laneShift % 4 + 1>(L, H);
    return reinterpret(Vec<Byte, 64>(x_mm512_alignr_epi8<byteShift % 16>(
                         reinterpret(hh, OutputType<Byte>()),
                         reinterpret(ll, OutputType<Byte>()))),
                       OutputType<T>());
  } else {
    return setzero(OutputType<T>(), Integer<64>());
  }
}

// ---------------------------------------------------------------------------
// srle: element-wise right shift (via alignre) v
// ---------------------------------------------------------------------------

// TODO: srle: solution with byte-wise shift intrinsics instead of align?

// COUNT: in elements
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 64> srle(const Vec<T, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < Vec<T, 64>::elements) {
    return alignre<COUNT>(setzero(OutputType<T>(), Integer<64>()), a);
  } else {
    return setzero(OutputType<T>(), Integer<64>());
  }
}

// ---------------------------------------------------------------------------
// slle: element-wise left shift (via alignre) v
// ---------------------------------------------------------------------------

// TODO: slle: solution with byte-wise shift intrinsics instead of align?

// COUNT: in elements
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 64> slle(const Vec<T, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < Vec<T, 64>::elements) {
    return alignre<Vec<T, 64>::elements - COUNT>(
      a, setzero(OutputType<T>(), Integer<64>()));
  } else {
    return setzero(OutputType<T>(), Integer<64>());
  }
}

// ---------------------------------------------------------------------------
// swizzle v
// ---------------------------------------------------------------------------

// ---------- swizzle aux functions -----------

// alignoff is the element-wise offset (relates to size of byte)
template <size_t ALIGNOFF>
static SIMD_INLINE __m512i align_shuffle_512(__m512i lo, __m512i hi,
                                             __m512i mask)
{
  static_assert(ALIGNOFF < 32, "");
  return x_mm512_shuffle_epi8(x_mm512_alignr_epi8<ALIGNOFF>(hi, lo), mask);
}

// swizzle_64_16: swizzling of 128-bit lanes (for swizzle) v

// each block (e.g. h2) is a 128-bit lane:
//
// example:
//
//      ----v[0]---|----v[1]---
// n=2: l0 L0 h0 H0 l1 L1 h1 H1
//      --    --    --    --
//         --    --    --    --
//  ->  l0 h0 l1 h1 L0 H0 L1 H1
//      -----------|-----------
//
//
//      ----v[0]---|----v[1]---|----v[2]---
// n=3: l0 L0 h0 H0 l1 L1 h1 H1 l2 L2 h2 H2
//      --       --       --       --
//         --       --       --       --
//            --       --       --       --
//  ->  l0 H0 h1 L2 L0 l1 H1 h2 h0 L1 l2 H2
//      -----------|-----------|-----------
//
//
//      ----v[0]---|----v[1]---|----v[2]---|----v[3]---
// n=4: l0 L0 h0 H0 l1 L1 h1 H1 l2 L2 h2 H2 l3 L3 h3 H3
//      --          --          --          --
//         --          --          --          --
//            --          --          --          --
//               --          --          --          --
//  ->  l0 l1 l2 l3 L0 L1 L2 L3 h0 h1 h2 h3 H0 H1 H2 H3
//      -----------|-----------|-----------|-----------
//
//
//      ----v[0]---|----v[1]---|----v[2]---|----v[3]---|----v[4]---
// n=5: l0 L0 h0 H0 l1 L1 h1 H1 l2 L2 h2 H2 l3 L3 h3 H3 l4 L4 h4 H4
//      --             --             --             --
//         --             --             --             --
//            --             --             --             --
//               --             --             --             --
//                  --             --             --             --
//  ->  l0 L1 h2 H3 L0 h1 H2 l4 h0 H1 l3 L4 H0 l2 L3 h4 l1 L2 h3 H4
//      -----------|-----------|-----------|-----------|-----------

// primary template
template <size_t N, typename T>
struct Swizzle_64_16;

// N=2
// vIn:  0 1 2 3 | 4 5 6 7
// vOut: 0 2 4 6 | 1 3 5 7
template <typename T>
struct Swizzle_64_16<2, T>
{
  static SIMD_INLINE void _swizzle_64_16(const Vec<T, 64> vIn[2],
                                         Vec<T, 64> vOut[2])
  {
    vOut[0] = permute_64_16<0, 0, 0, 2, 1, 0, 1, 2>(vIn[0], vIn[1]);
    vOut[1] = permute_64_16<0, 1, 0, 3, 1, 1, 1, 3>(vIn[0], vIn[1]);
  }
};

// N=3
// vIn:  0 1 2 3 | 4  5 6  7 | 8 9 10 11
// vTmp: 0 6 1 4 | 7 10 5  8 | 3 9  2 11
// vOut: 0 3 6 9 | 1  4 7 10 | 2 5  8 11
template <typename T>
struct Swizzle_64_16<3, T>
{
  static SIMD_INLINE void _swizzle_64_16(const Vec<T, 64> vIn[3],
                                         Vec<T, 64> vOut[3])
  {
    Vec<T, 64> vTmp[3];
    vTmp[0] = permute_64_16<0, 0, 1, 2, 0, 1, 1, 0>(vIn[0], vIn[1]);
    vTmp[1] = permute_64_16<0, 3, 1, 2, 0, 1, 1, 0>(vIn[1], vIn[2]);
    vTmp[2] = permute_64_16<0, 3, 1, 1, 0, 2, 1, 3>(vIn[0], vIn[2]);

    vOut[0] = permute_64_16<0, 0, 1, 0, 0, 1, 1, 1>(vTmp[0], vTmp[2]);
    vOut[1] = permute_64_16<0, 2, 0, 3, 1, 0, 1, 1>(vTmp[0], vTmp[1]);
    vOut[2] = permute_64_16<1, 2, 0, 2, 0, 3, 1, 3>(vTmp[1], vTmp[2]);
  }
};

// N=4
// vIn:  0 1 2  3 | 4 5 6  7 | 8  9 10 11 | 12 13 14 15
// vTmp: 0 4 1  5 | 2 6 3  7 | 8 12  9 13 | 10 14 11 15
// vOut: 0 4 8 12 | 1 5 9 13 | 2  6 10 14 |  3  7 11 15
template <typename T>
struct Swizzle_64_16<4, T>
{
  static SIMD_INLINE void _swizzle_64_16(const Vec<T, 64> vIn[4],
                                         Vec<T, 64> vOut[4])
  {
    Vec<T, 64> vTmp[4];
    vTmp[0] = permute_64_16<0, 0, 1, 0, 0, 1, 1, 1>(vIn[0], vIn[1]);
    vTmp[1] = permute_64_16<0, 2, 1, 2, 0, 3, 1, 3>(vIn[0], vIn[1]);
    vTmp[2] = permute_64_16<0, 0, 1, 0, 0, 1, 1, 1>(vIn[2], vIn[3]);
    vTmp[3] = permute_64_16<0, 2, 1, 2, 0, 3, 1, 3>(vIn[2], vIn[3]);

    vOut[0] = permute_64_16<0, 0, 0, 1, 1, 0, 1, 1>(vTmp[0], vTmp[2]);
    vOut[1] = permute_64_16<0, 2, 0, 3, 1, 2, 1, 3>(vTmp[0], vTmp[2]);
    vOut[2] = permute_64_16<0, 0, 0, 1, 1, 0, 1, 1>(vTmp[1], vTmp[3]);
    vOut[3] = permute_64_16<0, 2, 0, 3, 1, 2, 1, 3>(vTmp[1], vTmp[3]);
  }
};

// N=5
// vIn:  0  1  2  3 | 4  5  6  7 | 8  9 10 11 | 12 13 14 15 | 16 17 18 19
// vTmp: 5 10  6 11 | 1 16  3 18 | 8 13  9 14 |  7 17  4 19 |  0 15  2 12
// vOut: 0  5 10 15 | 1  6 11 16 | 2  7 12 17 |  3  8 13 18 |  4  9 14 19
template <typename T>
struct Swizzle_64_16<5, T>
{
  static SIMD_INLINE void _swizzle_64_16(const Vec<T, 64> vIn[5],
                                         Vec<T, 64> vOut[5])
  {
    Vec<T, 64> vTmp[5];
    vTmp[0] = permute_64_16<0, 1, 1, 2, 0, 2, 1, 3>(vIn[1], vIn[2]);
    vTmp[1] = permute_64_16<0, 1, 1, 0, 0, 3, 1, 2>(vIn[0], vIn[4]);
    vTmp[2] = permute_64_16<0, 0, 1, 1, 0, 1, 1, 2>(vIn[2], vIn[3]);
    vTmp[3] = permute_64_16<0, 3, 1, 1, 0, 0, 1, 3>(vIn[1], vIn[4]);
    vTmp[4] = permute_64_16<0, 0, 1, 3, 0, 2, 1, 0>(vIn[0], vIn[3]);

    vOut[0] = permute_64_16<1, 0, 0, 0, 0, 1, 1, 1>(vTmp[0], vTmp[4]);
    vOut[1] = permute_64_16<1, 0, 0, 2, 0, 3, 1, 1>(vTmp[0], vTmp[1]);
    vOut[2] = permute_64_16<1, 2, 0, 0, 1, 3, 0, 1>(vTmp[3], vTmp[4]);
    vOut[3] = permute_64_16<0, 2, 1, 0, 1, 1, 0, 3>(vTmp[1], vTmp[2]);
    vOut[4] = permute_64_16<1, 2, 0, 2, 0, 3, 1, 3>(vTmp[2], vTmp[3]);
  }
};

// swizzle lanes (for implementation of swizzle functions)
template <size_t N, typename T>
static SIMD_INLINE void swizzle_64_16(const Vec<T, 64> vIn[N],
                                      Vec<T, 64> vOut[N])
{
  Swizzle_64_16<N, T>::_swizzle_64_16(vIn, vOut);
}

// ---------- swizzle (AoS to SoA) ----------

// 01. Apr 23 (Jonas Keller): switched from using tag dispatching to using
// enable_if SFINAE, which allows more cases with the same implementation
// to be combined

// -------------------- n = 1 --------------------

// all types
template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 64>[1], Integer<1>)
{
  // v remains unchanged
}

// -------------------- n = 2 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF((sizeof(T) <= 2 && std::is_integral<T>::value))>
static SIMD_INLINE void swizzle(Vec<T, 64> v[2], Integer<2>)
{
  Vec<T, 64> vs[2];
  swizzle_64_16<2>(v, vs);
  const __m512i mask = _mm512_broadcast_i32x4(get_swizzle_mask<2, T>());
  const __m512i s[2] = {
    x_mm512_shuffle_epi8(vs[0], mask),
    x_mm512_shuffle_epi8(vs[1], mask),
  };
  v[0] = _mm512_unpacklo_epi64(s[0], s[1]);
  v[1] = _mm512_unpackhi_epi64(s[0], s[1]);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[2], Integer<2>)
{
  const Vec<Float, 64> vFloat[2] = {
    reinterpret(v[0], OutputType<Float>()),
    reinterpret(v[1], OutputType<Float>()),
  };
  Vec<Float, 64> vs[2];
  swizzle_64_16<2>(vFloat, vs);
  const Vec<Float, 64> vOut[2] = {
    _mm512_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(2, 0, 2, 0)),
    _mm512_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(3, 1, 3, 1)),
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[2], Integer<2>)
{
  const Vec<Double, 64> vDouble[2] = {
    reinterpret(v[0], OutputType<Double>()),
    reinterpret(v[1], OutputType<Double>()),
  };
  Vec<Double, 64> vs[2];
  swizzle_64_16<2>(vDouble, vs);
  const Vec<Double, 64> vOut[2] = {
    _mm512_shuffle_pd(vs[0], vs[1], 0x00),
    _mm512_shuffle_pd(vs[0], vs[1], 0xFF),
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
}

// -------------------- n = 3 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF((sizeof(T) <= 2 && std::is_integral<T>::value))>
static SIMD_INLINE void swizzle(Vec<T, 64> v[3], Integer<3>)
{
  Vec<T, 64> vs[3];
  swizzle_64_16<3>(v, vs);
  __m512i mask = _mm512_broadcast_i32x4(get_swizzle_mask<3, T>());
  __m512i s0   = align_shuffle_512<0>(vs[0], vs[1], mask);
  __m512i s1   = align_shuffle_512<12>(vs[0], vs[1], mask);
  __m512i s2   = align_shuffle_512<8>(vs[1], vs[2], mask);
  __m512i s3   = align_shuffle_512<4>(vs[2], _mm512_undefined_epi32(), mask);
  __m512i l01  = _mm512_unpacklo_epi32(s0, s1);
  __m512i h01  = _mm512_unpackhi_epi32(s0, s1);
  __m512i l23  = _mm512_unpacklo_epi32(s2, s3);
  __m512i h23  = _mm512_unpackhi_epi32(s2, s3);
  v[0]         = _mm512_unpacklo_epi64(l01, l23);
  v[1]         = _mm512_unpackhi_epi64(l01, l23);
  v[2]         = _mm512_unpacklo_epi64(h01, h23);
}

// 32 bit types
// from Stan Melax: "3D Vector Normalization..."
// https://software.intel.com/en-us/articles/3d-vector-normalization-using-512-bit-intel-advanced-vector-extensions-intel-avx
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[3], Integer<3>)
{
  const Vec<Float, 64> vFloat[3] = {
    reinterpret(v[0], OutputType<Float>()),
    reinterpret(v[1], OutputType<Float>()),
    reinterpret(v[2], OutputType<Float>()),
  };
  Vec<Float, 64> vs[3];
  swizzle_64_16<3>(vFloat, vs);
  // x0y0z0x1 = v[0]
  // y1z1x2y2 = v[1]
  // z2x3y3z3 = v[2]
  __m512 x2y2x3y3 = _mm512_shuffle_ps(vs[1], vs[2], _MM_SHUFFLE(2, 1, 3, 2));
  __m512 y0z0y1z1 = _mm512_shuffle_ps(vs[0], vs[1], _MM_SHUFFLE(1, 0, 2, 1));
  // x0x1x2x3
  const Vec<Float, 64> vOut0 =
    _mm512_shuffle_ps(vs[0], x2y2x3y3, _MM_SHUFFLE(2, 0, 3, 0));
  // y0y1y2y3
  const Vec<Float, 64> vOut1 =
    _mm512_shuffle_ps(y0z0y1z1, x2y2x3y3, _MM_SHUFFLE(3, 1, 2, 0));
  // z0z1z2z3
  const Vec<Float, 64> vOut2 =
    _mm512_shuffle_ps(y0z0y1z1, vs[2], _MM_SHUFFLE(3, 0, 3, 1));
  v[0] = reinterpret(vOut0, OutputType<T>());
  v[1] = reinterpret(vOut1, OutputType<T>());
  v[2] = reinterpret(vOut2, OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[3], Integer<3>)
{
  const Vec<Double, 64> vDouble[3] = {
    reinterpret(v[0], OutputType<Double>()),
    reinterpret(v[1], OutputType<Double>()),
    reinterpret(v[2], OutputType<Double>()),
  };
  Vec<Double, 64> vs[3];
  swizzle_64_16<3>(vDouble, vs);
  const Vec<Double, 64> vOut[3] = {
    _mm512_shuffle_pd(vs[0], vs[1], 0xaa), // 0b1010_1010
    _mm512_shuffle_pd(vs[0], vs[2], 0x55), // 0b0101_0101
    _mm512_shuffle_pd(vs[1], vs[2], 0xaa), // 0b1010_1010
  };
  v[0] = reinterpret(vOut[0], OutputType<T>());
  v[1] = reinterpret(vOut[1], OutputType<T>());
  v[2] = reinterpret(vOut[2], OutputType<T>());
}

// -------------------- n = 4 --------------------

// 8 and 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF((sizeof(T) <= 2 && std::is_integral<T>::value))>
static SIMD_INLINE void swizzle(Vec<T, 64> v[4], Integer<4>)
{
  Vec<T, 64> vs[4];
  swizzle_64_16<4>(v, vs);
  __m512i mask = _mm512_broadcast_i32x4(get_swizzle_mask<4, T>());
  __m512i s[4];
  for (size_t j = 0; j < 4; j++) s[j] = x_mm512_shuffle_epi8(vs[j], mask);
  __m512i l01 = _mm512_unpacklo_epi32(s[0], s[1]);
  __m512i h01 = _mm512_unpackhi_epi32(s[0], s[1]);
  __m512i l23 = _mm512_unpacklo_epi32(s[2], s[3]);
  __m512i h23 = _mm512_unpackhi_epi32(s[2], s[3]);
  v[0]        = _mm512_unpacklo_epi64(l01, l23);
  v[1]        = _mm512_unpackhi_epi64(l01, l23);
  v[2]        = _mm512_unpacklo_epi64(h01, h23);
  v[3]        = _mm512_unpackhi_epi64(h01, h23);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[4], Integer<4>)
{
  Vec<Int, 64> vInt[4];
  for (size_t i = 0; i < 4; i++) vInt[i] = reinterpret(v[i], OutputType<Int>());
  Vec<Int, 64> vs[4];
  swizzle_64_16<4>(vInt, vs);
  const __m512i s[4] = {
    _mm512_unpacklo_epi32(vs[0], vs[1]),
    _mm512_unpackhi_epi32(vs[0], vs[1]),
    _mm512_unpacklo_epi32(vs[2], vs[3]),
    _mm512_unpackhi_epi32(vs[2], vs[3]),
  };
  const Vec<Int, 64> vOut[4] = {
    _mm512_unpacklo_epi64(s[0], s[2]),
    _mm512_unpackhi_epi64(s[0], s[2]),
    _mm512_unpacklo_epi64(s[1], s[3]),
    _mm512_unpackhi_epi64(s[1], s[3]),
  };
  for (size_t i = 0; i < 4; i++) v[i] = reinterpret(vOut[i], OutputType<T>());
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[4], Integer<4>)
{
  Vec<Double, 64> vDouble[4];
  for (size_t i = 0; i < 4; i++)
    vDouble[i] = reinterpret(v[i], OutputType<Double>());
  Vec<Double, 64> vs[4];
  swizzle_64_16<4>(vDouble, vs);
  const Vec<Double, 64> vOut[4] = {
    _mm512_shuffle_pd(vs[0], vs[2], 0x00), // 0b0000_0000
    _mm512_shuffle_pd(vs[0], vs[2], 0xFF), // 0b1111_1111
    _mm512_shuffle_pd(vs[1], vs[3], 0x00), // 0b0000_0000
    _mm512_shuffle_pd(vs[1], vs[3], 0xFF), // 0b1111_1111
  };
  for (size_t i = 0; i < 4; i++) v[i] = reinterpret(vOut[i], OutputType<T>());
}

// -------------------- n = 5 --------------------

// 8 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 1 && std::is_integral<T>::value)>
static SIMD_INLINE void swizzle(Vec<T, 64> v[5], Integer<5>)
{
  Vec<T, 64> vs[5];
  swizzle_64_16<5>(v, vs);
  const __m512i mask = _mm512_broadcast_i32x4(get_swizzle_mask<5, T>());
  const __m512i s[8] = {
    align_shuffle_512<0>(vs[0], vs[1], mask),
    align_shuffle_512<10>(vs[0], vs[1], mask),
    align_shuffle_512<4>(vs[1], vs[2], mask),
    align_shuffle_512<14>(vs[1], vs[2], mask),
    align_shuffle_512<8>(vs[2], vs[3], mask),
    align_shuffle_512<2>(vs[3], vs[4], mask),
    align_shuffle_512<12>(vs[3], vs[4], mask),
    align_shuffle_512<6>(vs[4], _mm512_undefined_epi32(), mask),
  };
  __m512i l01     = x_mm512_unpacklo_epi16(s[0], s[1]);
  __m512i h01     = x_mm512_unpackhi_epi16(s[0], s[1]);
  __m512i l23     = x_mm512_unpacklo_epi16(s[2], s[3]);
  __m512i h23     = x_mm512_unpackhi_epi16(s[2], s[3]);
  __m512i l45     = x_mm512_unpacklo_epi16(s[4], s[5]);
  __m512i h45     = x_mm512_unpackhi_epi16(s[4], s[5]);
  __m512i l67     = x_mm512_unpacklo_epi16(s[6], s[7]);
  __m512i h67     = x_mm512_unpackhi_epi16(s[6], s[7]);
  __m512i ll01l23 = _mm512_unpacklo_epi32(l01, l23);
  __m512i hl01l23 = _mm512_unpackhi_epi32(l01, l23);
  __m512i ll45l67 = _mm512_unpacklo_epi32(l45, l67);
  __m512i hl45l67 = _mm512_unpackhi_epi32(l45, l67);
  __m512i lh01h23 = _mm512_unpacklo_epi32(h01, h23);
  __m512i lh45h67 = _mm512_unpacklo_epi32(h45, h67);
  v[0]            = _mm512_unpacklo_epi64(ll01l23, ll45l67);
  v[1]            = _mm512_unpackhi_epi64(ll01l23, ll45l67);
  v[2]            = _mm512_unpacklo_epi64(hl01l23, hl45l67);
  v[3]            = _mm512_unpackhi_epi64(hl01l23, hl45l67);
  v[4]            = _mm512_unpacklo_epi64(lh01h23, lh45h67);
}

// 16 bit integer types
template <typename T,
          SIMD_ENABLE_IF(sizeof(T) == 2 && std::is_integral<T>::value),
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[5], Integer<5>)
{
  Vec<T, 64> vs[5];
  swizzle_64_16<5>(v, vs);
  const __m512i mask = _mm512_broadcast_i32x4(get_swizzle_mask<5, T>());
  const __m512i s[8] = {
    align_shuffle_512<0>(vs[0], vs[1], mask),
    align_shuffle_512<6>(vs[0], vs[1], mask),
    align_shuffle_512<4>(vs[1], vs[2], mask),
    align_shuffle_512<10>(vs[1], vs[2], mask),
    align_shuffle_512<8>(vs[2], vs[3], mask),
    align_shuffle_512<14>(vs[2], vs[3], mask),
    align_shuffle_512<12>(vs[3], vs[4], mask),
    align_shuffle_512<2>(vs[4], _mm512_undefined_epi32(), mask),
  };
  __m512i l02 = _mm512_unpacklo_epi32(s[0], s[2]);
  __m512i h02 = _mm512_unpackhi_epi32(s[0], s[2]);
  __m512i l13 = _mm512_unpacklo_epi32(s[1], s[3]);
  __m512i l46 = _mm512_unpacklo_epi32(s[4], s[6]);
  __m512i h46 = _mm512_unpackhi_epi32(s[4], s[6]);
  __m512i l57 = _mm512_unpacklo_epi32(s[5], s[7]);
  v[0]        = _mm512_unpacklo_epi64(l02, l46);
  v[1]        = _mm512_unpackhi_epi64(l02, l46);
  v[2]        = _mm512_unpacklo_epi64(h02, h46);
  v[3]        = _mm512_unpacklo_epi64(l13, l57);
  v[4]        = _mm512_unpackhi_epi64(l13, l57);
}

// 32 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 4), typename = void,
          typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[5], Integer<5>)
{
  Vec<Int, 64> vInt[5];
  for (size_t i = 0; i < 5; i++) {
    vInt[i] = reinterpret(v[i], OutputType<Int>());
  }
  Vec<Int, 64> vs[5];
  swizzle_64_16<5>(vInt, vs);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  // v[0]: 0 1 2 3
  // v[1]: 4 x x x
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                   x x x   x
  // 5 6 7 8
  __m512i s2 = x_mm512_alignr_epi8<4>(vs[2], vs[1]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                             x  x  x    x
  // 9 x x x
  __m512i s3 = x_mm512_alignr_epi8<4>(vs[3], vs[2]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                x  x    x  x
  // 10 11 12 13
  __m512i s4 = x_mm512_alignr_epi8<8>(vs[3], vs[2]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                              x  x    x  x
  // 14 x x x
  __m512i s5 = x_mm512_alignr_epi8<8>(vs[4], vs[3]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                 X    X  X  X
  // 15 16 17 18
  __m512i s6 = x_mm512_alignr_epi8<12>(vs[4], vs[3]);
  // v:    0 1 2 3 | 4 5 6 7 | 8 9 10 11 | 12 13 14 15 | 16 17 18 19
  //                                                               X X X X
  // 19 x x x
  __m512i s7 = x_mm512_alignr_epi8<12>(vs[0], vs[4]);
  // 0 1 2 3 / 5 6 7 8 -> 0 5 1 6 / 2 7 3 8
  __m512i l02 = _mm512_unpacklo_epi32(vs[0], s2);
  __m512i h02 = _mm512_unpackhi_epi32(vs[0], s2);
  // 4 x x x / 9 x x x -> 4 9 x x
  __m512i l13 = _mm512_unpacklo_epi32(vs[1], s3);
  // 10 11 12 13 / 15 16 17 18 -> 10 15 11 13 / 12 17 13 18
  __m512i l46 = _mm512_unpacklo_epi32(s4, s6);
  __m512i h46 = _mm512_unpackhi_epi32(s4, s6);
  // 14 x x x / 19 x x x -> 14 19 x x
  __m512i l57                = _mm512_unpacklo_epi32(s5, s7);
  const Vec<Int, 64> vOut[5] = {
    // 0 5 1 6 / 10 15 11 13 -> 0 5 10 15 / 1 6 11 16
    _mm512_unpacklo_epi64(l02, l46),
    _mm512_unpackhi_epi64(l02, l46),
    // 2 7 3 8 / 12 17 13 18 -> 2 7 12 17 / 3 8 13 18
    _mm512_unpacklo_epi64(h02, h46),
    _mm512_unpackhi_epi64(h02, h46),
    // 4 9 x x / 14 19 x x -> 4 9 14 19
    _mm512_unpacklo_epi64(l13, l57),
  };
  for (size_t i = 0; i < 5; i++) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// 64 bit types
template <typename T, SIMD_ENABLE_IF(sizeof(T) == 8), typename = void,
          typename = void, typename = void>
static SIMD_INLINE void swizzle(Vec<T, 64> v[5], Integer<5>)
{
  Vec<Double, 64> vDouble[5];
  for (size_t i = 0; i < 5; i++) {
    vDouble[i] = reinterpret(v[i], OutputType<Double>());
  }
  Vec<Double, 64> vs[5];
  swizzle_64_16<5>(vDouble, vs);
  const Vec<Double, 64> vOut[5] = {
    _mm512_shuffle_pd(vs[0], vs[2], 0xaa), // 0b1010_1010
    _mm512_shuffle_pd(vs[0], vs[3], 0x55), // 0b0101_0101
    _mm512_shuffle_pd(vs[1], vs[3], 0xaa), // 0b1010_1010
    _mm512_shuffle_pd(vs[1], vs[4], 0x55), // 0b0101_0101
    _mm512_shuffle_pd(vs[2], vs[4], 0xaa), // 0b1010_1010
  };
  for (size_t i = 0; i < 5; i++) {
    v[i] = reinterpret(vOut[i], OutputType<T>());
  }
}

// ---------------------------------------------------------------------------
// comparison functions
// ---------------------------------------------------------------------------

// 28. Mar 23 (Jonas Keller): checked the constants for _mm512_cmp_ps_mask in
// the Float comparison functions, they match the implementation of the SSE
// versions (see cmpps in Intel manual) and added corresponding comments

// ---------------------------------------------------------------------------
// compare < v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmplt(const Vec<Byte, 64> &a,
                                       const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmplt_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmplt(const Vec<SignedByte, 64> &a,
                                             const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmplt_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmplt(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmplt_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmplt(const Vec<Short, 64> &a,
                                        const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmplt_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmplt(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmplt(a.lo(), b.lo()), cmplt(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmplt(const Vec<Int, 64> &a,
                                      const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmplt_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmplt(const Vec<Long, 64> &a,
                                       const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmplt_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmplt(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmplt_ps (see cmpps instruction
  // in Intel manual)
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_LT_OS)));
}

static SIMD_INLINE Vec<Double, 64> cmplt(const Vec<Double, 64> &a,
                                         const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmplt_pd (see cmppd instruction
  // in Intel manual)
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_LT_OS)));
}

// ---------------------------------------------------------------------------
// compare <= v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmple(const Vec<Byte, 64> &a,
                                       const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmple_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmple(const Vec<SignedByte, 64> &a,
                                             const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmple_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmple(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmple_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmple(const Vec<Short, 64> &a,
                                        const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmple_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmple(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmple(a.lo(), b.lo()), cmple(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmple(const Vec<Int, 64> &a,
                                      const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmple_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmple(const Vec<Long, 64> &a,
                                       const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmple_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmple(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmple_ps (see cmpps instruction
  // in Intel manual)
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_LE_OS)));
}

static SIMD_INLINE Vec<Double, 64> cmple(const Vec<Double, 64> &a,
                                         const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmple_pd (see cmppd instruction
  // in Intel manual)
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_LE_OS)));
}

// ---------------------------------------------------------------------------
// compare == v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmpeq(const Vec<Byte, 64> &a,
                                       const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpeq_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmpeq(const Vec<SignedByte, 64> &a,
                                             const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpeq_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmpeq(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpeq_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmpeq(const Vec<Short, 64> &a,
                                        const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpeq_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmpeq(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmpeq(a.lo(), b.lo()), cmpeq(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmpeq(const Vec<Int, 64> &a,
                                      const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmpeq_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmpeq(const Vec<Long, 64> &a,
                                       const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmpeq_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmpeq(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmpeq_ps (see cmpps instruction
  // in Intel manual)
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_EQ_OQ)));
}

static SIMD_INLINE Vec<Double, 64> cmpeq(const Vec<Double, 64> &a,
                                         const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmpeq_pd (see cmppd instruction
  // in Intel manual)
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_EQ_OQ)));
}

// ---------------------------------------------------------------------------
// compare > v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmpgt(const Vec<Byte, 64> &a,
                                       const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpgt_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmpgt(const Vec<SignedByte, 64> &a,
                                             const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpgt_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmpgt(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpgt_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmpgt(const Vec<Short, 64> &a,
                                        const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpgt_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmpgt(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmpgt(a.lo(), b.lo()), cmpgt(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmpgt(const Vec<Int, 64> &a,
                                      const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmpgt_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmpgt(const Vec<Long, 64> &a,
                                       const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmpgt_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmpgt(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmplt_ps (see cmpps instruction
  // in Intel manual), except this is > instead of <
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_GT_OS)));
}

static SIMD_INLINE Vec<Double, 64> cmpgt(const Vec<Double, 64> &a,
                                         const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmplt_pd (see cmppd instruction
  // in Intel manual), except this is > instead of <
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_GT_OS)));
}

// ---------------------------------------------------------------------------
// compare >= v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmpge(const Vec<Byte, 64> &a,
                                       const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpge_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmpge(const Vec<SignedByte, 64> &a,
                                             const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpge_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmpge(const Vec<Word, 64> &a,
                                       const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpge_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmpge(const Vec<Short, 64> &a,
                                        const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpge_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmpge(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmpge(a.lo(), b.lo()), cmpge(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmpge(const Vec<Int, 64> &a,
                                      const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmpge_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmpge(const Vec<Long, 64> &a,
                                       const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmpge_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmpge(const Vec<Float, 64> &a,
                                        const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmple_ps (see cmpps instruction
  // in Intel manual), except this is >= instead of <=
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_GE_OS)));
}

static SIMD_INLINE Vec<Double, 64> cmpge(const Vec<Double, 64> &a,
                                         const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmple_pd (see cmppd instruction
  // in Intel manual), except this is >= instead of <=
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_GE_OS)));
}

// ---------------------------------------------------------------------------
// compare != v
// ---------------------------------------------------------------------------

// https://stackoverflow.com/questions/48099006/
// different-semantic-of-comparison-intrinsic-instructions-in-avx512

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> cmpneq(const Vec<Byte, 64> &a,
                                        const Vec<Byte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpneq_epu8_mask(a, b));
}

static SIMD_INLINE Vec<SignedByte, 64> cmpneq(const Vec<SignedByte, 64> &a,
                                              const Vec<SignedByte, 64> &b)
{
  return _mm512_movm_epi8(_mm512_cmpneq_epi8_mask(a, b));
}

static SIMD_INLINE Vec<Word, 64> cmpneq(const Vec<Word, 64> &a,
                                        const Vec<Word, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpneq_epu16_mask(a, b));
}

static SIMD_INLINE Vec<Short, 64> cmpneq(const Vec<Short, 64> &a,
                                         const Vec<Short, 64> &b)
{
  return _mm512_movm_epi16(_mm512_cmpneq_epi16_mask(a, b));
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> cmpneq(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(cmpneq(a.lo(), b.lo()), cmpneq(a.hi(), b.hi()));
}

#endif

static SIMD_INLINE Vec<Int, 64> cmpneq(const Vec<Int, 64> &a,
                                       const Vec<Int, 64> &b)
{
  return x_mm512_movm_epi32(_mm512_cmpneq_epi32_mask(a, b));
}

static SIMD_INLINE Vec<Long, 64> cmpneq(const Vec<Long, 64> &a,
                                        const Vec<Long, 64> &b)
{
  return x_mm512_movm_epi64(_mm512_cmpneq_epi64_mask(a, b));
}

static SIMD_INLINE Vec<Float, 64> cmpneq(const Vec<Float, 64> &a,
                                         const Vec<Float, 64> &b)
{
  // same constant as in implementation of _mm_cmpneq_ps (see cmpps instruction
  // in Intel manual)
  return _mm512_castsi512_ps(
    x_mm512_movm_epi32(_mm512_cmp_ps_mask(a, b, _CMP_NEQ_OQ)));
}

static SIMD_INLINE Vec<Double, 64> cmpneq(const Vec<Double, 64> &a,
                                          const Vec<Double, 64> &b)
{
  // same constant as in implementation of _mm_cmpneq_pd (see cmppd instruction
  // in Intel manual)
  return _mm512_castsi512_pd(
    x_mm512_movm_epi64(_mm512_cmp_pd_mask(a, b, _CMP_NEQ_OQ)));
}

// ---------------------------------------------------------------------------
// ifelse v
// ---------------------------------------------------------------------------

// 10. Apr 23 (Jonas Keller): made two versions of ifelse, one for 8 and 16 bit
// data types, and one for 32 and larger data types, so that for the latter
// the blendv instruction can be used even if avx512bw is not available

// NOTE: only works if cond elements are all 1-bits or all 0-bits

// version for 8 and 16 bit data types
template <typename T, SIMD_ENABLE_IF(sizeof(T) <= 2)>
static SIMD_INLINE Vec<T, 64> ifelse(const Vec<T, 64> &cond,
                                     const Vec<T, 64> &trueVal,
                                     const Vec<T, 64> &falseVal)
{
  // TODO: _mm512_movepi8_mask is slower than _mm512_or_si512, _mm512_and_si512
  // or _mm512_andnot_si512 according to the Intel Intrinsics Guide, maybe use
  // the non-avx512bw workaround always?
  // since _mm512_and_si512 and _mm512_andnot_si512 could potentially be
  // executed in parallel, that might be faster
#ifdef __AVX512BW__
  // cond -> __mask64
  const __mmask64 condReg =
    _mm512_movepi8_mask(reinterpret(cond, OutputType<Byte>()));
  // explicitly cast to __m512i to avoid compiler error with -O0
  const __m512i trueReg   = (__m512i) reinterpret(trueVal, OutputType<Byte>());
  const __m512i falseReg  = (__m512i) reinterpret(falseVal, OutputType<Byte>());
  const Vec<Byte, 64> res = _mm512_mask_blend_epi8(condReg, falseReg, trueReg);
#else
  const Vec<Byte, 64> res = _mm512_or_si512(
    _mm512_and_si512(reinterpret(cond, OutputType<Byte>()),
                     reinterpret(trueVal, OutputType<Byte>())),
    _mm512_andnot_si512(reinterpret(cond, OutputType<Byte>()),
                        reinterpret(falseVal, OutputType<Byte>())));
#endif
  return reinterpret(res, OutputType<T>());
}

// version for 32 bit and larger data types
template <typename T, SIMD_ENABLE_IF(sizeof(T) > 2), typename = void>
static SIMD_INLINE Vec<T, 64> ifelse(const Vec<T, 64> &cond,
                                     const Vec<T, 64> &trueVal,
                                     const Vec<T, 64> &falseVal)
{
  // TODO: _mm512_movepi32_mask is slower than _mm512_or_si512, _mm512_and_si512
  // or _mm512_andnot_si512 according to the Intel Intrinsics Guide, maybe use
  // the non-avx512dq workaround always?
  // since _mm512_and_si512 and _mm512_andnot_si512 could potentially be
  // executed in parallel, that might be faster
#ifdef __AVX512DQ__
  // cond -> __mmask16
  const __mmask16 condReg =
    _mm512_movepi32_mask(reinterpret(cond, OutputType<Int>()));
  // explicitly cast to __m512i to avoid compiler error with -O0
  const __m512i trueReg  = (__m512i) reinterpret(trueVal, OutputType<Int>());
  const __m512i falseReg = (__m512i) reinterpret(falseVal, OutputType<Int>());
  const Vec<Int, 64> res = _mm512_mask_blend_epi32(condReg, falseReg, trueReg);
#else
  const Vec<Int, 64> res = _mm512_or_si512(
    _mm512_and_si512(reinterpret(cond, OutputType<Int>()),
                     reinterpret(trueVal, OutputType<Int>())),
    _mm512_andnot_si512(reinterpret(cond, OutputType<Int>()),
                        reinterpret(falseVal, OutputType<Int>())));
#endif
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_and v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> bit_and(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  // reinterpret as byte for float and double versions
  const Vec<Byte, 64> res = _mm512_and_si512(
    reinterpret(a, OutputType<Byte>()), reinterpret(b, OutputType<Byte>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_or v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> bit_or(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  // reinterpret as byte for float and double versions
  const Vec<Byte, 64> res = _mm512_or_si512(reinterpret(a, OutputType<Byte>()),
                                            reinterpret(b, OutputType<Byte>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_andnot v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> bit_andnot(const Vec<T, 64> &a,
                                         const Vec<T, 64> &b)
{
  // reinterpret as byte for float and double versions
  const Vec<Byte, 64> res = _mm512_andnot_si512(
    reinterpret(a, OutputType<Byte>()), reinterpret(b, OutputType<Byte>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_xor v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> bit_xor(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  // reinterpret as byte for float and double versions
  const Vec<Byte, 64> res = _mm512_xor_si512(
    reinterpret(a, OutputType<Byte>()), reinterpret(b, OutputType<Byte>()));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// bit_not v
// ---------------------------------------------------------------------------

// all integer versions
template <typename T>
static SIMD_INLINE Vec<T, 64> bit_not(const Vec<T, 64> &a)
{
  // reinterpret as byte for float and double versions
  // from Agner Fog's VCL vectori256.h operator ~
  const Vec<Byte, 64> res =
    _mm512_xor_si512(reinterpret(a, OutputType<Byte>()), _mm512_set1_epi32(-1));
  return reinterpret(res, OutputType<T>());
}

// ---------------------------------------------------------------------------
// avg: average with rounding down v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__

static SIMD_INLINE Vec<Byte, 64> avg(const Vec<Byte, 64> &a,
                                     const Vec<Byte, 64> &b)
{
  return _mm512_avg_epu8(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<SignedByte, 64> avg(const Vec<SignedByte, 64> &a,
                                           const Vec<SignedByte, 64> &b)
{
  // from Agner Fog's VCL vectori128.h
  const __m512i signbit = _mm512_set1_epi8(int8_t(0x80));
  const __m512i a1      = _mm512_xor_si512(a, signbit); // add 0x80
  const __m512i b1      = _mm512_xor_si512(b, signbit); // add 0x80
  const __m512i m1      = _mm512_avg_epu8(a1, b1);      // unsigned avg
  return _mm512_xor_si512(m1, signbit);                 // sub 0x80
}

static SIMD_INLINE Vec<Word, 64> avg(const Vec<Word, 64> &a,
                                     const Vec<Word, 64> &b)
{
  return _mm512_avg_epu16(a, b);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Short, 64> avg(const Vec<Short, 64> &a,
                                      const Vec<Short, 64> &b)
{
  // from Agner Fog's VCL vectori128.h
  const __m512i signbit = _mm512_set1_epi16(int16_t(0x8000));
  const __m512i a1      = _mm512_xor_si512(a, signbit); // add 0x8000
  const __m512i b1      = _mm512_xor_si512(b, signbit); // add 0x8000
  const __m512i m1      = _mm512_avg_epu16(a1, b1);     // unsigned avg
  return _mm512_xor_si512(m1, signbit);                 // sub 0x8000
}

#else

// non-avx512bw workaround
template <typename T>
static SIMD_INLINE Vec<T, 64> avg(const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  return Vec<T, 64>(avg(a.lo(), b.lo()), avg(a.hi(), b.hi()));
}

#endif

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Int, 64> avg(const Vec<Int, 64> &a,
                                    const Vec<Int, 64> &b)
{
  const auto halfA = _mm512_srai_epi32(a, 1);
  const auto halfB = _mm512_srai_epi32(b, 1);
  const auto sum   = _mm512_add_epi32(halfA, halfB);
  const auto lsb =
    _mm512_and_si512(_mm512_or_si512(a, b), _mm512_set1_epi32(1));
  return _mm512_add_epi32(lsb, sum);
}

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average
static SIMD_INLINE Vec<Long, 64> avg(const Vec<Long, 64> &a,
                                     const Vec<Long, 64> &b)
{
  const auto halfA = _mm512_srai_epi64(a, 1);
  const auto halfB = _mm512_srai_epi64(b, 1);
  const auto sum   = _mm512_add_epi64(halfA, halfB);
  const auto lsb =
    _mm512_and_si512(_mm512_or_si512(a, b), _mm512_set1_epi64(1));
  return _mm512_add_epi64(lsb, sum);
}

// NOTE: Float version doesn't round!
static SIMD_INLINE Vec<Float, 64> avg(const Vec<Float, 64> &a,
                                      const Vec<Float, 64> &b)
{
  return _mm512_mul_ps(_mm512_add_ps(a, b), _mm512_set1_ps(0.5f));
}

// NOTE: Double version doesn't round!
static SIMD_INLINE Vec<Double, 64> avg(const Vec<Double, 64> &a,
                                       const Vec<Double, 64> &b)
{
  return _mm512_mul_pd(_mm512_add_pd(a, b), _mm512_set1_pd(0.5));
}

// ---------------------------------------------------------------------------
// test_all_zeros v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_zeros(const Vec<T, 64> &a)
{
  const auto intA = reinterpret(a, OutputType<Int>());
  return _mm512_test_epi32_mask(intA, intA) == 0;
}

// ---------------------------------------------------------------------------
// test_all_ones v
// ---------------------------------------------------------------------------

// description of testn intrinsics was not clear, chosen other way
// note: contrary to IEEE 754, this function considers -0.0f to be negative
template <typename T>
static SIMD_INLINE bool test_all_ones(const Vec<T, 64> &a)
{
  return test_all_zeros(bit_not(a));
}

// ---------------------------------------------------------------------------
// reverse
// ---------------------------------------------------------------------------

template <typename T, SIMD_ENABLE_IF(sizeof(T) <= 2)>
static SIMD_INLINE Vec<T, 64> reverse(const Vec<T, 64> &a)
{
  __m512i mask;
  SIMD_IF_CONSTEXPR (sizeof(T) == 1) {
    mask = _mm512_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
                           16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,
                           29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,
                           42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,
                           55, 56, 57, 58, 59, 60, 61, 62, 63);
  } else {
    mask = _mm512_set_epi8(1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14,
                           17, 16, 19, 18, 21, 20, 23, 22, 25, 24, 27, 26, 29,
                           28, 31, 30, 33, 32, 35, 34, 37, 36, 39, 38, 41, 40,
                           43, 42, 45, 44, 47, 46, 49, 48, 51, 50, 53, 52, 55,
                           54, 57, 56, 59, 58, 61, 60, 63, 62);
  }
#ifdef __AVX512VBMI__
  return _mm512_permutexvar_epi8(mask, a);
#else
  const Vec<T, 64> r = x_mm512_shuffle_epi8(a, mask);
  return _mm512_permutexvar_epi64(_mm512_set_epi64(1, 0, 3, 2, 5, 4, 7, 6), r);
#endif
}

static SIMD_INLINE Vec<Int, 64> reverse(const Vec<Int, 64> &a)
{
  const auto mask =
    _mm512_set_epi32(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
  return _mm512_permutexvar_epi32(mask, a);
}

static SIMD_INLINE Vec<Long, 64> reverse(const Vec<Long, 64> &a)
{
  return _mm512_permutexvar_epi64(_mm512_set_epi64(0, 1, 2, 3, 4, 5, 6, 7), a);
}

// float version, slightly changed int version
static SIMD_INLINE Vec<Float, 64> reverse(const Vec<Float, 64> &a)
{
  const auto mask =
    _mm512_set_epi32(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
  return _mm512_permutexvar_ps(mask, a);
}

// double version
static SIMD_INLINE Vec<Double, 64> reverse(const Vec<Double, 64> &a)
{
  return _mm512_permutexvar_pd(_mm512_set_epi64(0, 1, 2, 3, 4, 5, 6, 7), a);
}

// ---------------------------------------------------------------------------
// msb2int
// ---------------------------------------------------------------------------

// 27. Aug 22 (Jonas Keller): added msb2int functions

static SIMD_INLINE uint64_t msb2int(const Vec<Int, 64> &a)
{
#ifdef __AVX512DQ__
  return _mm512_movepi32_mask(a);
#else
  const __m512i mask = _mm512_set1_epi32(uint32_t(0x80000000));
  return _mm512_test_epi32_mask(a, mask);
#endif
}

static SIMD_INLINE uint64_t msb2int(const Vec<Long, 64> &a)
{
#ifdef __AVX512DQ__
  return _mm512_movepi64_mask(a);
#else
  const __m512i mask = _mm512_set1_epi64(uint64_t(0x8000000000000000));
  return _mm512_test_epi64_mask(a, mask);
#endif
}

static SIMD_INLINE uint64_t msb2int(const Vec<Float, 64> &a)
{
#ifdef __AVX512DQ__
  return _mm512_movepi32_mask(_mm512_castps_si512(a));
#else
  const __m512i mask = _mm512_set1_epi32(0x80000000);
  return _mm512_test_epi32_mask(_mm512_castps_si512(a), mask);
#endif
}

static SIMD_INLINE uint64_t msb2int(const Vec<Double, 64> &a)
{
#ifdef __AVX512DQ__
  return uint64_t(_mm512_movepi64_mask(_mm512_castpd_si512(a)));
#else

  const __m512i mask = _mm512_set1_epi64(0x8000000000000000);
  // _cvtmask8_u32 requires AVX512DQ, so just convert using implicit conversion
  return _mm512_test_epi64_mask(_mm512_castpd_si512(a), mask);
#endif
}

// from:
// https://lemire.me/blog/2018/01/08/how-fast-can-you-bit-interleave-32-bit-integers/
static SIMD_INLINE uint64_t interleave_uint32_with_zeros(uint32_t input)
{
  uint64_t word = input;
  word          = (word ^ (word << 16)) & 0x0000ffff0000ffff;
  word          = (word ^ (word << 8)) & 0x00ff00ff00ff00ff;
  word          = (word ^ (word << 4)) & 0x0f0f0f0f0f0f0f0f;
  word          = (word ^ (word << 2)) & 0x3333333333333333;
  word          = (word ^ (word << 1)) & 0x5555555555555555;
  return word;
}

static SIMD_INLINE uint64_t msb2int(const Vec<Byte, 64> &a)
{
#ifdef __AVX512BW__
  return _mm512_movepi8_mask(a);
#else
  const uint64_t part3 = msb2int(reinterpret(a, OutputType<Int>()));
  const uint64_t part2 = msb2int(reinterpret(slle<1>(a), OutputType<Int>()));
  const uint64_t part1 = msb2int(reinterpret(slle<2>(a), OutputType<Int>()));
  const uint64_t part0 = msb2int(reinterpret(slle<3>(a), OutputType<Int>()));
  // TODO: is there a more efficient way to interleave with 3 zeros instead of
  // interleaving with 1 zero twice?
  const uint64_t part3_with_zeros =
    interleave_uint32_with_zeros(interleave_uint32_with_zeros(part3));
  const uint64_t part2_with_zeros =
    interleave_uint32_with_zeros(interleave_uint32_with_zeros(part2));
  const uint64_t part1_with_zeros =
    interleave_uint32_with_zeros(interleave_uint32_with_zeros(part1));
  const uint64_t part0_with_zeros =
    interleave_uint32_with_zeros(interleave_uint32_with_zeros(part0));
  return part0_with_zeros | (part1_with_zeros << 1) | (part2_with_zeros << 2) |
         (part3_with_zeros << 3);
#endif
}

static SIMD_INLINE uint64_t msb2int(const Vec<SignedByte, 64> &a)
{
  return msb2int(reinterpret(a, OutputType<Byte>()));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Short, 64> &a)
{
#ifdef __AVX512BW__
  return _mm512_movepi16_mask(a);
#else
  const uint64_t odd  = msb2int(reinterpret(a, OutputType<Int>()));
  const uint64_t even = msb2int(reinterpret(slle<1>(a), OutputType<Int>()));
  return interleave_uint32_with_zeros(even) |
         (interleave_uint32_with_zeros(odd) << 1);
#endif
}

static SIMD_INLINE uint64_t msb2int(const Vec<Word, 64> &a)
{
  return msb2int(reinterpret(a, OutputType<Short>()));
}

// ---------------------------------------------------------------------------
// int2msb
// ---------------------------------------------------------------------------

// 06. Oct 22 (Jonas Keller): added int2msb functions

static SIMD_INLINE Vec<Byte, 64> int2msb(const uint64_t a, OutputType<Byte>,
                                         Integer<64>)
{
#ifdef __AVX512BW__
  return _mm512_maskz_set1_epi8(__mmask64(a), (int8_t) 0x80);
#else
  const __m256i shuffleIndeces = _mm256_set_epi64x(
    0x0303030303030303, 0x0202020202020202, 0x0101010101010101, 0);
  const __m256i aVecLo =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a), shuffleIndeces);
  const __m256i aVecHi =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a >> 32), shuffleIndeces);
  const __m256i sel        = _mm256_set1_epi64x(0x8040201008040201);
  const __m256i selectedLo = _mm256_and_si256(aVecLo, sel);
  const __m256i selectedHi = _mm256_and_si256(aVecHi, sel);
  const __m256i resultLo   = _mm256_cmpeq_epi8(selectedLo, sel);
  const __m256i resultHi   = _mm256_cmpeq_epi8(selectedHi, sel);
  const __m512i result =
    _mm512_inserti64x4(_mm512_castsi256_si512(resultLo), resultHi, 1);
  return _mm512_and_si512(result, _mm512_set1_epi32(0x80808080));
#endif
}

static SIMD_INLINE Vec<SignedByte, 64> int2msb(const uint64_t a,
                                               OutputType<SignedByte>,
                                               Integer<64>)
{
  return reinterpret(int2msb(a, OutputType<Byte>(), Integer<64>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 64> int2msb(const uint64_t a, OutputType<Short>,
                                          Integer<64>)
{
#ifdef __AVX512BW__
  return _mm512_maskz_set1_epi16(__mmask32(a), (int16_t) 0x8000);
#else
  const __m256i sel = _mm256_set_epi16(
    (int16_t) 0x8000, 0x4000, 0x2000, 0x1000, 0x0800, 0x0400, 0x0200, 0x0100,
    0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004, 0x0002, 0x0001);
  const __m256i aVecLo     = _mm256_set1_epi16(a);
  const __m256i aVecHi     = _mm256_set1_epi16(a >> 16);
  const __m256i selectedLo = _mm256_and_si256(aVecLo, sel);
  const __m256i selectedHi = _mm256_and_si256(aVecHi, sel);
  const __m256i resultLo   = _mm256_cmpeq_epi16(selectedLo, sel);
  const __m256i resultHi   = _mm256_cmpeq_epi16(selectedHi, sel);
  const __m512i result =
    _mm512_inserti64x4(_mm512_castsi256_si512(resultLo), resultHi, 1);
  return _mm512_and_si512(result, _mm512_set1_epi32(0x80008000));
#endif
}

static SIMD_INLINE Vec<Word, 64> int2msb(const uint64_t a, OutputType<Word>,
                                         Integer<64>)
{
  return reinterpret(int2msb(a, OutputType<Short>(), Integer<64>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 64> int2msb(const uint64_t a, OutputType<Int>,
                                        Integer<64>)
{
  return _mm512_maskz_set1_epi32(__mmask16(a), 0x80000000);
}

static SIMD_INLINE Vec<Long, 64> int2msb(const uint64_t a, OutputType<Long>,
                                         Integer<64>)
{
  return _mm512_maskz_set1_epi64(__mmask8(a), 0x8000000000000000);
}

static SIMD_INLINE Vec<Float, 64> int2msb(const uint64_t a, OutputType<Float>,
                                          Integer<64>)
{
  return reinterpret(int2msb(a, OutputType<Int>(), Integer<64>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 64> int2msb(const uint64_t a, OutputType<Double>,
                                           Integer<64>)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_set1_epi64(__mmask8(a), 0x8000000000000000));
}

// ---------------------------------------------------------------------------
// int2bits
// ---------------------------------------------------------------------------

// 09. Oct 22 (Jonas Keller): added int2bits functions

static SIMD_INLINE Vec<Byte, 64> int2bits(const uint64_t a, OutputType<Byte>,
                                          Integer<64>)
{
#ifdef __AVX512BW__
  return _mm512_maskz_set1_epi8(__mmask64(a), (int8_t) 0xff);
#else
  const __m256i shuffleIndeces = _mm256_set_epi64x(
    0x0303030303030303, 0x0202020202020202, 0x0101010101010101, 0);
  const __m256i aVecLo =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a), shuffleIndeces);
  const __m256i aVecHi =
    _mm256_shuffle_epi8(_mm256_set1_epi32(a >> 32), shuffleIndeces);
  const __m256i sel        = _mm256_set1_epi64x(0x8040201008040201);
  const __m256i selectedLo = _mm256_and_si256(aVecLo, sel);
  const __m256i selectedHi = _mm256_and_si256(aVecHi, sel);
  const __m256i resultLo   = _mm256_cmpeq_epi8(selectedLo, sel);
  const __m256i resultHi   = _mm256_cmpeq_epi8(selectedHi, sel);
  return _mm512_inserti64x4(_mm512_castsi256_si512(resultLo), resultHi, 1);
#endif
}

static SIMD_INLINE Vec<SignedByte, 64> int2bits(const uint64_t a,
                                                OutputType<SignedByte>,
                                                Integer<64>)
{
  return reinterpret(int2bits(a, OutputType<Byte>(), Integer<64>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Short, 64> int2bits(const uint64_t a, OutputType<Short>,
                                           Integer<64>)
{
#ifdef __AVX512BW__
  return _mm512_maskz_set1_epi16(__mmask32(a), (int16_t) 0xffff);
#else
  const __m256i sel = _mm256_set_epi16(
    (int16_t) 0x8000, 0x4000, 0x2000, 0x1000, 0x0800, 0x0400, 0x0200, 0x0100,
    0x0080, 0x0040, 0x0020, 0x0010, 0x0008, 0x0004, 0x0002, 0x0001);
  const __m256i aVecLo     = _mm256_set1_epi16(a);
  const __m256i aVecHi     = _mm256_set1_epi16(a >> 16);
  const __m256i selectedLo = _mm256_and_si256(aVecLo, sel);
  const __m256i selectedHi = _mm256_and_si256(aVecHi, sel);
  const __m256i resultLo   = _mm256_cmpeq_epi16(selectedLo, sel);
  const __m256i resultHi   = _mm256_cmpeq_epi16(selectedHi, sel);
  return _mm512_inserti64x4(_mm512_castsi256_si512(resultLo), resultHi, 1);
#endif
}

static SIMD_INLINE Vec<Word, 64> int2bits(const uint64_t a, OutputType<Word>,
                                          Integer<64>)
{
  return reinterpret(int2bits(a, OutputType<Short>(), Integer<64>()),
                     OutputType<Word>());
}

static SIMD_INLINE Vec<Int, 64> int2bits(const uint64_t a, OutputType<Int>,
                                         Integer<64>)
{
  return _mm512_maskz_set1_epi32(__mmask16(a), 0xffffffff);
}

static SIMD_INLINE Vec<Long, 64> int2bits(const uint64_t a, OutputType<Long>,
                                          Integer<64>)
{
  return _mm512_maskz_set1_epi64(__mmask8(a), 0xffffffffffffffff);
}

static SIMD_INLINE Vec<Float, 64> int2bits(const uint64_t a, OutputType<Float>,
                                           Integer<64>)
{
  return reinterpret(int2bits(a, OutputType<Int>(), Integer<64>()),
                     OutputType<Float>());
}

static SIMD_INLINE Vec<Double, 64> int2bits(const uint64_t a,
                                            OutputType<Double>, Integer<64>)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_set1_epi64(__mmask8(a), 0xffffffffffffffff));
}

// ---------------------------------------------------------------------------
// iota
// ---------------------------------------------------------------------------

// 30. Jan 23 (Jonas Keller): added iota

static SIMD_INLINE Vec<Byte, 64> iota(OutputType<Byte>, Integer<64>)
{
  return _mm512_set_epi8(63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50,
                         49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36,
                         35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22,
                         21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,
                         7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<SignedByte, 64> iota(OutputType<SignedByte>, Integer<64>)
{
  return _mm512_set_epi8(63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50,
                         49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36,
                         35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22,
                         21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8,
                         7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Short, 64> iota(OutputType<Short>, Integer<64>)
{
  return _mm512_set_epi16(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19,
                          18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4,
                          3, 2, 1, 0);
}

static SIMD_INLINE Vec<Word, 64> iota(OutputType<Word>, Integer<64>)
{
  return _mm512_set_epi16(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19,
                          18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4,
                          3, 2, 1, 0);
}

static SIMD_INLINE Vec<Int, 64> iota(OutputType<Int>, Integer<64>)
{
  return _mm512_set_epi32(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Long, 64> iota(OutputType<Long>, Integer<64>)
{
  return _mm512_set_epi64(7, 6, 5, 4, 3, 2, 1, 0);
}

static SIMD_INLINE Vec<Float, 64> iota(OutputType<Float>, Integer<64>)
{
  return _mm512_set_ps(15.0f, 14.0f, 13.0f, 12.0f, 11.0f, 10.0f, 9.0f, 8.0f,
                       7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);
}

static SIMD_INLINE Vec<Double, 64> iota(OutputType<Double>, Integer<64>)
{
  return _mm512_set_pd(7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.0);
}

} // namespace base
} // namespace internal
} // namespace simd

#endif

#endif // SIMD_VEC_BASE_IMPL_INTEL_64_H_

// ===========================================================================
//
// SIMDVecBaseImplNEON16.H --
// encapsulation for ARM NEON vector extension
// inspired by Agner Fog's C++ Vector Class Library
// http://www.agner.org/optimize/#vectorclass
// (VCL License: GNU General Public License Version 3,
//  http://www.gnu.org/licenses/gpl-3.0.en.html)
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace

// NOTES:
//
// echo | gcc -E -dM -mcpu=cortex-a9 -mfpu=neon - | more
// echo | arm-linux-gnueabihf-gcc -E -dM -mcpu=cortex-a15 -mfpu=neon - | more
//
// -mfpu=neon
// -mfpu=neon-fp16
//
// GCC 4.9:
// GCC now supports Cortex-A12 and the Cortex-R7 through the
// -mcpu=cortex-a12 and -mcpu=cortex-r7 options.
//
// GCC now has tuning for the Cortex-A57 and Cortex-A53 through the
// -mcpu=cortex-a57 and -mcpu=cortex-a53 options.
//
// Initial big.LITTLE tuning support for the combination of Cortex-A57
// and Cortex-A53 was added through the -mcpu=cortex-a57.cortex-a53
// option. Similar support was added for the combination of Cortex-A15
// and Cortex-A7 through the -mcpu=cortex-a15.cortex-a7 option.

#ifndef SIMD_VEC_BASE_IMPL_NEON_16_H_
#define SIMD_VEC_BASE_IMPL_NEON_16_H_

// ===========================================================================
//
// SIMDIntrinsNEON.H --
// includes include files for vector intrinsics on ARM CPUs
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SIMD_INTRINS_NEON_H_
#define SIMD_INTRINS_NEON_H_

#ifdef SIMDVEC_NEON_ENABLE
#include <arm_neon.h>

// -------------------------------------------------------------------------
// single element array type (for generalizations)
// -------------------------------------------------------------------------

// defined as the other array types in arm_neon.h
// but with type conversion constructor and operator=
#define SIMDVEC_NEON_64X1(NEON_T)                                              \
  struct NEON_T##x1_t                                                          \
  {                                                                            \
    NEON_T##_t val[1];                                                         \
    NEON_T##x1_t() {}                                                          \
    NEON_T##x1_t(const NEON_T##_t &x)                                          \
    {                                                                          \
      val[0] = x;                                                              \
    }                                                                          \
    NEON_T##x1_t &operator=(const NEON_T##_t &x)                               \
    {                                                                          \
      val[0] = x;                                                              \
      return *this;                                                            \
    }                                                                          \
  };

SIMDVEC_NEON_64X1(uint8x8)
SIMDVEC_NEON_64X1(int8x8)
SIMDVEC_NEON_64X1(uint16x4)
SIMDVEC_NEON_64X1(int16x4)
SIMDVEC_NEON_64X1(int32x2)
SIMDVEC_NEON_64X1(float32x2)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_64X1(int64x1)
SIMDVEC_NEON_64X1(float64x1)
#endif

#undef SIMDVEC_NEON_64X1

// -------------------------------------------------------------------------
// vreinterpret[q] with same input and output type (not avail. as intrinsic)
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_VREINTERPRET_SAME(T, NEON_SUF)                            \
  static SIMD_INLINE T vreinterpretq_##NEON_SUF##_##NEON_SUF(T a)              \
  {                                                                            \
    return a;                                                                  \
  }

SIMDVEC_NEON_VREINTERPRET_SAME(uint8x16_t, u8)
SIMDVEC_NEON_VREINTERPRET_SAME(int8x16_t, s8)
SIMDVEC_NEON_VREINTERPRET_SAME(uint16x8_t, u16)
SIMDVEC_NEON_VREINTERPRET_SAME(int16x8_t, s16)
SIMDVEC_NEON_VREINTERPRET_SAME(uint32x4_t, u32)
SIMDVEC_NEON_VREINTERPRET_SAME(int32x4_t, s32)
SIMDVEC_NEON_VREINTERPRET_SAME(float32x4_t, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_VREINTERPRET_SAME(uint64x2_t, u64)
SIMDVEC_NEON_VREINTERPRET_SAME(int64x2_t, s64)
SIMDVEC_NEON_VREINTERPRET_SAME(float64x2_t, f64)
#endif

#endif

#endif // SIMD_INTRINS_NEON_H_

#include <algorithm>
#include <cstddef>
#include <cstdint>
#include <type_traits>

#if defined(SIMDVEC_NEON_ENABLE) && defined(_SIMD_VEC_16_AVAIL_) &&            \
  !defined(SIMDVEC_SANDBOX)

namespace simd {
namespace internal {
namespace base {
// =========================================================================
// type templates
// =========================================================================

// -------------------------------------------------------------------------
// default vector type collection
// -------------------------------------------------------------------------

template <typename T>
struct _NEONRegType;
// clang-format off
template <> struct _NEONRegType<Byte> { using Type = uint8x16_t; };
template <> struct _NEONRegType<SignedByte> { using Type = int8x16_t; };
template <> struct _NEONRegType<Word> { using Type = uint16x8_t; };
template <> struct _NEONRegType<Short> { using Type = int16x8_t; };
template <> struct _NEONRegType<Int> { using Type = int32x4_t; };
template <> struct _NEONRegType<Float> { using Type = float32x4_t; };
#ifdef SIMD_64BIT_TYPES
template <> struct _NEONRegType<Long> { using Type = int64x2_t; };
template <> struct _NEONRegType<Double> { using Type = float64x2_t; };
#endif
// clang-format on

template <typename T>
using NEONRegType = typename _NEONRegType<T>::Type;

// -------------------------------------------------------------------------
// 64bit array type collection
// -------------------------------------------------------------------------

template <size_t N, typename T>
struct SIMDVecNeonArray64;

#define SIMDVEC_NEON_ARRAY64(NUM, T, NEON_T)                                   \
  template <>                                                                  \
  struct SIMDVecNeonArray64<NUM, T>                                            \
  {                                                                            \
    using Type    = NEON_T##x##NUM##_t;                                        \
    using ValType = NEON_T##_t;                                                \
  };

#define SIMDVEC_NEON_ARRAY64_ALLNUM(T, NEON_T)                                 \
  SIMDVEC_NEON_ARRAY64(1, T, NEON_T)                                           \
  SIMDVEC_NEON_ARRAY64(2, T, NEON_T)                                           \
  SIMDVEC_NEON_ARRAY64(3, T, NEON_T)                                           \
  SIMDVEC_NEON_ARRAY64(4, T, NEON_T)

SIMDVEC_NEON_ARRAY64_ALLNUM(Byte, uint8x8)
SIMDVEC_NEON_ARRAY64_ALLNUM(SignedByte, int8x8)
SIMDVEC_NEON_ARRAY64_ALLNUM(Word, uint16x4)
SIMDVEC_NEON_ARRAY64_ALLNUM(Short, int16x4)
SIMDVEC_NEON_ARRAY64_ALLNUM(Int, int32x2)
SIMDVEC_NEON_ARRAY64_ALLNUM(Float, float32x2)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_ARRAY64_ALLNUM(Double, float64x1)
#endif

#undef SIMDVEC_NEON_ARRAY64
#undef SIMDVEC_NEON_ARRAY64_ALLNUM

} // namespace base
} // namespace internal

// =========================================================================
// Vec instantiation for NEON
// =========================================================================

template <typename T>
class Vec<T, 16>
{
  using RegType = internal::base::NEONRegType<T>;
  RegType reg   = {};

public:
  using Type                       = T;
  static constexpr size_t elements = 16 / sizeof(T);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = 16;

  Vec() = default;
  Vec(const RegType &x) { reg = x; }
  Vec &operator=(const RegType &x)
  {
    reg = x;
    return *this;
  }
  operator RegType() const { return reg; }
  // 29. Nov 22 (Jonas Keller):
  // defined operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size) { return simd_aligned_malloc(bytes, size); }
  void operator delete[](void *p) { simd_aligned_free(p); }
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<T, bytes>, bytes>;
};

namespace internal {
namespace base {

// =========================================================================
// macros for common functions
// =========================================================================

// -------------------------------------------------------------------------
// binary functions (same input and output type)
// -------------------------------------------------------------------------

// wrapper for arbitrary binary function
#define SIMDVEC_NEON_BINARY(FCT, TYPE, NEON_FCT, NEON_SUF)                     \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       const Vec<TYPE, 16> &b)                 \
  {                                                                            \
    return NEON_FCT##_##NEON_SUF(a, b);                                        \
  }

#ifdef SIMD_64BIT_TYPES
#define SIMDVEC_NEON_BINARY_ALLINT(FCT, NEON_FCT)                              \
  SIMDVEC_NEON_BINARY(FCT, Byte, NEON_FCT, u8)                                 \
  SIMDVEC_NEON_BINARY(FCT, SignedByte, NEON_FCT, s8)                           \
  SIMDVEC_NEON_BINARY(FCT, Word, NEON_FCT, u16)                                \
  SIMDVEC_NEON_BINARY(FCT, Short, NEON_FCT, s16)                               \
  SIMDVEC_NEON_BINARY(FCT, Int, NEON_FCT, s32)                                 \
  SIMDVEC_NEON_BINARY(FCT, Long, NEON_FCT, s64)
#else
#define SIMDVEC_NEON_BINARY_ALLINT(FCT, NEON_FCT)                              \
  SIMDVEC_NEON_BINARY(FCT, Byte, NEON_FCT, u8)                                 \
  SIMDVEC_NEON_BINARY(FCT, SignedByte, NEON_FCT, s8)                           \
  SIMDVEC_NEON_BINARY(FCT, Word, NEON_FCT, u16)                                \
  SIMDVEC_NEON_BINARY(FCT, Short, NEON_FCT, s16)                               \
  SIMDVEC_NEON_BINARY(FCT, Int, NEON_FCT, s32)
#endif

#ifdef SIMD_64BIT_TYPES
#define SIMDVEC_NEON_BINARY_ALLFLOAT(FCT, NEON_FCT)                            \
  SIMDVEC_NEON_BINARY(FCT, Float, NEON_FCT, f32)                               \
  SIMDVEC_NEON_BINARY(FCT, Double, NEON_FCT, f64)
#else
#define SIMDVEC_NEON_BINARY_ALLFLOAT(FCT, NEON_FCT)                            \
  SIMDVEC_NEON_BINARY(FCT, Float, NEON_FCT, f32)
#endif

#define SIMDVEC_NEON_BINARY_ALL(FCT, NEON_FCT)                                 \
  SIMDVEC_NEON_BINARY_ALLINT(FCT, NEON_FCT)                                    \
  SIMDVEC_NEON_BINARY_ALLFLOAT(FCT, NEON_FCT)

// -------------------------------------------------------------------------
// unary functions
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_UNARY(FCT, TYPE, NEON_FCT, NEON_SUF)                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a)                 \
  {                                                                            \
    return NEON_FCT##_##NEON_SUF(a);                                           \
  }

// #########################################################################
// #########################################################################
// #########################################################################

// =========================================================================
// Vec function instantiations or overloading for NEON
// =========================================================================

// -------------------------------------------------------------------------
// reinterpretation casts
// -------------------------------------------------------------------------

// wrapper for vreinterpretq
#define SIMDVEC_NEON_REINTERP(TDST, NEON_TDST, TSRC, NEON_TSRC)                \
  static SIMD_INLINE Vec<TDST, 16> reinterpret(const Vec<TSRC, 16> &vec,       \
                                               OutputType<TDST>)               \
  {                                                                            \
    return vreinterpretq_##NEON_TDST##_##NEON_TSRC(vec);                       \
  }

// wrapper for all dst types and same source type
#ifdef SIMD_64BIT_TYPES
#define SIMDVEC_NEON_REINTERP_ALLDST(TSRC, NEON_TSRC)                          \
  SIMDVEC_NEON_REINTERP(Byte, u8, TSRC, NEON_TSRC)                             \
  SIMDVEC_NEON_REINTERP(SignedByte, s8, TSRC, NEON_TSRC)                       \
  SIMDVEC_NEON_REINTERP(Word, u16, TSRC, NEON_TSRC)                            \
  SIMDVEC_NEON_REINTERP(Short, s16, TSRC, NEON_TSRC)                           \
  SIMDVEC_NEON_REINTERP(Int, s32, TSRC, NEON_TSRC)                             \
  SIMDVEC_NEON_REINTERP(Long, s64, TSRC, NEON_TSRC)                            \
  SIMDVEC_NEON_REINTERP(Float, f32, TSRC, NEON_TSRC)                           \
  SIMDVEC_NEON_REINTERP(Double, f64, TSRC, NEON_TSRC)
#else
#define SIMDVEC_NEON_REINTERP_ALLDST(TSRC, NEON_TSRC)                          \
  SIMDVEC_NEON_REINTERP(Byte, u8, TSRC, NEON_TSRC)                             \
  SIMDVEC_NEON_REINTERP(SignedByte, s8, TSRC, NEON_TSRC)                       \
  SIMDVEC_NEON_REINTERP(Word, u16, TSRC, NEON_TSRC)                            \
  SIMDVEC_NEON_REINTERP(Short, s16, TSRC, NEON_TSRC)                           \
  SIMDVEC_NEON_REINTERP(Int, s32, TSRC, NEON_TSRC)                             \
  SIMDVEC_NEON_REINTERP(Float, f32, TSRC, NEON_TSRC)
#endif

// wrapper for all dst and src types
SIMDVEC_NEON_REINTERP_ALLDST(Byte, u8)
SIMDVEC_NEON_REINTERP_ALLDST(SignedByte, s8)
SIMDVEC_NEON_REINTERP_ALLDST(Word, u16)
SIMDVEC_NEON_REINTERP_ALLDST(Short, s16)
SIMDVEC_NEON_REINTERP_ALLDST(Int, s32)
SIMDVEC_NEON_REINTERP_ALLDST(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_REINTERP_ALLDST(Long, s64)
SIMDVEC_NEON_REINTERP_ALLDST(Double, f64)
#endif

#undef SIMDVEC_NEON_REINTERP_ALLDST
#undef SIMDVEC_NEON_REINTERP

// -------------------------------------------------------------------------
// convert (without changes in the number of of elements)
// -------------------------------------------------------------------------

// conversion seems to be saturated in all cases (specified by the
// rounding mode):
// http://stackoverflow.com/questions/24546927/
//  behavior-of-arm-neon-float-integer-conversion-with-overflow

// saturated
// TODO: rounding in cvts (float->int)? +0.5?
// TODO: (NOT the same behavior as in SIMDVecBaseImplIntel16.H
// TODO:  float->int always uses round towards zero = trunc?)
// TODO: cvts: should we saturate in the same way as for Intel?
// TODO: (Intel saturates to max. float which is convertible to int,
// TODO:  NEON saturates to 0x7fffffff)
static SIMD_INLINE Vec<Int, 16> cvts(const Vec<Float, 16> &a, OutputType<Int>)
{
  return vcvtq_s32_f32(a);
}

// saturation is not necessary in this case
static SIMD_INLINE Vec<Float, 16> cvts(const Vec<Int, 16> &a, OutputType<Float>)
{
  return vcvtq_f32_s32(a);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> cvts(const Vec<Double, 16> &a,
                                      OutputType<Long>)
{
  return vcvtq_s64_f64(a);
}

static SIMD_INLINE Vec<Double, 16> cvts(const Vec<Long, 16> &a,
                                        OutputType<Double>)
{
  return vcvtq_f64_s64(a);
}
#endif

// -------------------------------------------------------------------------
// setzero
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_SETZERO(TYPE, NEON_SUF)                                   \
  static SIMD_INLINE Vec<TYPE, 16> setzero(OutputType<TYPE>, Integer<16>)      \
  {                                                                            \
    return vmovq_n##_##NEON_SUF(TYPE(0));                                      \
  }

SIMDVEC_NEON_SETZERO(Byte, u8)
SIMDVEC_NEON_SETZERO(SignedByte, s8)
SIMDVEC_NEON_SETZERO(Word, u16)
SIMDVEC_NEON_SETZERO(Short, s16)
SIMDVEC_NEON_SETZERO(Int, s32)
SIMDVEC_NEON_SETZERO(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_SETZERO(Long, s64)
SIMDVEC_NEON_SETZERO(Double, f64)
#endif

#undef SIMDVEC_NEON_SETZERO

// -------------------------------------------------------------------------
// set1
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_SET1(TYPE, NEON_SUF)                                      \
  static SIMD_INLINE Vec<TYPE, 16> set1(TYPE a, Integer<16>)                   \
  {                                                                            \
    return vdupq_n##_##NEON_SUF(a);                                            \
  }

SIMDVEC_NEON_SET1(Byte, u8)
SIMDVEC_NEON_SET1(SignedByte, s8)
SIMDVEC_NEON_SET1(Word, u16)
SIMDVEC_NEON_SET1(Short, s16)
SIMDVEC_NEON_SET1(Int, s32)
SIMDVEC_NEON_SET1(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_SET1(Long, s64)
SIMDVEC_NEON_SET1(Double, f64)
#endif

#undef SIMDVEC_NEON_SET1

// -------------------------------------------------------------------------
// load
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_LOAD(TYPE, NEON_SUF)                                      \
  static SIMD_INLINE Vec<TYPE, 16> load(const TYPE *const p, Integer<16>)      \
  {                                                                            \
    return vld1q##_##NEON_SUF(p);                                              \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 16> loadu(const TYPE *const p, Integer<16>)     \
  {                                                                            \
    return vld1q##_##NEON_SUF(p);                                              \
  }

SIMDVEC_NEON_LOAD(Byte, u8)
SIMDVEC_NEON_LOAD(SignedByte, s8)
SIMDVEC_NEON_LOAD(Word, u16)
SIMDVEC_NEON_LOAD(Short, s16)
SIMDVEC_NEON_LOAD(Int, s32)
SIMDVEC_NEON_LOAD(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_LOAD(Long, s64)
SIMDVEC_NEON_LOAD(Double, f64)
#endif

#undef SIMDVEC_NEON_LOAD

// -------------------------------------------------------------------------
// store
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_STORE(TYPE, NEON_SUF)                                     \
  static SIMD_INLINE void store(TYPE *const p, const Vec<TYPE, 16> &a)         \
  {                                                                            \
    return vst1q##_##NEON_SUF(p, a);                                           \
  }                                                                            \
  static SIMD_INLINE void storeu(TYPE *const p, const Vec<TYPE, 16> &a)        \
  {                                                                            \
    return vst1q##_##NEON_SUF(p, a);                                           \
  }                                                                            \
  static SIMD_INLINE void stream_store(TYPE *const p, const Vec<TYPE, 16> &a)  \
  {                                                                            \
    return vst1q##_##NEON_SUF(p, a);                                           \
  }

SIMDVEC_NEON_STORE(Byte, u8)
SIMDVEC_NEON_STORE(SignedByte, s8)
SIMDVEC_NEON_STORE(Word, u16)
SIMDVEC_NEON_STORE(Short, s16)
SIMDVEC_NEON_STORE(Int, s32)
SIMDVEC_NEON_STORE(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_STORE(Long, s64)
SIMDVEC_NEON_STORE(Double, f64)
#endif

#undef SIMDVEC_NEON_STORE

// -------------------------------------------------------------------------
// fences
// -------------------------------------------------------------------------

// http://infocenter.arm.com/help/
//   index.jsp?topic=/com.arm.doc.faqs/ka14552.html
// TODO: is this portable to clang?

// NOTE: implemented as full barrier
static SIMD_INLINE void lfence()
{
  SIMD_FULL_MEMBARRIER;
}

// NOTE: implemented as full barrier
static SIMD_INLINE void sfence()
{
  SIMD_FULL_MEMBARRIER;
}

// NOTE: implemented as full barrier
static SIMD_INLINE void mfence()
{
  SIMD_FULL_MEMBARRIER;
}

// -------------------------------------------------------------------------
// extract: with template parameter for immediate argument
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_EXTRACT(TYPE, NEON_SUF)                                   \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE TYPE extract(const Vec<TYPE, 16> &a)                      \
  {                                                                            \
    SIMD_IF_CONSTEXPR (COUNT < Vec<TYPE, 16>::elements) {                      \
      return vgetq_lane##_##NEON_SUF(a, COUNT);                                \
    } else {                                                                   \
      return TYPE(0);                                                          \
    }                                                                          \
  }

SIMDVEC_NEON_EXTRACT(Byte, u8)
SIMDVEC_NEON_EXTRACT(SignedByte, s8)
SIMDVEC_NEON_EXTRACT(Word, u16)
SIMDVEC_NEON_EXTRACT(Short, s16)
SIMDVEC_NEON_EXTRACT(Int, s32)
SIMDVEC_NEON_EXTRACT(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_EXTRACT(Long, s64)
SIMDVEC_NEON_EXTRACT(Double, f64)
#endif

#undef SIMDVEC_NEON_EXTRACT

// -------------------------------------------------------------------------
// add
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALL(add, vaddq)

// -------------------------------------------------------------------------
// adds
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALLINT(adds, vqaddq)
// float NOT saturated
SIMDVEC_NEON_BINARY(adds, Float, vaddq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_BINARY(adds, Double, vaddq, f64)
#endif

// -------------------------------------------------------------------------
// sub
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALL(sub, vsubq)

// -------------------------------------------------------------------------
// subs
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALLINT(subs, vqsubq)
// float NOT saturated
SIMDVEC_NEON_BINARY(subs, Float, vsubq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_BINARY(subs, Double, vsubq, f64)
#endif

// -------------------------------------------------------------------------
// neg (negate = two's complement or unary minus), only signed types
// -------------------------------------------------------------------------

SIMDVEC_NEON_UNARY(neg, SignedByte, vnegq, s8)
SIMDVEC_NEON_UNARY(neg, Short, vnegq, s16)
SIMDVEC_NEON_UNARY(neg, Int, vnegq, s32)
SIMDVEC_NEON_UNARY(neg, Float, vnegq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_UNARY(neg, Long, vnegq, s64)
SIMDVEC_NEON_UNARY(neg, Double, vnegq, f64)
#endif

// -------------------------------------------------------------------------
// min
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY(min, Byte, vminq, u8)
SIMDVEC_NEON_BINARY(min, SignedByte, vminq, s8)
SIMDVEC_NEON_BINARY(min, Word, vminq, u16)
SIMDVEC_NEON_BINARY(min, Short, vminq, s16)
SIMDVEC_NEON_BINARY(min, Int, vminq, s32)
SIMDVEC_NEON_BINARY(min, Float, vminq, f32)
#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> min(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // vminq_s64 does not exist
  return vbslq_s64(vcltq_s64(a, b), a, b);
}
SIMDVEC_NEON_BINARY(min, Double, vminq, f64)
#endif

// -------------------------------------------------------------------------
// max
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY(max, Byte, vmaxq, u8)
SIMDVEC_NEON_BINARY(max, SignedByte, vmaxq, s8)
SIMDVEC_NEON_BINARY(max, Word, vmaxq, u16)
SIMDVEC_NEON_BINARY(max, Short, vmaxq, s16)
SIMDVEC_NEON_BINARY(max, Int, vmaxq, s32)
SIMDVEC_NEON_BINARY(max, Float, vmaxq, f32)
#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> max(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // vmaxq_s64 does not exist
  return vbslq_s64(vcgtq_s64(a, b), a, b);
}
SIMDVEC_NEON_BINARY(max, Double, vmaxq, f64)
#endif

// -------------------------------------------------------------------------
// mul, div
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY(mul, Float, vmulq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_BINARY(mul, Double, vmulq, f64)
#endif

const auto DIV_NEWTON_STEPS = 2;

// adapted from Jens Froemmer's Ba thesis (2014)
static SIMD_INLINE Vec<Float, 16> div(const Vec<Float, 16> &num,
                                      const Vec<Float, 16> &denom)
{
  // get estimate of reciprocal of denom
  float32x4_t reciprocal = vrecpeq_f32(denom);
  // refine estimate using Newton-Raphson steps
  for (size_t i = 0; i < DIV_NEWTON_STEPS; i++)
    reciprocal = vmulq_f32(vrecpsq_f32(denom, reciprocal), reciprocal);
  // num * (1.0 / denom)
  return vmulq_f32(num, reciprocal);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> div(const Vec<Double, 16> &num,
                                       const Vec<Double, 16> &denom)
{
  // get estimate of reciprocal of denom
  float64x2_t reciprocal = vrecpeq_f64(denom);
  // refine estimate using Newton-Raphson steps
  for (size_t i = 0; i < DIV_NEWTON_STEPS; i++)
    reciprocal = vmulq_f64(vrecpsq_f64(denom, reciprocal), reciprocal);
  // num * (1.0 / denom)
  return vmulq_f64(num, reciprocal);
}
#endif

// -------------------------------------------------------------------------
// ceil, floor, round, truncate
// -------------------------------------------------------------------------

// 25. Mar 23 (Jonas Keller): added versions for integer types

// versions for integer types do nothing:

template <typename T>
static SIMD_INLINE Vec<T, 16> ceil(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> floor(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> round(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

template <typename T>
static SIMD_INLINE Vec<T, 16> truncate(const Vec<T, 16> &a)
{
  static_assert(std::is_integral<T>::value, "");
  return a;
}

// http://www.rowleydownload.co.uk/arm/documentation/gnu/gcc/
//   ARM-NEON-Intrinsics.html
// vrnd, only some architectures, see arm_neon.h

#if __ARM_ARCH >= 8

// 10. Apr 19 (rm): BINARY->UNARY, qp -> pq etc., still not tested
SIMDVEC_NEON_UNARY(ceil, Float, vrndpq, f32)
SIMDVEC_NEON_UNARY(floor, Float, vrndmq, f32)
SIMDVEC_NEON_UNARY(round, Float, vrndnq, f32)
SIMDVEC_NEON_UNARY(truncate, Float, vrndq, f32)

#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_UNARY(ceil, Double, vrndpq, f64)
SIMDVEC_NEON_UNARY(floor, Double, vrndmq, f64)
SIMDVEC_NEON_UNARY(round, Double, vrndnq, f64)
SIMDVEC_NEON_UNARY(truncate, Double, vrndq, f64)
#endif

#else

static SIMD_INLINE Vec<Float, 16> truncate(const Vec<Float, 16> &a)
{
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  float32x4_t limit = vmovq_n_f32(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  uint32x4_t noRndReq = vcgeq_f32(vabsq_f32(a), limit);
  // truncated result (for |a| < limit)
  float32x4_t aTrunc = vcvtq_f32_s32(vcvtq_s32_f32(a));
  // select result
  return vbslq_f32(noRndReq, a, aTrunc);
}

// https://en.wikipedia.org/wiki/Floor_and_ceiling_functions
//
// floor, ceil:
//                 floor(x), x >= 0
// truncate(x) = {
//                 ceil(x), x < 0
//
// floor(x) = ceil(x)  - (x in Z ? 0 : 1)
// ceil(x)  = floor(x) + (x in Z ? 0 : 1)

static SIMD_INLINE Vec<Float, 16> floor(const Vec<Float, 16> &a)
{
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  float32x4_t limit = vmovq_n_f32(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  uint32x4_t noRndReq = vcgeq_f32(vabsq_f32(a), limit);
  // bool mask: true if a is negative
  uint32x4_t isNeg =
    vreinterpretq_u32_s32(vshrq_n_s32(vreinterpretq_s32_f32(a), 31));
  // truncated result (for |a| < limit)
  float32x4_t aTrunc = vcvtq_f32_s32(vcvtq_s32_f32(a));
  // check if a is an integer
  uint32x4_t isNotInt = vmvnq_u32(vceqq_f32(a, aTrunc));
  // constant 1.0
  float32x4_t one = vmovq_n_f32(1.0f);
  // mask which is 1.0f for negative non-integer values, 0.0f otherwise
  float32x4_t oneMask = vreinterpretq_f32_u32(
    vandq_u32(vandq_u32(isNeg, isNotInt), vreinterpretq_u32_f32(one)));
  // if negative, trunc computes ceil, to turn it into floor we sub
  // 1 if aTrunc is non-integer
  aTrunc = vsubq_f32(aTrunc, oneMask);
  // select result (a or aTrunc)
  return vbslq_f32(noRndReq, a, aTrunc);
}

static SIMD_INLINE Vec<Float, 16> ceil(const Vec<Float, 16> &a)
{
  // if e>=23, floating point number represents an integer, 2^23 = 8388608
  float32x4_t limit = vmovq_n_f32(8388608.f);
  // bool mask: no rounding required if abs(a) >= limit
  uint32x4_t noRndReq = vcgeq_f32(vabsq_f32(a), limit);
  // bool mask: true if a is negative
  uint32x4_t isNotNeg =
    vmvnq_u32(vreinterpretq_u32_s32(vshrq_n_s32(vreinterpretq_s32_f32(a), 31)));
  // truncated result (for |a| < limit)
  float32x4_t aTrunc = vcvtq_f32_s32(vcvtq_s32_f32(a));
  // check if a is an integer
  uint32x4_t isNotInt = vmvnq_u32(vceqq_f32(a, aTrunc));
  // constant 1.0
  float32x4_t one = vmovq_n_f32(1.0f);
  // mask which is 1.0f for non-negative non-integer values, 0.0f otherwise
  float32x4_t oneMask = vreinterpretq_f32_u32(
    vandq_u32(vandq_u32(isNotNeg, isNotInt), vreinterpretq_u32_f32(one)));
  // if non-negative, trunc computes floor, to turn it into ceil we
  // add 1 if aTrunc is non-integer
  aTrunc = vaddq_f32(aTrunc, oneMask);
  // select result (a or aTrunc)
  return vbslq_f32(noRndReq, a, aTrunc);
}

// NOTE: rounds ties (*.5) towards infinity, different from Intel
static SIMD_INLINE Vec<Float, 16> round(const Vec<Float, 16> &a)
{
  return floor(add(a, set1(Float(0.5f), Integer<16>())));
}

#endif

// -------------------------------------------------------------------------
// elementary mathematical functions
// -------------------------------------------------------------------------

// estimate of a reciprocal
SIMDVEC_NEON_UNARY(rcp, Float, vrecpeq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_UNARY(rcp, Double, vrecpeq, f64)
#endif

// estimate of a reverse square root
SIMDVEC_NEON_UNARY(rsqrt, Float, vrsqrteq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_UNARY(rsqrt, Double, vrsqrteq, f64)
#endif

const auto SQRT_NEWTON_STEPS = 2;

// square root (may not be very efficient)
static SIMD_INLINE Vec<Float, 16> sqrt(const Vec<Float, 16> &a)
{
  // vector with 0s, vector with 1s
  float32x4_t zero = vmovq_n_f32(0.0f), one = vmovq_n_f32(1.0f);
  // check for 0 to avoid div-by-0 (should also cover -0.0f)
  uint32x4_t isZero = vceqq_f32(a, zero);
  // avoid inf in rev. sqrt, replace 0 by 1
  float32x4_t as = vbslq_f32(isZero, one, a);
  // get estimate of reciprocal sqrt
  float32x4_t rSqrt = vrsqrteq_f32(as);
  // refine estimate using Newton-Raphson steps
  for (size_t i = 0; i < SQRT_NEWTON_STEPS; i++)
    rSqrt = vmulq_f32(vrsqrtsq_f32(as, vmulq_f32(rSqrt, rSqrt)), rSqrt);
  // sqrt(a) = a * (1.0 / sqrt(a))
  float32x4_t res = vmulq_f32(as, rSqrt);
  // select result
  return vbslq_f32(isZero, zero, res);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> sqrt(const Vec<Double, 16> &a)
{
  // vector with 0s, vector with 1s
  float64x2_t zero = vmovq_n_f64(0.0), one = vmovq_n_f64(1.0);
  // check for 0 to avoid div-by-0 (should also cover -0.0)
  uint64x2_t isZero = vceqq_f64(a, zero);
  // avoid inf in rev. sqrt, replace 0 by 1
  float64x2_t as = vbslq_f64(isZero, one, a);
  // get estimate of reciprocal sqrt
  float64x2_t rSqrt = vrsqrteq_f64(as);
  // refine estimate using Newton-Raphson steps
  for (size_t i = 0; i < SQRT_NEWTON_STEPS; i++)
    rSqrt = vmulq_f64(vrsqrtsq_f64(as, vmulq_f64(rSqrt, rSqrt)), rSqrt);
  // sqrt(a) = a * (1.0 / sqrt(a))
  float64x2_t res = vmulq_f64(as, rSqrt);
  // select result
  return vbslq_f64(isZero, zero, res);
}
#endif

// -------------------------------------------------------------------------
// abs
// -------------------------------------------------------------------------

// 25. Mar 25 (Jonas Keller): added abs for unsigned integers

// unsigned integers
template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 16> abs(const Vec<T, 16> &a)
{
  return a;
}

SIMDVEC_NEON_UNARY(abs, SignedByte, vabsq, s8)
SIMDVEC_NEON_UNARY(abs, Short, vabsq, s16)
SIMDVEC_NEON_UNARY(abs, Int, vabsq, s32)
SIMDVEC_NEON_UNARY(abs, Float, vabsq, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_UNARY(abs, Long, vabsq, s64)
SIMDVEC_NEON_UNARY(abs, Double, vabsq, f64)
#endif

// -------------------------------------------------------------------------
// unpack
// -------------------------------------------------------------------------

// TODO: unpack is inefficient here since vzipq does both unpacklo and
// TODO: unpackhi but only half of the result is used

// via cast to larger datatype
#define SIMDVEC_NEON_UNPACK(TYPE, BYTES, NEON_SUF, NEON_SUF2)                  \
  template <size_t PART>                                                       \
  static SIMD_INLINE Vec<TYPE, 16> unpack(                                     \
    const Vec<TYPE, 16> &a, const Vec<TYPE, 16> &b, Part<PART>, Bytes<BYTES>)  \
  {                                                                            \
    return vreinterpretq_##NEON_SUF##_##NEON_SUF2(                             \
      (vzipq_##NEON_SUF2(vreinterpretq_##NEON_SUF2##_##NEON_SUF(a),            \
                         vreinterpretq_##NEON_SUF2##_##NEON_SUF(b)))           \
        .val[PART]);                                                           \
  }

// via extraction of low or high halfs
// (NOTE: PART and BYTES are needed in argument list)
#define SIMDVEC_NEON_UNPACK_HALFS(TYPE, BYTES, NEON_SUF)                       \
  static SIMD_INLINE Vec<TYPE, 16> unpack(                                     \
    const Vec<TYPE, 16> &a, const Vec<TYPE, 16> &b, Part<0>, Bytes<BYTES>)     \
  {                                                                            \
    return vcombine_##NEON_SUF(vget_low##_##NEON_SUF(a),                       \
                               vget_low##_##NEON_SUF(b));                      \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 16> unpack(                                     \
    const Vec<TYPE, 16> &a, const Vec<TYPE, 16> &b, Part<1>, Bytes<BYTES>)     \
  {                                                                            \
    return vcombine_##NEON_SUF(vget_high##_##NEON_SUF(a),                      \
                               vget_high##_##NEON_SUF(b));                     \
  }

SIMDVEC_NEON_UNPACK(Byte, 1, u8, u8)
SIMDVEC_NEON_UNPACK(Byte, 2, u8, u16)
SIMDVEC_NEON_UNPACK(Byte, 4, u8, u32)
SIMDVEC_NEON_UNPACK_HALFS(Byte, 8, u8)
SIMDVEC_NEON_UNPACK(SignedByte, 1, s8, s8)
SIMDVEC_NEON_UNPACK(SignedByte, 2, s8, s16)
SIMDVEC_NEON_UNPACK(SignedByte, 4, s8, s32)
SIMDVEC_NEON_UNPACK_HALFS(SignedByte, 8, s8)
SIMDVEC_NEON_UNPACK(Word, 2, u16, u16)
SIMDVEC_NEON_UNPACK(Word, 4, u16, u32)
SIMDVEC_NEON_UNPACK_HALFS(Word, 8, u16)
SIMDVEC_NEON_UNPACK(Short, 2, s16, s16)
SIMDVEC_NEON_UNPACK(Short, 4, s16, s32)
SIMDVEC_NEON_UNPACK_HALFS(Short, 8, s16)
SIMDVEC_NEON_UNPACK(Int, 4, s32, s32)
SIMDVEC_NEON_UNPACK_HALFS(Int, 8, s32)
SIMDVEC_NEON_UNPACK(Float, 4, f32, f32)
SIMDVEC_NEON_UNPACK_HALFS(Float, 8, f32)

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> unpack(const Vec<Long, 16> &a,
                                        const Vec<Long, 16> &b, Part<0>,
                                        Bytes<8>)
{
  return vcombine_s64(vget_low_s64(a), vget_low_s64(b));
}
static SIMD_INLINE Vec<Long, 16> unpack(const Vec<Long, 16> &a,
                                        const Vec<Long, 16> &b, Part<1>,
                                        Bytes<8>)
{
  return vcombine_s64(vget_high_s64(a), vget_high_s64(b));
}
static SIMD_INLINE Vec<Double, 16> unpack(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b, Part<0>,
                                          Bytes<8>)
{
  return vcombine_f64(vget_low_f64(a), vget_low_f64(b));
}
static SIMD_INLINE Vec<Double, 16> unpack(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b, Part<1>,
                                          Bytes<8>)
{
  return vcombine_f64(vget_high_f64(a), vget_high_f64(b));
}
#endif

#undef SIMDVEC_NEON_UNPACK
#undef SIMDVEC_NEON_UNPACK_HALFS

// ---------------------------------------------------------------------------
// unpack16
// ---------------------------------------------------------------------------

// 16-byte-lane oriented unpack: for 16 bytes same as generalized unpack
// unpack blocks of NUM_ELEMS elements of type T
// PART=0: low half of input vectors,
// PART=1: high half of input vectors
template <size_t PART, size_t BYTES, typename T>
static SIMD_INLINE Vec<T, 16> unpack16(const Vec<T, 16> &a, const Vec<T, 16> &b,
                                       Part<PART>, Bytes<BYTES>)
{
  return unpack(a, b, Part<PART>(), Bytes<BYTES>());
}

// ---------------------------------------------------------------------------
// extract 128-bit lane as Vec<T, 16>, does nothing for 16 bytes
// ---------------------------------------------------------------------------

template <size_t LANE_INDEX, typename T>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, 16> &a)
{
  return a;
}

// -------------------------------------------------------------------------
// zip
// -------------------------------------------------------------------------

// a, b passed by-value to avoid problems with identical input/output args.

// via cast to larger datatype
#define SIMDVEC_NEON_ZIP(TYPE, NUM_ELEMS, NEON_SUF, NEON_SUF2, NEONX2_2)       \
  static SIMD_INLINE void zip(const Vec<TYPE, 16> a, const Vec<TYPE, 16> b,    \
                              Vec<TYPE, 16> &c, Vec<TYPE, 16> &d,              \
                              Elements<NUM_ELEMS>)                             \
  {                                                                            \
    NEONX2_2 res;                                                              \
    res = vzipq_##NEON_SUF2(vreinterpretq_##NEON_SUF2##_##NEON_SUF(a),         \
                            vreinterpretq_##NEON_SUF2##_##NEON_SUF(b));        \
    c   = vreinterpretq_##NEON_SUF##_##NEON_SUF2(res.val[0]);                  \
    d   = vreinterpretq_##NEON_SUF##_##NEON_SUF2(res.val[1]);                  \
  }

// via extraction of low or high halfs
// (NOTE: NUM_ELEMS is needed in argument list)
#define SIMDVEC_NEON_ZIP_HALFS(TYPE, NUM_ELEMS, NEON_SUF)                      \
  static SIMD_INLINE void zip(const Vec<TYPE, 16> a, const Vec<TYPE, 16> b,    \
                              Vec<TYPE, 16> &c, Vec<TYPE, 16> &d,              \
                              Elements<NUM_ELEMS>)                             \
  {                                                                            \
    c = vcombine_##NEON_SUF(vget_low_##NEON_SUF(a), vget_low_##NEON_SUF(b));   \
    d = vcombine_##NEON_SUF(vget_high_##NEON_SUF(a), vget_high_##NEON_SUF(b)); \
  }

SIMDVEC_NEON_ZIP(Byte, 1, u8, u8, uint8x16x2_t)
SIMDVEC_NEON_ZIP(Byte, 2, u8, u16, uint16x8x2_t)
SIMDVEC_NEON_ZIP(Byte, 4, u8, u32, uint32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(Byte, 8, u8)
SIMDVEC_NEON_ZIP(SignedByte, 1, s8, s8, int8x16x2_t)
SIMDVEC_NEON_ZIP(SignedByte, 2, s8, s16, int16x8x2_t)
SIMDVEC_NEON_ZIP(SignedByte, 4, s8, s32, int32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(SignedByte, 8, s8)
SIMDVEC_NEON_ZIP(Word, 1, u16, u16, uint16x8x2_t)
SIMDVEC_NEON_ZIP(Word, 2, u16, u32, uint32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(Word, 4, u16)
SIMDVEC_NEON_ZIP(Short, 1, s16, s16, int16x8x2_t)
SIMDVEC_NEON_ZIP(Short, 2, s16, s32, int32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(Short, 4, s16)
SIMDVEC_NEON_ZIP(Int, 1, s32, s32, int32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(Int, 2, s32)
SIMDVEC_NEON_ZIP(Float, 1, f32, f32, float32x4x2_t)
SIMDVEC_NEON_ZIP_HALFS(Float, 2, f32)

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void zip(const Vec<Long, 16> a, const Vec<Long, 16> b,
                            Vec<Long, 16> &c, Vec<Long, 16> &d, Elements<1>)
{
  c = vcombine_s64(vget_low_s64(a), vget_low_s64(b));
  d = vcombine_s64(vget_high_s64(a), vget_high_s64(b));
}
static SIMD_INLINE void zip(const Vec<Double, 16> a, const Vec<Double, 16> b,
                            Vec<Double, 16> &c, Vec<Double, 16> &d, Elements<1>)
{
  c = vcombine_f64(vget_low_f64(a), vget_low_f64(b));
  d = vcombine_f64(vget_high_f64(a), vget_high_f64(b));
}
#endif

template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip(const Vec<T, 16> a, const Vec<T, 16> b,
                            Vec<T, 16> &c, Vec<T, 16> &d)
{
  return zip(a, b, c, d, Elements<NUM_ELEMS>());
}

#undef SIMDVEC_NEON_ZIP
#undef SIMDVEC_NEON_ZIP_HALFS

// ---------------------------------------------------------------------------
// zip16 hub  (16-byte-lane oriented zip): for 16 bytes same as zip
// ---------------------------------------------------------------------------

// a, b are passed by-value to avoid problems with identical input/output args.

template <size_t NUM_ELEMS, typename T>
static SIMD_INLINE void zip16(const Vec<T, 16> a, const Vec<T, 16> b,
                              Vec<T, 16> &l, Vec<T, 16> &h)
{
  zip<NUM_ELEMS, T>(a, b, l, h);
}

// -------------------------------------------------------------------------
// unzip
// -------------------------------------------------------------------------

// -------------------------------------------------------------------------
// unzip
// -------------------------------------------------------------------------

// a, b passed by-value to avoid problems with identical input/output args.

// via cast to larger datatype
#define SIMDVEC_NEON_UNZIP(TYPE, BYTES, NEON_SUF, NEON_SUF2, NEONX2_2)         \
  static SIMD_INLINE void unzip(const Vec<TYPE, 16> a, const Vec<TYPE, 16> b,  \
                                Vec<TYPE, 16> &c, Vec<TYPE, 16> &d,            \
                                Bytes<BYTES>)                                  \
  {                                                                            \
    NEONX2_2 res;                                                              \
    res = vuzpq_##NEON_SUF2(vreinterpretq_##NEON_SUF2##_##NEON_SUF(a),         \
                            vreinterpretq_##NEON_SUF2##_##NEON_SUF(b));        \
    c   = vreinterpretq_##NEON_SUF##_##NEON_SUF2(res.val[0]);                  \
    d   = vreinterpretq_##NEON_SUF##_##NEON_SUF2(res.val[1]);                  \
  }

// via extraction of low or high halfs
// (NOTE: BYTES is needed in argument list)
#define SIMDVEC_NEON_UNZIP_HALFS(TYPE, BYTES, NEON_SUF)                        \
  static SIMD_INLINE void unzip(const Vec<TYPE, 16> a, const Vec<TYPE, 16> b,  \
                                Vec<TYPE, 16> &c, Vec<TYPE, 16> &d,            \
                                Bytes<BYTES>)                                  \
  {                                                                            \
    c = vcombine_##NEON_SUF(vget_low_##NEON_SUF(a), vget_low_##NEON_SUF(b));   \
    d = vcombine_##NEON_SUF(vget_high_##NEON_SUF(a), vget_high_##NEON_SUF(b)); \
  }

SIMDVEC_NEON_UNZIP(Byte, 1, u8, u8, uint8x16x2_t)
SIMDVEC_NEON_UNZIP(Byte, 2, u8, u16, uint16x8x2_t)
SIMDVEC_NEON_UNZIP(Byte, 4, u8, u32, uint32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(Byte, 8, u8)

SIMDVEC_NEON_UNZIP(SignedByte, 1, s8, s8, int8x16x2_t)
SIMDVEC_NEON_UNZIP(SignedByte, 2, s8, s16, int16x8x2_t)
SIMDVEC_NEON_UNZIP(SignedByte, 4, s8, s32, int32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(SignedByte, 8, s8)

SIMDVEC_NEON_UNZIP(Word, 2, u16, u16, uint16x8x2_t)
SIMDVEC_NEON_UNZIP(Word, 4, u16, u32, uint32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(Word, 8, u16)

SIMDVEC_NEON_UNZIP(Short, 2, s16, s16, int16x8x2_t)
SIMDVEC_NEON_UNZIP(Short, 4, s16, s32, int32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(Short, 8, s16)

SIMDVEC_NEON_UNZIP(Int, 4, s32, s32, int32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(Int, 8, s32)

SIMDVEC_NEON_UNZIP(Float, 4, f32, f32, float32x4x2_t)
SIMDVEC_NEON_UNZIP_HALFS(Float, 8, f32)

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void unzip(const Vec<Long, 16> a, const Vec<Long, 16> b,
                              Vec<Long, 16> &c, Vec<Long, 16> &d, Bytes<8>)
{
  c = vcombine_s64(vget_low_s64(a), vget_low_s64(b));
  d = vcombine_s64(vget_high_s64(a), vget_high_s64(b));
}
static SIMD_INLINE void unzip(const Vec<Double, 16> a, const Vec<Double, 16> b,
                              Vec<Double, 16> &c, Vec<Double, 16> &d, Bytes<8>)
{
  c = vcombine_f64(vget_low_f64(a), vget_low_f64(b));
  d = vcombine_f64(vget_high_f64(a), vget_high_f64(b));
}
#endif

#undef SIMDVEC_NEON_UNZIP
#undef SIMDVEC_NEON_UNZIP_HALFS

// ---------------------------------------------------------------------------
// packs
// ---------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE Vec<SignedByte, 16> packs(const Vec<Short, 16> &a,
                                             const Vec<Short, 16> &b,
                                             OutputType<SignedByte>)
{
  return vcombine_s8(vqmovn_s16(a), vqmovn_s16(b));
}

static SIMD_INLINE Vec<Short, 16> packs(const Vec<Int, 16> &a,
                                        const Vec<Int, 16> &b,
                                        OutputType<Short>)
{
  return vcombine_s16(vqmovn_s32(a), vqmovn_s32(b));
}

static SIMD_INLINE Vec<Short, 16> packs(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b,
                                        OutputType<Short>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Short>());
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Int, 16> packs(const Vec<Long, 16> &a,
                                      const Vec<Long, 16> &b, OutputType<Int>)
{
  return vcombine_s32(vqmovn_s64(a), vqmovn_s64(b));
}

static SIMD_INLINE Vec<Int, 16> packs(const Vec<Double, 16> &a,
                                      const Vec<Double, 16> &b, OutputType<Int>)
{
  return vcombine_s32(vqmovn_s64(vcvtq_s64_f64(a)),
                      vqmovn_s64(vcvtq_s64_f64(b)));
}

static SIMD_INLINE Vec<Float, 16> packs(const Vec<Long, 16> &a,
                                        const Vec<Long, 16> &b,
                                        OutputType<Float>)
{
  return vcombine_f32(vcvt_f32_f64(vcvtq_f64_s64(a)),
                      vcvt_f32_f64(vcvtq_f64_s64(b)));
}

static SIMD_INLINE Vec<Float, 16> packs(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b,
                                        OutputType<Float>)
{
  return vcombine_f32(vcvt_f32_f64(a), vcvt_f32_f64(b));
}
#endif

// unsigned -> unsigned

static SIMD_INLINE Vec<Byte, 16> packs(const Vec<Word, 16> &a,
                                       const Vec<Word, 16> &b, OutputType<Byte>)
{
  return vcombine_u8(vqmovn_u16(a), vqmovn_u16(b));
}

// signed -> unsigned

static SIMD_INLINE Vec<Byte, 16> packs(const Vec<Short, 16> &a,
                                       const Vec<Short, 16> &b,
                                       OutputType<Byte>)
{
  return vcombine_u8(vqmovun_s16(a), vqmovun_s16(b));
}

static SIMD_INLINE Vec<Word, 16> packs(const Vec<Int, 16> &a,
                                       const Vec<Int, 16> &b, OutputType<Word>)
{
  return vcombine_u16(vqmovun_s32(a), vqmovun_s32(b));
}

static SIMD_INLINE Vec<Word, 16> packs(const Vec<Float, 16> &a,
                                       const Vec<Float, 16> &b,
                                       OutputType<Word>)
{
  return packs(cvts(a, OutputType<Int>()), cvts(b, OutputType<Int>()),
               OutputType<Word>());
}

// unsigned -> signed

static SIMD_INLINE Vec<SignedByte, 16> packs(const Vec<Word, 16> &a,
                                             const Vec<Word, 16> &b,
                                             OutputType<SignedByte>)
{
  return vcombine_s8(
    vreinterpret_s8_u8(vmin_u8(vqmovn_u16(a), vdup_n_u8(0x7f))),
    vreinterpret_s8_u8(vmin_u8(vqmovn_u16(b), vdup_n_u8(0x7f))));
}

// -------------------------------------------------------------------------
// generalized extend: no stage
// -------------------------------------------------------------------------

// combinations:
// - signed   -> extended signed (sign extension)
// - unsigned -> extended unsigned (zero extension)
// - unsigned -> extended signed (zero extension)
// - signed   -> extended unsigned (saturation and zero extension)

// some types
template <typename T>
static SIMD_INLINE void extend(const Vec<T, 16> &vIn, Vec<T, 16> vOut[1])
{
  vOut[0] = vIn;
}

// same size, different type

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Byte, 16> vOut[1])
{
  vOut[0] = vreinterpretq_u8_s8(vmaxq_s8(vIn, vdupq_n_s8(0)));
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn,
                               Vec<SignedByte, 16> vOut[1])
{
  vOut[0] = vreinterpretq_s8_u8(vminq_u8(vIn, vdupq_n_u8(0x7f)));
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Word, 16> vOut[1])
{
  vOut[0] = vreinterpretq_u16_s16(vmaxq_s16(vIn, vdupq_n_s16(0)));
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Short, 16> vOut[1])
{
  vOut[0] = vreinterpretq_s16_u16(vminq_u16(vIn, vdupq_n_u16(0x7fff)));
}

// -------------------------------------------------------------------------
// generalized extend: single stage
// -------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Short, 16> vOut[2])
{
  vOut[0] = vmovl_s8(vget_low_s8(vIn));
  vOut[1] = vmovl_s8(vget_high_s8(vIn));
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Int, 16> vOut[2])
{
  vOut[0] = vmovl_s16(vget_low_s16(vIn));
  vOut[1] = vmovl_s16(vget_high_s16(vIn));
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn,
                               Vec<Float, 16> vOut[2])
{
  vOut[0] = vcvtq_f32_s32(vmovl_s16(vget_low_s16(vIn)));
  vOut[1] = vcvtq_f32_s32(vmovl_s16(vget_high_s16(vIn)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<Int, 16> &vIn, Vec<Long, 16> vOut[2])
{
  vOut[0] = vmovl_s32(vget_low_s32(vIn));
  vOut[1] = vmovl_s32(vget_high_s32(vIn));
}

static SIMD_INLINE void extend(const Vec<Int, 16> &vIn, Vec<Double, 16> vOut[2])
{
  vOut[0] = vcvtq_f64_s64(vmovl_s32(vget_low_s32(vIn)));
  vOut[1] = vcvtq_f64_s64(vmovl_s32(vget_high_s32(vIn)));
}

static SIMD_INLINE void extend(const Vec<Float, 16> &vIn, Vec<Long, 16> vOut[2])
{
  vOut[0] = vcvtq_s64_f64(vcvt_f64_f32(vget_low_f32(vIn)));
  vOut[1] = vcvtq_s64_f64(vcvt_f64_f32(vget_high_f32(vIn)));
}

static SIMD_INLINE void extend(const Vec<Float, 16> &vIn,
                               Vec<Double, 16> vOut[2])
{
  vOut[0] = vcvt_f64_f32(vget_low_f32(vIn));
  vOut[1] = vcvt_f64_f32(vget_high_f32(vIn));
}
#endif

// unsigned -> unsigned

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Word, 16> vOut[2])
{
  vOut[0] = vmovl_u8(vget_low_u8(vIn));
  vOut[1] = vmovl_u8(vget_high_u8(vIn));
}

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Short, 16> vOut[2])
{
  vOut[0] = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(vIn)));
  vOut[1] = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(vIn)));
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Int, 16> vOut[2])
{
  vOut[0] = vreinterpretq_s32_u32(vmovl_u16(vget_low_u16(vIn)));
  vOut[1] = vreinterpretq_s32_u32(vmovl_u16(vget_high_u16(vIn)));
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Float, 16> vOut[2])
{
  vOut[0] = vcvtq_f32_u32(vmovl_u16(vget_low_u16(vIn)));
  vOut[1] = vcvtq_f32_u32(vmovl_u16(vget_high_u16(vIn)));
}

// signed -> unsigned

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Word, 16> vOut[2])
{
  const auto saturated = vmaxq_s8(vIn, vdupq_n_s8(0));
  vOut[0]              = vmovl_u8(vget_low_u8(vreinterpretq_u8_s8(saturated)));
  vOut[1]              = vmovl_u8(vget_high_u8(vreinterpretq_u8_s8(saturated)));
}

// -------------------------------------------------------------------------
// generalized extend: two stages
// -------------------------------------------------------------------------

// signed -> signed

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Int, 16> vOut[4])
{
  Vec<Short, 16> vShort[2];
  extend(vIn, vShort);
  extend(vShort[0], vOut);
  extend(vShort[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Float, 16> vOut[4])
{
  Vec<Short, 16> vShort[2];
  extend(vIn, vShort);
  extend(vShort[0], vOut);
  extend(vShort[1], vOut + 2);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<Short, 16> &vIn, Vec<Long, 16> vOut[4])
{
  Vec<Int, 16> vInt[2];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<Short, 16> &vIn,
                               Vec<Double, 16> vOut[4])
{
  Vec<Int, 16> vInt[2];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
}
#endif

// unsigned -> signed

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Int, 16> vOut[4])
{
  Vec<Short, 16> vShort[2];
  extend(vIn, vShort);
  extend(vShort[0], vOut);
  extend(vShort[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Float, 16> vOut[4])
{
  Vec<Short, 16> vShort[2];
  extend(vIn, vShort);
  extend(vShort[0], vOut);
  extend(vShort[1], vOut + 2);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<Word, 16> &vIn, Vec<Long, 16> vOut[4])
{
  Vec<Int, 16> vInt[2];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
}

static SIMD_INLINE void extend(const Vec<Word, 16> &vIn,
                               Vec<Double, 16> vOut[4])
{
  Vec<Int, 16> vInt[2];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
}
#endif

// -------------------------------------------------------------------------
// generalized extend: three stages
// -------------------------------------------------------------------------

// signed -> signed

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Long, 16> vOut[8])
{
  Vec<Int, 16> vInt[4];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
  extend(vInt[2], vOut + 4);
  extend(vInt[3], vOut + 6);
}

static SIMD_INLINE void extend(const Vec<SignedByte, 16> &vIn,
                               Vec<Double, 16> vOut[8])
{
  Vec<Int, 16> vInt[4];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
  extend(vInt[2], vOut + 4);
  extend(vInt[3], vOut + 6);
}
#endif

// unsigned -> signed

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn, Vec<Long, 16> vOut[8])
{
  Vec<Int, 16> vInt[4];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
  extend(vInt[2], vOut + 4);
  extend(vInt[3], vOut + 6);
}

static SIMD_INLINE void extend(const Vec<Byte, 16> &vIn,
                               Vec<Double, 16> vOut[8])
{
  Vec<Int, 16> vInt[4];
  extend(vIn, vInt);
  extend(vInt[0], vOut);
  extend(vInt[1], vOut + 2);
  extend(vInt[2], vOut + 4);
  extend(vInt[3], vOut + 6);
}
#endif

// -------------------------------------------------------------------------
// generalized extend: special case int <-> float, long <-> double
// -------------------------------------------------------------------------

static SIMD_INLINE void extend(const Vec<Int, 16> &vIn, Vec<Float, 16> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Float>());
}

static SIMD_INLINE void extend(const Vec<Float, 16> &vIn, Vec<Int, 16> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Int>());
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void extend(const Vec<Long, 16> &vIn,
                               Vec<Double, 16> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Double>());
}

static SIMD_INLINE void extend(const Vec<Double, 16> &vIn,
                               Vec<Long, 16> vOut[1])
{
  vOut[0] = cvts(vIn, OutputType<Long>());
}
#endif

// -------------------------------------------------------------------------
// shift functions
// -------------------------------------------------------------------------

// it was necessary to introduce a special case COUNT == 0, since this
// is not allowed for the shift intrinsics (just returns the
// argument); since the ARM docs aren't clear in this point, we also
// treat the case COUNT == no-of-bits as special case (in two
// versions: one using FCT on sizeof(TYPE)*8 - 1, the other setting result to
// zero)

// is non-zero and in a range
template <bool nonZero, bool inRange>
struct IsNonZeroInRange
{};

// is non-zero and in a given range
template <size_t RANGE, size_t INDEX>
struct IsNonZeroInGivenRange
  : public IsNonZeroInRange<(INDEX != 0), (INDEX < RANGE)>
{};

#define SIMDVEC_NEON_SHIFT(FCT, TYPE, NEON_FCT, NEON_SUF)                      \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<true, true>)           \
  {                                                                            \
    return NEON_FCT##_##NEON_SUF(a, COUNT);                                    \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<false, true>)          \
  {                                                                            \
    return a;                                                                  \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a)                 \
  {                                                                            \
    return FCT<COUNT>(a, IsNonZeroInGivenRange<sizeof(TYPE) * 8, COUNT>());    \
  }

// out-of-range implemented with FCT of sizeof(TYPE)*8 - 1
#define SIMDVEC_NEON_SHIFT_ARITH(FCT, TYPE, NEON_FCT, NEON_SUF)                \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<true, false>)          \
  {                                                                            \
    return NEON_FCT##_##NEON_SUF(a, sizeof(TYPE) * 8 - 1);                     \
  }                                                                            \
  SIMDVEC_NEON_SHIFT(FCT, TYPE, NEON_FCT, NEON_SUF)

// out-of-range implemented with set-to-zero
#define SIMDVEC_NEON_SHIFT_LOGICAL(FCT, TYPE, NEON_FCT, NEON_SUF)              \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &,                  \
                                       IsNonZeroInRange<true, false>)          \
  {                                                                            \
    return vmovq_n_##NEON_SUF(TYPE(0));                                        \
  }                                                                            \
  SIMDVEC_NEON_SHIFT(FCT, TYPE, NEON_FCT, NEON_SUF)

#define SIMDVEC_NEON_SHIFT_REINTER(FCT, TYPE, NFCT, NSUF, NSUF2)               \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<true, true>)           \
  {                                                                            \
    return vreinterpretq_##NSUF##_##NSUF2(                                     \
      NFCT##_##NSUF2(vreinterpretq_##NSUF2##_##NSUF(a), COUNT));               \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<false, true>)          \
  {                                                                            \
    return a;                                                                  \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a)                 \
  {                                                                            \
    return FCT<COUNT>(a, IsNonZeroInGivenRange<sizeof(TYPE) * 8, COUNT>());    \
  }

#define SIMDVEC_NEON_SHIFT_REINTER_ARITH(FCT, TYPE, NFCT, NSUF, NSUF2)         \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &a,                 \
                                       IsNonZeroInRange<true, false>)          \
  {                                                                            \
    return vreinterpretq_##NSUF##_##NSUF2(NFCT##_##NSUF2(                      \
      vreinterpretq_##NSUF2##_##NSUF(a), sizeof(TYPE) * 8 - 1));               \
  }                                                                            \
  SIMDVEC_NEON_SHIFT_REINTER(FCT, TYPE, NFCT, NSUF, NSUF2)

#define SIMDVEC_NEON_SHIFT_REINTER_LOGICAL(FCT, TYPE, NFCT, NSUF, NSUF2)       \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> FCT(const Vec<TYPE, 16> &,                  \
                                       IsNonZeroInRange<true, false>)          \
  {                                                                            \
    return vmovq_n_##NSUF(TYPE(0));                                            \
  }                                                                            \
  SIMDVEC_NEON_SHIFT_REINTER(FCT, TYPE, NFCT, NSUF, NSUF2)

// srai

// requires cast of unsigned types to signed!
// http://stackoverflow.com/questions/18784988/neon-intrinsic-for-arithmetic-shift
// out-of-range case handled with FCT=srai

// 13. Nov 22 (Jonas Keller):
// added missing Byte and SignedByte versions of srai

SIMDVEC_NEON_SHIFT_REINTER_ARITH(srai, Byte, vshrq_n, u8, s8)
SIMDVEC_NEON_SHIFT_ARITH(srai, SignedByte, vshrq_n, s8)
SIMDVEC_NEON_SHIFT_REINTER_ARITH(srai, Word, vshrq_n, u16, s16)
SIMDVEC_NEON_SHIFT_ARITH(srai, Short, vshrq_n, s16)
SIMDVEC_NEON_SHIFT_ARITH(srai, Int, vshrq_n, s32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_SHIFT_ARITH(srai, Long, vshrq_n, s64)
#endif

// srli

// requires cast of signed types to unsigned!
// http://stackoverflow.com/questions/18784988/neon-intrinsic-for-arithmetic-shift
// out-of-range case handled with set-to-zero

SIMDVEC_NEON_SHIFT_LOGICAL(srli, Byte, vshrq_n, u8)
SIMDVEC_NEON_SHIFT_REINTER_LOGICAL(srli, SignedByte, vshrq_n, s8, u8)
SIMDVEC_NEON_SHIFT_LOGICAL(srli, Word, vshrq_n, u16)
SIMDVEC_NEON_SHIFT_REINTER_LOGICAL(srli, Short, vshrq_n, s16, u16)
SIMDVEC_NEON_SHIFT_REINTER_LOGICAL(srli, Int, vshrq_n, s32, u32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_SHIFT_REINTER_LOGICAL(srli, Long, vshrq_n, s64, u64)
#endif

// slli

// out-of-range case handled with set-to-zero
SIMDVEC_NEON_SHIFT_LOGICAL(slli, Byte, vshlq_n, u8)
SIMDVEC_NEON_SHIFT_LOGICAL(slli, SignedByte, vshlq_n, s8)
SIMDVEC_NEON_SHIFT_LOGICAL(slli, Word, vshlq_n, u16)
SIMDVEC_NEON_SHIFT_LOGICAL(slli, Short, vshlq_n, s16)
SIMDVEC_NEON_SHIFT_LOGICAL(slli, Int, vshlq_n, s32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_SHIFT_LOGICAL(slli, Long, vshlq_n, s64)
#endif

#undef SIMDVEC_NEON_SHIFT
#undef SIMDVEC_NEON_SHIFT_ARITH
#undef SIMDVEC_NEON_SHIFT_LOGICAL
#undef SIMDVEC_NEON_SHIFT_REINTER
#undef SIMDVEC_NEON_SHIFT_REINTER_ARITH
#undef SIMDVEC_NEON_SHIFT_REINTER_LOGICAL

// 19. Dec 22 (Jonas Keller): added sra, srl and sll functions

// -------------------------------------------------------------------------
// sra
// -------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> sra(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(8)));
  return vreinterpretq_u8_s8(
    vshlq_s8(vreinterpretq_s8_u8(a), vdupq_n_s8(scount)));
}

static SIMD_INLINE Vec<SignedByte, 16> sra(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(8)));
  return vshlq_s8(a, vdupq_n_s8(scount));
}

static SIMD_INLINE Vec<Word, 16> sra(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(16)));
  return vreinterpretq_u16_s16(
    vshlq_s16(vreinterpretq_s16_u16(a), vdupq_n_s16(scount)));
}

static SIMD_INLINE Vec<Short, 16> sra(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(16)));
  return vshlq_s16(a, vdupq_n_s16(scount));
}

static SIMD_INLINE Vec<Int, 16> sra(const Vec<Int, 16> &a, const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(32)));
  return vshlq_s32(a, vdupq_n_s32(scount));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> sra(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(64)));
  return vshlq_s64(a, vdupq_n_s64(scount));
}
#endif

// -------------------------------------------------------------------------
// srl
// -------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> srl(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(8)));
  return vshlq_u8(a, vdupq_n_s8(scount));
}

static SIMD_INLINE Vec<SignedByte, 16> srl(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(8)));
  return vreinterpretq_s8_u8(
    vshlq_u8(vreinterpretq_u8_s8(a), vdupq_n_s8(scount)));
}

static SIMD_INLINE Vec<Word, 16> srl(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(16)));
  return vshlq_u16(a, vdupq_n_s16(scount));
}

static SIMD_INLINE Vec<Short, 16> srl(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(16)));
  return vreinterpretq_s16_u16(
    vshlq_u16(vreinterpretq_u16_s16(a), vdupq_n_s16(scount)));
}

static SIMD_INLINE Vec<Int, 16> srl(const Vec<Int, 16> &a, const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(32)));
  return vreinterpretq_s32_u32(
    vshlq_u32(vreinterpretq_u32_s32(a), vdupq_n_s32(scount)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> srl(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  int8_t scount = -((int8_t) std::min(count, uint8_t(64)));
  return vreinterpretq_s64_u64(
    vshlq_u64(vreinterpretq_u64_s64(a), vdupq_n_s64(scount)));
}
#endif

// -------------------------------------------------------------------------
// sll
// -------------------------------------------------------------------------

static SIMD_INLINE Vec<Byte, 16> sll(const Vec<Byte, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_u8(a, vdupq_n_s8(std::min(count, uint8_t(8))));
}

static SIMD_INLINE Vec<SignedByte, 16> sll(const Vec<SignedByte, 16> &a,
                                           const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_s8(a, vdupq_n_s8(std::min(count, uint8_t(8))));
}

static SIMD_INLINE Vec<Word, 16> sll(const Vec<Word, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_u16(a, vdupq_n_s16(std::min(count, uint8_t(16))));
}

static SIMD_INLINE Vec<Short, 16> sll(const Vec<Short, 16> &a,
                                      const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_s16(a, vdupq_n_s16(std::min(count, uint8_t(16))));
}

static SIMD_INLINE Vec<Int, 16> sll(const Vec<Int, 16> &a, const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_s32(a, vdupq_n_s32(std::min(count, uint8_t(32))));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> sll(const Vec<Long, 16> &a,
                                     const uint8_t count)
{
  if (count == 0) {
    // TODO: is this necessary? what does vshlq do for count==0?
    return a;
  }
  return vshlq_s64(a, vdupq_n_s64(std::min(count, uint8_t(64))));
}
#endif

// 26. Sep 22 (Jonas Keller):
// added Byte and SignedByte versions of hadd, hadds, hsub and hsubs
// added Word version of hadds and hsubs

// -------------------------------------------------------------------------
// hadd
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_HADD(TYPE, NEON_SUF)                                      \
  static SIMD_INLINE Vec<TYPE, 16> hadd(const Vec<TYPE, 16> &a,                \
                                        const Vec<TYPE, 16> &b)                \
  {                                                                            \
    return vcombine_##NEON_SUF(                                                \
      vpadd_##NEON_SUF(vget_low_##NEON_SUF(a), vget_high_##NEON_SUF(a)),       \
      vpadd_##NEON_SUF(vget_low_##NEON_SUF(b), vget_high_##NEON_SUF(b)));      \
  }

SIMDVEC_NEON_HADD(Byte, u8)
SIMDVEC_NEON_HADD(SignedByte, s8)
SIMDVEC_NEON_HADD(Word, u16)
SIMDVEC_NEON_HADD(Short, s16)
SIMDVEC_NEON_HADD(Int, s32)
SIMDVEC_NEON_HADD(Float, f32)
#ifdef SIMD_64BIT_TYPES
// vpadd_s64 does not exist, because int64x1_t is just a long, so we use the
// regular plus operator for long
static SIMD_INLINE Vec<Long, 16> hadd(const Vec<Long, 16> &a,
                                      const Vec<Long, 16> &b)
{
  return vcombine_s64(vget_low_s64(a) + vget_high_s64(a),
                      vget_low_s64(b) + vget_high_s64(b));
}
// vpadd_f64 does not exist, because float64x1_t is just a double, so we use
// the regular plus operator for double
static SIMD_INLINE Vec<Double, 16> hadd(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return vcombine_f64(vget_low_f64(a) + vget_high_f64(a),
                      vget_low_f64(b) + vget_high_f64(b));
}
#endif

#undef SIMDVEC_NEON_HADD

// -------------------------------------------------------------------------
// hadds
// -------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> hadds(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return adds(x, y);
}

static SIMD_INLINE Vec<Short, 16> hadds(const Vec<Short, 16> &a,
                                        const Vec<Short, 16> &b)
{
  return vcombine_s16(vqmovn_s32(vpaddlq_s16(a)), vqmovn_s32(vpaddlq_s16(b)));
}

static SIMD_INLINE Vec<Int, 16> hadds(const Vec<Int, 16> &a,
                                      const Vec<Int, 16> &b)
{
  return vcombine_s32(vqmovn_s64(vpaddlq_s32(a)), vqmovn_s64(vpaddlq_s32(b)));
}

// Float not saturated
static SIMD_INLINE Vec<Float, 16> hadds(const Vec<Float, 16> &a,
                                        const Vec<Float, 16> &b)
{
  return hadd(a, b);
}

#ifdef SIMD_64BIT_TYPES
// Double not saturated
static SIMD_INLINE Vec<Double, 16> hadds(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return hadd(a, b);
}
#endif

// -------------------------------------------------------------------------
// hsub
// -------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> hsub(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return sub(x, y);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> hsub(const Vec<Double, 16> &a,
                                        const Vec<Double, 16> &b)
{
  return vcombine_f64(vget_low_f64(a) - vget_high_f64(a),
                      vget_low_f64(b) - vget_high_f64(b));
}
#endif

// -------------------------------------------------------------------------
// hsubs
// -------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> hsubs(const Vec<T, 16> &a, const Vec<T, 16> &b)
{
  Vec<T, 16> x, y;
  unzip(a, b, x, y, Bytes<sizeof(T)>());
  return subs(x, y);
}

#ifdef SIMD_64BIT_TYPES
// Double not saturated
static SIMD_INLINE Vec<Double, 16> hsubs(const Vec<Double, 16> &a,
                                         const Vec<Double, 16> &b)
{
  return vcombine_f64(vget_low_f64(a) - vget_high_f64(a),
                      vget_low_f64(b) - vget_high_f64(b));
}
#endif

// -------------------------------------------------------------------------
// alignre (moved above srle, slle)
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_ALIGNRE(TYPE, NEON_SUF)                                   \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> alignre(                                    \
    const Vec<TYPE, 16> &, const Vec<TYPE, 16> &l,                             \
    Range<true, 0, Vec<TYPE, 16>::elements>)                                   \
  {                                                                            \
    return l;                                                                  \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> alignre(                                    \
    const Vec<TYPE, 16> &h, const Vec<TYPE, 16> &l,                            \
    Range<false, 0, Vec<TYPE, 16>::elements>)                                  \
  {                                                                            \
    return vextq_##NEON_SUF(l, h, COUNT);                                      \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> alignre(                                    \
    const Vec<TYPE, 16> &h, const Vec<TYPE, 16> &,                             \
    Range<true, Vec<TYPE, 16>::elements, 2 * Vec<TYPE, 16>::elements>)         \
  {                                                                            \
    return h;                                                                  \
  }                                                                            \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Vec<TYPE, 16> alignre(                                    \
    const Vec<TYPE, 16> &h, const Vec<TYPE, 16> &,                             \
    Range<false, Vec<TYPE, 16>::elements, 2 * Vec<TYPE, 16>::elements>)        \
  {                                                                            \
    return vextq_##NEON_SUF(h, vmovq_n_##NEON_SUF(TYPE(0)),                    \
                            COUNT - Vec<TYPE, 16>::elements);                  \
  }                                                                            \
  template <size_t COUNT, bool AT_LL, size_t LL_INCL, size_t UL_EXCL>          \
  static SIMD_INLINE Vec<TYPE, 16> alignre(const Vec<TYPE, 16> &,              \
                                           const Vec<TYPE, 16> &,              \
                                           Range<AT_LL, LL_INCL, UL_EXCL>)     \
  {                                                                            \
    return vmovq_n_##NEON_SUF(TYPE(0));                                        \
  }

SIMDVEC_NEON_ALIGNRE(Byte, u8)
SIMDVEC_NEON_ALIGNRE(SignedByte, s8)
SIMDVEC_NEON_ALIGNRE(Word, u16)
SIMDVEC_NEON_ALIGNRE(Short, s16)
SIMDVEC_NEON_ALIGNRE(Int, s32)
SIMDVEC_NEON_ALIGNRE(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_ALIGNRE(Long, s64)
SIMDVEC_NEON_ALIGNRE(Double, f64)
#endif

template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> alignre(const Vec<T, 16> &h, const Vec<T, 16> &l)
{
  return alignre<COUNT>(h, l, SizeRange<COUNT, Vec<T, 16>::elements>());
}

#undef SIMDVEC_NEON_ALIGNRE

// -------------------------------------------------------------------------
// element-wise shift right
// -------------------------------------------------------------------------

// all types, done via alignre
template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> srle(const Vec<T, 16> &a)
{
  return alignre<COUNT>(setzero(OutputType<T>(), Integer<16>()), a);
}

// -------------------------------------------------------------------------
// element-wise shift left
// -------------------------------------------------------------------------

// all types, done via alignre

template <size_t COUNT, typename T>
static SIMD_INLINE Vec<T, 16> slle(const Vec<T, 16> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < Vec<T, 16>::elements) {
    return alignre<Vec<T, 16>::elements - COUNT>(
      a, setzero(OutputType<T>(), Integer<16>()));
  } else {
    return setzero(OutputType<T>(), Integer<16>());
  }
}

// -------------------------------------------------------------------------
// swizzle
// -------------------------------------------------------------------------

// swizzle tables

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<2>,
                                          Integer<1>)
{
  const uint8x8_t table[2] SIMD_ATTR_ALIGNED(16) = {
    {0, 2, 4, 6, 8, 10, 12, 14},
    {1, 3, 5, 7, 9, 11, 13, 15},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<3>,
                                          Integer<1>)
{
  const uint8x8_t table[3] SIMD_ATTR_ALIGNED(16) = {
    {0, 3, 6, 9, 12, 15, 18, 21},
    {1, 4, 7, 10, 13, 16, 19, 22},
    {2, 5, 8, 11, 14, 17, 20, 23},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<4>,
                                          Integer<1>)
{
  const uint8x8_t table[4] SIMD_ATTR_ALIGNED(16) = {
    {0, 4, 8, 12, 16, 20, 24, 28},
    {1, 5, 9, 13, 17, 21, 25, 29},
    {2, 6, 10, 14, 18, 22, 26, 30},
    {3, 7, 11, 15, 19, 23, 27, 31},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<2>,
                                          Integer<2>)
{
  const uint8x8_t table[2] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 4, 5, 8, 9, 12, 13},
    {2, 3, 6, 7, 10, 11, 14, 15},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<3>,
                                          Integer<2>)
{
  const uint8x8_t table[3] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 6, 7, 12, 13, 18, 19},
    {2, 3, 8, 9, 14, 15, 20, 21},
    {4, 5, 10, 11, 16, 17, 22, 23},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<4>,
                                          Integer<2>)
{
  const uint8x8_t table[4] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 8, 9, 16, 17, 24, 25},
    {2, 3, 10, 11, 18, 19, 26, 27},
    {4, 5, 12, 13, 20, 21, 28, 29},
    {6, 7, 14, 15, 22, 23, 30, 31},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<2>,
                                          Integer<4>)
{
  const uint8x8_t table[2] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 2, 3, 8, 9, 10, 11},
    {4, 5, 6, 7, 12, 13, 14, 15},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<3>,
                                          Integer<4>)
{
  const uint8x8_t table[3] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 2, 3, 12, 13, 14, 15},
    {4, 5, 6, 7, 16, 17, 18, 19},
    {8, 9, 10, 11, 20, 21, 22, 23},
  };
  return table[index];
}

static SIMD_INLINE uint8x8_t swizzleTable(const size_t index, Integer<4>,
                                          Integer<4>)
{
  const uint8x8_t table[4] SIMD_ATTR_ALIGNED(16) = {
    {0, 1, 2, 3, 16, 17, 18, 19},
    {4, 5, 6, 7, 20, 21, 22, 23},
    {8, 9, 10, 11, 24, 25, 26, 27},
    {12, 13, 14, 15, 28, 29, 30, 31},
  };
  return table[index];
}

template <size_t N, typename T>
static SIMD_INLINE uint8x8_t swizzleTable(const size_t index)
{
  return swizzleTable(index, Integer<N>(), Integer<sizeof(T)>());
}

template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16>[1], Integer<1>)
{
  // v remains unchanged
}

template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16> v[2], Integer<2>)
{
  const Vec<Byte, 16> vByte[2] = {
    reinterpret(v[0], OutputType<Byte>()),
    reinterpret(v[1], OutputType<Byte>()),
  };
  for (size_t i = 0; i < 2; i++) {
    v[i] =
      reinterpret(Vec<Byte, 16>(vcombine_u8(
                    vtbl2_u8({vget_low_u8(vByte[0]), vget_high_u8(vByte[0])},
                             swizzleTable<2, T>(i)),
                    vtbl2_u8({vget_low_u8(vByte[1]), vget_high_u8(vByte[1])},
                             swizzleTable<2, T>(i)))),
                  OutputType<T>());
  }
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void swizzle(Vec<Long, 16> v[2], Integer<2>)
{
  const Vec<Long, 16> tmp[2] = {v[0], v[1]};
  v[0] = vcombine_s64(vget_low_s64(tmp[0]), vget_low_s64(tmp[1]));
  v[1] = vcombine_s64(vget_high_s64(tmp[0]), vget_high_s64(tmp[1]));
}

static SIMD_INLINE void swizzle(Vec<Double, 16> v[2], Integer<2>)
{
  const Vec<Double, 16> tmp[2] = {v[0], v[1]};
  v[0] = vcombine_f64(vget_low_f64(tmp[0]), vget_low_f64(tmp[1]));
  v[1] = vcombine_f64(vget_high_f64(tmp[0]), vget_high_f64(tmp[1]));
}
#endif

template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16> v[3], Integer<3>)
{
  const Vec<Byte, 16> vByte[3] = {
    reinterpret(v[0], OutputType<Byte>()),
    reinterpret(v[1], OutputType<Byte>()),
    reinterpret(v[2], OutputType<Byte>()),
  };
  const uint8x8x3_t vu[2] = {
    {vget_low_u8(vByte[0]), vget_high_u8(vByte[0]), vget_low_u8(vByte[1])},
    {vget_high_u8(vByte[1]), vget_low_u8(vByte[2]), vget_high_u8(vByte[2])},
  };
  for (size_t i = 0; i < 3; i++) {
    v[i] = reinterpret(
      Vec<Byte, 16>(vcombine_u8(vtbl3_u8(vu[0], swizzleTable<3, T>(i)),
                                vtbl3_u8(vu[1], swizzleTable<3, T>(i)))),
      OutputType<T>());
  }
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void swizzle(Vec<Long, 16> v[3], Integer<3>)
{
  const Vec<Long, 16> tmp[3] = {v[0], v[1], v[2]};
  v[0] = vcombine_s64(vget_low_s64(tmp[0]), vget_high_s64(tmp[1]));
  v[1] = vcombine_s64(vget_high_s64(tmp[0]), vget_low_s64(tmp[2]));
  v[2] = vcombine_s64(vget_low_s64(tmp[1]), vget_high_s64(tmp[2]));
}

static SIMD_INLINE void swizzle(Vec<Double, 16> v[3], Integer<3>)
{
  const Vec<Double, 16> tmp[3] = {v[0], v[1], v[2]};
  v[0] = vcombine_f64(vget_low_f64(tmp[0]), vget_high_f64(tmp[1]));
  v[1] = vcombine_f64(vget_high_f64(tmp[0]), vget_low_f64(tmp[2]));
  v[2] = vcombine_f64(vget_low_f64(tmp[1]), vget_high_f64(tmp[2]));
}
#endif

template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16> v[4], Integer<4>)
{
  const Vec<Byte, 16> vByte[4] = {
    reinterpret(v[0], OutputType<Byte>()),
    reinterpret(v[1], OutputType<Byte>()),
    reinterpret(v[2], OutputType<Byte>()),
    reinterpret(v[3], OutputType<Byte>()),
  };
  const uint8x8x4_t vu[2] = {
    {vget_low_u8(vByte[0]), vget_high_u8(vByte[0]), vget_low_u8(vByte[1]),
     vget_high_u8(vByte[1])},
    {vget_low_u8(vByte[2]), vget_high_u8(vByte[2]), vget_low_u8(vByte[3]),
     vget_high_u8(vByte[3])},
  };
  for (size_t i = 0; i < 4; i++) {
    v[i] = reinterpret(
      Vec<Byte, 16>(vcombine_u8(vtbl4_u8(vu[0], swizzleTable<4, T>(i)),
                                vtbl4_u8(vu[1], swizzleTable<4, T>(i)))),
      OutputType<T>());
  }
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void swizzle(Vec<Long, 16> v[4], Integer<4>)
{
  const Vec<Long, 16> tmp[4] = {v[0], v[1], v[2], v[3]};
  v[0] = vcombine_s64(vget_low_s64(tmp[0]), vget_low_s64(tmp[2]));
  v[1] = vcombine_s64(vget_high_s64(tmp[0]), vget_high_s64(tmp[2]));
  v[2] = vcombine_s64(vget_low_s64(tmp[1]), vget_low_s64(tmp[3]));
  v[3] = vcombine_s64(vget_high_s64(tmp[1]), vget_high_s64(tmp[3]));
}

static SIMD_INLINE void swizzle(Vec<Double, 16> v[4], Integer<4>)
{
  const Vec<Double, 16> tmp[4] = {v[0], v[1], v[2], v[3]};
  v[0] = vcombine_f64(vget_low_f64(tmp[0]), vget_low_f64(tmp[2]));
  v[1] = vcombine_f64(vget_high_f64(tmp[0]), vget_high_f64(tmp[2]));
  v[2] = vcombine_f64(vget_low_f64(tmp[1]), vget_low_f64(tmp[3]));
  v[3] = vcombine_f64(vget_high_f64(tmp[1]), vget_high_f64(tmp[3]));
}
#endif

// ---------- n = 5 ----------

// swizzle table

// arrays are padded from 24 to 32 elements to keep alignment
static const uint8_t swizzleMask5Lo[5][32] SIMD_ATTR_ALIGNED(16) = {
  {},
  {0, 5, 10, 15, 1, 6, 11, 16, 2,  7,  12, 17,
   3, 8, 13, 18, 4, 9, 14, 19, 99, 99, 99, 99},
  {0, 1, 10, 11, 2, 3, 12, 13, 4,  5,  14, 15,
   6, 7, 16, 17, 8, 9, 18, 19, 99, 99, 99, 99},
  {},
  {0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11,
   12, 13, 14, 15, 16, 17, 18, 19, 99, 99, 99, 99},
};

// arrays are padded from 24 to 32 elements to keep alignment
static const uint8_t swizzleMask5Hi[5][32] SIMD_ATTR_ALIGNED(16) = {
  {},
  {4, 9,  14, 19, 5, 10, 15, 20, 6,  11, 16, 21,
   7, 12, 17, 22, 8, 13, 18, 23, 99, 99, 99, 99},
  {4,  5,  14, 15, 6,  7,  16, 17, 8,  9,  18, 19,
   10, 11, 20, 21, 12, 13, 22, 23, 99, 99, 99, 99},
  {},
  {4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15,
   16, 17, 18, 19, 20, 21, 22, 23, 99, 99, 99, 99},
};

// n = 5
template <size_t SIZE>
struct SwizzleTable5
{
  // two tables for n=3
  uint8x8x3_t table[2];
  SwizzleTable5()
  {
    for (size_t i = 0; i < 3; i++) {
      // first half (applied to vectors 0,1,2)
      table[0].val[i] = vld1_u8(&swizzleMask5Lo[SIZE][i * 8]);
      // second half (applied to vectors 2,3,4)
      table[1].val[i] = vld1_u8(&swizzleMask5Hi[SIZE][i * 8]);
    }
  }
};

// n = 5
template <typename T>
static SIMD_INLINE void swizzle(Vec<T, 16> v[5], Integer<5>)
{
  //     |  v0l  v0h  |  v1l  v1h  |  v2l  v2h  |  v3l  v3h  |  v4l  v4h  |
  // i=0:
  // k:      0    1       2
  // j:      0    1       2
  //     | vu0.0 v0.1  vu0.2|
  // i=1:
  // k:                   2    3       4
  // j:                   0    1       2
  //                  |vu1.0 vu1.1   vu1.2|
  // i=2:
  // k:                                     5       6    7
  // j:                                     0       1    2
  //                                      |vu2.0  v2.1 vu2.2|
  // i=3:
  // k:                                                  7       8    9
  // j:                                                  0       1    2
  //                                                  |vu3.0  vu3.1 vu3.2 |
  //
  //       n=0:                             n=1:
  //       i=0:         i=1:                i=0:        i=1:
  //       k=0:         k=1:                k=2:        k=3:
  // j=0:
  //     | t.table[0].val[0]|             | t.table[0].val[0]|
  //                  | t.table[1].val[0] |           | t.table[1].val[0] |
  // j=1:
  //     | t.table[0].val[1]|             | t.table[0].val[1]|
  //                  | t.table[1].val[1] |           | t.table[1].val[1] |
  // j=2:
  //     | t.table[0].val[2]|             | t.table[0].val[2]|
  //                  | t.table[1].val[2] |           | t.table[1].val[2] |

  uint8x8x3_t vu[4];
  // input half-vector index starts at k0
  const size_t k0[4] = {0, 2, 5, 7};
  for (size_t i = 0; i < 4; i++) {
    for (size_t j = 0; j < 3; j++) {
      const size_t k         = k0[i] + j;
      const Vec<Byte, 16> vb = reinterpret(v[k >> 1], OutputType<Byte>());
      vu[i].val[j]           = (k & 1) ? vget_high_u8(vb) : vget_low_u8(vb);
    }
  }
  static const SwizzleTable5<sizeof(T)> t;
  uint8x8_t r[2][3][3];
  // n: left/right half of input
  // k: index of vu
  for (size_t n = 0, k = 0; n < 2; n++)
    // i: left/right half of half input
    for (size_t i = 0; i < 2; i++, k++)
      // j: different 3-tables
      for (size_t j = 0; j < 3; j++)
        // apply table
        r[n][i][j] = vtbl3_u8(vu[k], t.table[i].val[j]);
  // zip 4-byte blocks together
  int32x2x2_t z[2][3];
  for (size_t n = 0; n < 2; n++)
    for (size_t j = 0; j < 3; j++)
      z[n][j] = vzip_s32(vreinterpret_s32_u8(r[n][0][j]),
                         vreinterpret_s32_u8(r[n][1][j]));
  // combine left and right halfs
  for (size_t j = 0, k = 0; j < 3; j++) {
    for (size_t lh = 0; lh < 2; lh++) {
      v[k] = reinterpret(
        Vec<Int, 16>(vcombine_s32(z[0][j].val[lh], z[1][j].val[lh])),
        OutputType<T>());
      k++;
      if (k >= 5) break;
    }
  }
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE void swizzle(Vec<Long, 16> v[5], Integer<5>)
{
  const Vec<Long, 16> tmp[5] = {v[0], v[1], v[2], v[3], v[4]};
  v[0] = vcombine_s64(vget_low_s64(tmp[0]), vget_high_s64(tmp[2]));
  v[1] = vcombine_s64(vget_high_s64(tmp[0]), vget_low_s64(tmp[3]));
  v[2] = vcombine_s64(vget_low_s64(tmp[1]), vget_high_s64(tmp[3]));
  v[3] = vcombine_s64(vget_high_s64(tmp[1]), vget_low_s64(tmp[4]));
  v[4] = vcombine_s64(vget_low_s64(tmp[2]), vget_high_s64(tmp[4]));
}

static SIMD_INLINE void swizzle(Vec<Double, 16> v[5], Integer<5>)
{
  const Vec<Double, 16> tmp[5] = {v[0], v[1], v[2], v[3], v[4]};
  v[0] = vcombine_f64(vget_low_f64(tmp[0]), vget_high_f64(tmp[2]));
  v[1] = vcombine_f64(vget_high_f64(tmp[0]), vget_low_f64(tmp[3]));
  v[2] = vcombine_f64(vget_low_f64(tmp[1]), vget_high_f64(tmp[3]));
  v[3] = vcombine_f64(vget_high_f64(tmp[1]), vget_low_f64(tmp[4]));
  v[4] = vcombine_f64(vget_low_f64(tmp[2]), vget_high_f64(tmp[4]));
}
#endif

// -------------------------------------------------------------------------
// compare functions
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_CMP(CMP, TYPE, NEON_SUF, NEON_USUF)                       \
  static SIMD_INLINE Vec<TYPE, 16> cmp##CMP(const Vec<TYPE, 16> &a,            \
                                            const Vec<TYPE, 16> &b)            \
  {                                                                            \
    return vreinterpretq_##NEON_SUF##_##NEON_USUF(                             \
      vc##CMP##q##_##NEON_SUF(a, b));                                          \
  }

#ifdef SIMD_64BIT_TYPES
#define SIMDVEC_NEON_CMP_ALL(CMP)                                              \
  SIMDVEC_NEON_CMP(CMP, Byte, u8, u8)                                          \
  SIMDVEC_NEON_CMP(CMP, SignedByte, s8, u8)                                    \
  SIMDVEC_NEON_CMP(CMP, Word, u16, u16)                                        \
  SIMDVEC_NEON_CMP(CMP, Short, s16, u16)                                       \
  SIMDVEC_NEON_CMP(CMP, Int, s32, u32)                                         \
  SIMDVEC_NEON_CMP(CMP, Long, s64, u64)                                        \
  SIMDVEC_NEON_CMP(CMP, Float, f32, u32)                                       \
  SIMDVEC_NEON_CMP(CMP, Double, f64, u64)
#else
#define SIMDVEC_NEON_CMP_ALL(CMP)                                              \
  SIMDVEC_NEON_CMP(CMP, Byte, u8, u8)                                          \
  SIMDVEC_NEON_CMP(CMP, SignedByte, s8, u8)                                    \
  SIMDVEC_NEON_CMP(CMP, Word, u16, u16)                                        \
  SIMDVEC_NEON_CMP(CMP, Short, s16, u16)                                       \
  SIMDVEC_NEON_CMP(CMP, Int, s32, u32)                                         \
  SIMDVEC_NEON_CMP(CMP, Float, f32, u32)
#endif

SIMDVEC_NEON_CMP_ALL(lt)
SIMDVEC_NEON_CMP_ALL(le)
SIMDVEC_NEON_CMP_ALL(eq)
SIMDVEC_NEON_CMP_ALL(gt)
SIMDVEC_NEON_CMP_ALL(ge)

#undef SIMDVEC_NEON_CMP_ALL
#undef SIMDVEC_NEON_CMP

// -------------------------------------------------------------------------
// compare !=
// -------------------------------------------------------------------------

#define SIMDVEC_NEON_CMPNEQ(TYPE, NEON_SUF, NEON_USUF)                         \
  static SIMD_INLINE Vec<TYPE, 16> cmpneq(const Vec<TYPE, 16> &a,              \
                                          const Vec<TYPE, 16> &b)              \
  {                                                                            \
    return vreinterpretq_##NEON_SUF##_u32(                                     \
      vmvnq_u32(vreinterpretq_u32_##NEON_USUF(vceqq_##NEON_SUF(a, b))));       \
  }

SIMDVEC_NEON_CMPNEQ(Byte, u8, u8)
SIMDVEC_NEON_CMPNEQ(SignedByte, s8, u8)
SIMDVEC_NEON_CMPNEQ(Word, u16, u16)
SIMDVEC_NEON_CMPNEQ(Short, s16, u16)
SIMDVEC_NEON_CMPNEQ(Int, s32, u32)
SIMDVEC_NEON_CMPNEQ(Float, f32, u32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_CMPNEQ(Long, s64, u64)
SIMDVEC_NEON_CMPNEQ(Double, f64, u64)
#endif

#undef SIMDVEC_NEON_CMPNEQ

// -------------------------------------------------------------------------
// ifelse
// -------------------------------------------------------------------------

// vbslq, unsigned mask
#define SIMDVEC_NEON_IFELSE(T, NEON_SUF, NEON_USUF)                            \
  static SIMD_INLINE Vec<T, 16> ifelse(const Vec<T, 16> &cond,                 \
                                       const Vec<T, 16> &trueVal,              \
                                       const Vec<T, 16> &falseVal)             \
  {                                                                            \
    return vbslq_##NEON_SUF(vreinterpretq_##NEON_USUF##_##NEON_SUF(cond),      \
                            trueVal, falseVal);                                \
  }

SIMDVEC_NEON_IFELSE(Byte, u8, u8)
SIMDVEC_NEON_IFELSE(SignedByte, s8, u8)
SIMDVEC_NEON_IFELSE(Word, u16, u16)
SIMDVEC_NEON_IFELSE(Short, s16, u16)
SIMDVEC_NEON_IFELSE(Int, s32, u32)
SIMDVEC_NEON_IFELSE(Float, f32, u32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_IFELSE(Long, s64, u64)
SIMDVEC_NEON_IFELSE(Double, f64, u64)
#endif

#undef SIMDVEC_NEON_IFELSE

// -------------------------------------------------------------------------
// bit_and
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALLINT(bit_and, vandq)

static SIMD_INLINE Vec<Float, 16> bit_and(const Vec<Float, 16> &a,
                                          const Vec<Float, 16> &b)
{
  return vreinterpretq_f32_s32(
    vandq_s32(vreinterpretq_s32_f32(a), vreinterpretq_s32_f32(b)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> bit_and(const Vec<Double, 16> &a,
                                           const Vec<Double, 16> &b)
{
  return vreinterpretq_f64_s64(
    vandq_s64(vreinterpretq_s64_f64(a), vreinterpretq_s64_f64(b)));
}
#endif

// -------------------------------------------------------------------------
// bit_or
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALLINT(bit_or, vorrq)

static SIMD_INLINE Vec<Float, 16> bit_or(const Vec<Float, 16> &a,
                                         const Vec<Float, 16> &b)
{
  return vreinterpretq_f32_s32(
    vorrq_s32(vreinterpretq_s32_f32(a), vreinterpretq_s32_f32(b)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> bit_or(const Vec<Double, 16> &a,
                                          const Vec<Double, 16> &b)
{
  return vreinterpretq_f64_s64(
    vorrq_s64(vreinterpretq_s64_f64(a), vreinterpretq_s64_f64(b)));
}
#endif

// -------------------------------------------------------------------------
// bit_andnot
// -------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 16> bit_andnot(const Vec<T, 16> &a,
                                         const Vec<T, 16> &b)
{
  return bit_and(bit_not(a), b);
}

// -------------------------------------------------------------------------
// bit_xor
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY_ALLINT(bit_xor, veorq)

static SIMD_INLINE Vec<Float, 16> bit_xor(const Vec<Float, 16> &a,
                                          const Vec<Float, 16> &b)
{
  return vreinterpretq_f32_s32(
    veorq_s32(vreinterpretq_s32_f32(a), vreinterpretq_s32_f32(b)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Double, 16> bit_xor(const Vec<Double, 16> &a,
                                           const Vec<Double, 16> &b)
{
  return vreinterpretq_f64_s64(
    veorq_s64(vreinterpretq_s64_f64(a), vreinterpretq_s64_f64(b)));
}
#endif

// -------------------------------------------------------------------------
// bit_not
// -------------------------------------------------------------------------

SIMDVEC_NEON_UNARY(bit_not, Byte, vmvnq, u8)
SIMDVEC_NEON_UNARY(bit_not, SignedByte, vmvnq, s8)
SIMDVEC_NEON_UNARY(bit_not, Word, vmvnq, u16)
SIMDVEC_NEON_UNARY(bit_not, Short, vmvnq, s16)
SIMDVEC_NEON_UNARY(bit_not, Int, vmvnq, s32)

static SIMD_INLINE Vec<Float, 16> bit_not(const Vec<Float, 16> &a)
{
  return vreinterpretq_f32_s32(vmvnq_s32(vreinterpretq_s32_f32(a)));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> bit_not(const Vec<Long, 16> &a)
{
  return vreinterpretq_s64_u32(vmvnq_u32(vreinterpretq_u32_s64(a)));
}
static SIMD_INLINE Vec<Double, 16> bit_not(const Vec<Double, 16> &a)
{
  return vreinterpretq_f64_s32(vmvnq_s32(vreinterpretq_s32_f64(a)));
}
#endif

// -------------------------------------------------------------------------
// avg: average with rounding up
// -------------------------------------------------------------------------

SIMDVEC_NEON_BINARY(avg, Byte, vrhaddq, u8)
SIMDVEC_NEON_BINARY(avg, SignedByte, vrhaddq, s8)
SIMDVEC_NEON_BINARY(avg, Word, vrhaddq, u16)
SIMDVEC_NEON_BINARY(avg, Short, vrhaddq, s16)
SIMDVEC_NEON_BINARY(avg, Int, vrhaddq, s32)

static SIMD_INLINE Vec<Float, 16> avg(const Vec<Float, 16> &a,
                                      const Vec<Float, 16> &b)
{
  return vmulq_n_f32(vaddq_f32(a, b), 0.5f);
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> avg(const Vec<Long, 16> &a,
                                     const Vec<Long, 16> &b)
{
  // vrhaddq_s64 does not exist
  // workaround from Hacker's Delight, 2-5 Average of Two Integers:
  // (a | b) - ((a ^ b) >> 1)
  return vsubq_s64(vorrq_s64(a, b), vshrq_n_s64(veorq_s64(a, b), 1));
}
static SIMD_INLINE Vec<Double, 16> avg(const Vec<Double, 16> &a,
                                       const Vec<Double, 16> &b)
{
  return vmulq_n_f64(vaddq_f64(a, b), 0.5);
}
#endif

// -------------------------------------------------------------------------
// test_all_zeros
// -------------------------------------------------------------------------

// from solution suggested by Henri Ylitie
// http://stackoverflow.com/questions/15389539/
//   fastest-way-to-test-a-128-bit-neon-register-
//   for-a-value-of-0-using-intrinsics

static SIMD_INLINE float32x2_t vorr_f32(float32x2_t a, float32x2_t b)
{
  return vreinterpret_f32_s32(
    vorr_s32(vreinterpret_s32_f32(a), vreinterpret_s32_f32(b)));
}

// vpmax has to operate on unsigned (u32), otherwise 0 could be
// the max. of a pair even though the other value is non-zero (neg.)
#define SIMDVEC_NEON_TESTALLZEROS(T, NEON_SUF)                                 \
  static SIMD_INLINE bool test_all_zeros(const Vec<T, 16> &a)                  \
  {                                                                            \
    uint32x4_t au  = vreinterpretq_u32_##NEON_SUF(a);                          \
    uint32x2_t tmp = vorr_u32(vget_low_u32(au), vget_high_u32(au));            \
    return !vget_lane_u32(vpmax_u32(tmp, tmp), 0);                             \
  }

SIMDVEC_NEON_TESTALLZEROS(Byte, u8)
SIMDVEC_NEON_TESTALLZEROS(SignedByte, s8)
SIMDVEC_NEON_TESTALLZEROS(Word, u16)
SIMDVEC_NEON_TESTALLZEROS(Short, s16)
SIMDVEC_NEON_TESTALLZEROS(Int, s32)
SIMDVEC_NEON_TESTALLZEROS(Float, f32)
#ifdef SIMD_64BIT_TYPES
SIMDVEC_NEON_TESTALLZEROS(Long, s64)
SIMDVEC_NEON_TESTALLZEROS(Double, f64)
#endif

#undef SIMDVEC_NEON_TESTALLZEROS

// -------------------------------------------------------------------------
// test_all_ones
// -------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE bool test_all_ones(const Vec<T, 16> &a)
{
  return test_all_zeros(bit_not(a));
}

// -------------------------------------------------------------------------
// reverse
// -------------------------------------------------------------------------

// https://stackoverflow.com/questions/18760784/reverse-vector-order-in-arm-neon-intrinsics

#define SIMDVEC_NEON_REVERSE(T, NEON_SUF)                                      \
  static SIMD_INLINE Vec<T, 16> reverse(const Vec<T, 16> &a)                   \
  {                                                                            \
    const auto t = vrev64q_##NEON_SUF(a);                                      \
    return vcombine_##NEON_SUF(vget_high_##NEON_SUF(t),                        \
                               vget_low_##NEON_SUF(t));                        \
  }

SIMDVEC_NEON_REVERSE(Byte, u8)
SIMDVEC_NEON_REVERSE(SignedByte, s8)
SIMDVEC_NEON_REVERSE(Word, u16)
SIMDVEC_NEON_REVERSE(Short, s16)
SIMDVEC_NEON_REVERSE(Int, s32)
SIMDVEC_NEON_REVERSE(Float, f32)
#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> reverse(const Vec<Long, 16> &a)
{
  return vcombine_s64(vget_high_s64(a), vget_low_s64(a));
}
static SIMD_INLINE Vec<Double, 16> reverse(const Vec<Double, 16> &a)
{
  return vcombine_f64(vget_high_f64(a), vget_low_f64(a));
}
#endif

#undef SIMDVEC_NEON_REVERSE

// ---------------------------------------------------------------------------
// msb2int
// ---------------------------------------------------------------------------

// 17. Sep 22 (Jonas Keller): added msb2int functions

static SIMD_INLINE uint64_t msb2int(const Vec<Byte, 16> &a)
{
  // from: https://stackoverflow.com/a/58381188/8461272

  // Example input (half scale):
  // 0x89 FF 1D C0 00 10 99 33

  // Shift out everything but the sign bits
  // 0x01 01 00 01 00 00 01 00
  uint8x16_t high_bits = vshrq_n_u8(a, 7);

  // Merge the even lanes together with vsra. The '??' bytes are garbage.
  // vsri could also be used, but it is slightly slower on aarch64.
  // 0x??03 ??02 ??00 ??01
  uint16x8_t paired16 = vsraq_n_u16(vreinterpretq_u16_u8(high_bits),
                                    vreinterpretq_u16_u8(high_bits), 7);
  // Repeat with wider lanes.
  // 0x??????0B ??????04
  uint32x4_t paired32 = vsraq_n_u32(vreinterpretq_u32_u16(paired16),
                                    vreinterpretq_u32_u16(paired16), 14);
  // 0x??????????????4B
  uint64x2_t paired64 = vsraq_n_u64(vreinterpretq_u64_u32(paired32),
                                    vreinterpretq_u64_u32(paired32), 28);
  // Extract the low 8 bits from each lane and join.
  // 0x4B
  return vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 0) |
         ((int) vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 8) << 8);
}

static SIMD_INLINE uint64_t msb2int(const Vec<SignedByte, 16> &a)
{
  // the same as msb2int(Vec<Byte,16>)
  return msb2int(reinterpret(a, OutputType<Byte>()));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Word, 16> &a)
{
  // analogous to msb2int(Vec<Byte,16>)
  // idea from: https://stackoverflow.com/a/58381188/8461272

  // Shift out everything but the sign bits
  uint16x8_t high_bits = vshrq_n_u16(a, 15);

  // Merge the even lanes together with vsra. The '??' bytes are garbage.
  uint32x4_t paired32 = vsraq_n_u32(vreinterpretq_u32_u16(high_bits),
                                    vreinterpretq_u32_u16(high_bits), 15);
  // Repeat with wider lanes.
  uint64x2_t paired64 = vsraq_n_u64(vreinterpretq_u64_u32(paired32),
                                    vreinterpretq_u64_u32(paired32), 30);
  // Extract the low 4 bits from each lane and join.
  return (vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 0) & 0xf) |
         (vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 8) << 4);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Short, 16> &a)
{
  // the same as msb2int(Vec<Word,16>)
  return msb2int(reinterpret(a, OutputType<Word>()));
}

static SIMD_INLINE uint64_t msb2int(const Vec<Int, 16> &a)
{
  // analogous to msb2int(Vec<Byte,16>)
  // idea from: https://stackoverflow.com/a/58381188/8461272

  // Shift out everything but the sign bits
  uint32x4_t high_bits = vshrq_n_u32(vreinterpretq_u32_s32(a), 31);

  // Merge the even lanes together with vsra. The '??' bytes are garbage.
  uint64x2_t paired64 = vsraq_n_u64(vreinterpretq_u64_u32(high_bits),
                                    vreinterpretq_u64_u32(high_bits), 31);
  // Extract the low 2 bits from each lane and join.
  return (vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 0) & 0x3) |
         ((vgetq_lane_u8(vreinterpretq_u8_u64(paired64), 8) & 0x3) << 2);
}

static SIMD_INLINE uint64_t msb2int(const Vec<Float, 16> &a)
{
  // the same as msb2int(Vec<Int,16>)
  return msb2int(reinterpret(a, OutputType<Int>()));
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE uint64_t msb2int(const Vec<Long, 16> &a)
{
  // shift out everything but the sign bits
  uint64x2_t high_bits = vshrq_n_u64(vreinterpretq_u64_s64(a), 63);
  // extract the low bit from each lane and join
  return vgetq_lane_u8(vreinterpretq_u8_u64(high_bits), 0) |
         (vgetq_lane_u8(vreinterpretq_u8_u64(high_bits), 8) << 1);
}
static SIMD_INLINE uint64_t msb2int(const Vec<Double, 16> &a)
{
  // the same as msb2int(Vec<Long,16>)
  return msb2int(reinterpret(a, OutputType<Long>()));
}
#endif

// ---------------------------------------------------------------------------
// int2msb
// ---------------------------------------------------------------------------

// 06. Oct 22 (Jonas Keller): added int2msb functions

static SIMD_INLINE Vec<Byte, 16> int2msb(const uint64_t a, OutputType<Byte>,
                                         Integer<16>)
{
  uint8x8_t aVecLo = vdup_n_u8(a & 0xff);
  uint8x8_t aVecHi = vdup_n_u8((a >> 8) & 0xff);
  uint8x16_t aVec  = vcombine_u8(aVecLo, aVecHi);
  // shift the bits to the msb
  int8x16_t shiftAmounts = {7, 6, 5, 4, 3, 2, 1, 0, 7, 6, 5, 4, 3, 2, 1, 0};
  uint8x16_t shifted     = vshlq_u8(aVec, shiftAmounts);
  return vandq_u8(shifted, vdupq_n_u8(0x80));
}

static SIMD_INLINE Vec<SignedByte, 16> int2msb(const uint64_t a,
                                               OutputType<SignedByte>,
                                               Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Byte>(), Integer<16>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Word, 16> int2msb(const uint64_t a, OutputType<Word>,
                                         Integer<16>)
{
  uint16x8_t aVec = vdupq_n_u16(a & 0xff);
  // shift the bits to the msb
  int16x8_t shiftAmounts = {15, 14, 13, 12, 11, 10, 9, 8};
  uint16x8_t shifted     = vshlq_u16(aVec, shiftAmounts);
  return vandq_u16(shifted, vdupq_n_u16(0x8000));
}

static SIMD_INLINE Vec<Short, 16> int2msb(const uint64_t a, OutputType<Short>,
                                          Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Word>(), Integer<16>()),
                     OutputType<Short>());
}

static SIMD_INLINE Vec<Int, 16> int2msb(const uint64_t a, OutputType<Int>,
                                        Integer<16>)
{
  int32x4_t aVec = vdupq_n_s32(a & 0xf);
  // shift the bits to the msb
  int32x4_t shiftAmounts = {31, 30, 29, 28};
  int32x4_t shifted      = vshlq_s32(aVec, shiftAmounts);
  return vandq_s32(shifted, vdupq_n_s32(0x80000000));
}

static SIMD_INLINE Vec<Float, 16> int2msb(const uint64_t a, OutputType<Float>,
                                          Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Int>(), Integer<16>()),
                     OutputType<Float>());
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> int2msb(const uint64_t a, OutputType<Long>,
                                         Integer<16>)
{
  int64x2_t aVec = vdupq_n_s64(a & 0x3);
  // shift the bits to the msb
  int64x2_t shiftAmounts = {63, 62};
  int64x2_t shifted      = vshlq_s64(aVec, shiftAmounts);
  int64x2_t result       = vandq_s64(shifted, vdupq_n_s64(0x8000000000000000));
  return result;
}
static SIMD_INLINE Vec<Double, 16> int2msb(const uint64_t a, OutputType<Double>,
                                           Integer<16>)
{
  return reinterpret(int2msb(a, OutputType<Long>(), Integer<16>()),
                     OutputType<Double>());
}
#endif

// ---------------------------------------------------------------------------
// int2bits
// ---------------------------------------------------------------------------

// 09. Oct 22 (Jonas Keller): added int2bits functions

static SIMD_INLINE Vec<Byte, 16> int2bits(const uint64_t a, OutputType<Byte>,
                                          Integer<16>)
{
  uint8x8_t aVecLo = vdup_n_u8(a & 0xff);
  uint8x8_t aVecHi = vdup_n_u8((a >> 8) & 0xff);
  uint8x16_t aVec  = vcombine_u8(aVecLo, aVecHi);
  uint8x16_t sel   = {0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80,
                      0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80};
  return vtstq_u8(aVec, sel);
}

static SIMD_INLINE Vec<SignedByte, 16> int2bits(const uint64_t a,
                                                OutputType<SignedByte>,
                                                Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Byte>(), Integer<16>()),
                     OutputType<SignedByte>());
}

static SIMD_INLINE Vec<Word, 16> int2bits(const uint64_t a, OutputType<Word>,
                                          Integer<16>)
{
  uint16x8_t aVec = vdupq_n_u16(a & 0xff);
  uint16x8_t sel  = {0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80};
  return vtstq_u16(aVec, sel);
}

static SIMD_INLINE Vec<Short, 16> int2bits(const uint64_t a, OutputType<Short>,
                                           Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Word>(), Integer<16>()),
                     OutputType<Short>());
}

static SIMD_INLINE Vec<Int, 16> int2bits(const uint64_t a, OutputType<Int>,
                                         Integer<16>)
{
  int32x4_t aVec = vdupq_n_s32(a & 0xf);
  int32x4_t sel  = {0x01, 0x02, 0x04, 0x08};
  return vreinterpretq_s32_u32(vtstq_s32(aVec, sel));
}

static SIMD_INLINE Vec<Float, 16> int2bits(const uint64_t a, OutputType<Float>,
                                           Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Int>(), Integer<16>()),
                     OutputType<Float>());
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> int2bits(const uint64_t a, OutputType<Long>,
                                          Integer<16>)
{
  int64x2_t aVec = vdupq_n_s64(a & 0xf);
  int64x2_t sel  = {0x01, 0x02};
  return vreinterpretq_s64_u64(vtstq_s64(aVec, sel));
}
static SIMD_INLINE Vec<Double, 16> int2bits(const uint64_t a,
                                            OutputType<Double>, Integer<16>)
{
  return reinterpret(int2bits(a, OutputType<Long>(), Integer<16>()),
                     OutputType<Double>());
}
#endif

// ---------------------------------------------------------------------------
// iota
// ---------------------------------------------------------------------------

// 30. Jan 23 (Jonas Keller): added iota

static SIMD_INLINE Vec<Byte, 16> iota(OutputType<Byte>, Integer<16>)
{
  uint8x16_t res = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
  return res;
}

static SIMD_INLINE Vec<SignedByte, 16> iota(OutputType<SignedByte>, Integer<16>)
{
  int8x16_t res = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
  return res;
}

static SIMD_INLINE Vec<Word, 16> iota(OutputType<Word>, Integer<16>)
{
  uint16x8_t res = {0, 1, 2, 3, 4, 5, 6, 7};
  return res;
}

static SIMD_INLINE Vec<Short, 16> iota(OutputType<Short>, Integer<16>)
{
  int16x8_t res = {0, 1, 2, 3, 4, 5, 6, 7};
  return res;
}

static SIMD_INLINE Vec<Int, 16> iota(OutputType<Int>, Integer<16>)
{
  int32x4_t res = {0, 1, 2, 3};
  return res;
}

static SIMD_INLINE Vec<Float, 16> iota(OutputType<Float>, Integer<16>)
{
  float32x4_t res = {0.0f, 1.0f, 2.0f, 3.0f};
  return res;
}

#ifdef SIMD_64BIT_TYPES
static SIMD_INLINE Vec<Long, 16> iota(OutputType<Long>, Integer<16>)
{
  int64x2_t res = {0, 1};
  return res;
}
static SIMD_INLINE Vec<Double, 16> iota(OutputType<Double>, Integer<16>)
{
  float64x2_t res = {0.0, 1.0};
  return res;
}
#endif
} // namespace base
} // namespace internal
} // namespace simd

#endif

#endif // SIMD_VEC_BASE_IMPL_NEON_16_H_

// ===========================================================================
//
// SIMDVecBaseImplSandbox.H --
// test template functions for Vec and base level ("level-0") function
// templates these functions just print out template parameters and some
// arguments
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved sandbox implementations into internal
// namespace

#ifndef SIMD_VEC_BASE_IMPL_SANDBOX_H_
#define SIMD_VEC_BASE_IMPL_SANDBOX_H_

// TODO: should this only contain level 0 functions?

#include <cstdint>
#include <cstdio>

#ifdef SIMDVEC_SANDBOX

namespace simd {

// ===========================================================================
// generic template for Vec
// ===========================================================================

template <typename T, size_t SIMD_WIDTH>
class Vec
{
public:
  using Type                       = T;
  static constexpr size_t elements = SIMD_WIDTH / sizeof(T);
  static constexpr size_t elems    = elements;
  static constexpr size_t bytes    = SIMD_WIDTH;
  Vec() {}
  // 05. Sep 23 (Jonas Keller): added allocator
  using allocator = simd_aligned_allocator<Vec<T, bytes>, bytes>;
};

namespace internal {
namespace base {

// ===========================================================================
// reinterpretation cast
// ===========================================================================

template <typename Tdst, typename Tsrc, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tdst, SIMD_WIDTH> reinterpret(
  const Vec<Tsrc, SIMD_WIDTH> &, OutputType<Tdst>)
{
  printf("reinterpret<%s,%s,%zu>(V)\n", TypeInfo<Tdst>::name(),
         TypeInfo<Tsrc>::name(), SIMD_WIDTH);
  return Vec<Tdst, SIMD_WIDTH>();
}

// ===========================================================================
// generic functions on Vec
// ===========================================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setzero(OutputType<T>,
                                              Integer<SIMD_WIDTH>)
{
  printf("setzero<%s,%zu>()\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> set1(T, Integer<SIMD_WIDTH>)
{
  printf("set1<%s,%zu>()\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// template argument order reversed so that T can be omitted in instantiation
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE Vec<T, SIMD_WIDTH> load(const T *const p,
                                           Integer<SIMD_WIDTH>)
{
  printf("load<%zu,%s>(%p)\n", SIMD_WIDTH, TypeInfo<T>::name(), (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

// template argument order reversed so that T can be omitted in instantiation
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE Vec<T, SIMD_WIDTH> loadu(const T *const p,
                                            Integer<SIMD_WIDTH>)
{
  printf("loadu<%zu,%s>(%p)\n", SIMD_WIDTH, TypeInfo<T>::name(), (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store(T *const p, const Vec<T, SIMD_WIDTH> &)
{
  printf("store<%s,%zu>(%p,V)\n", TypeInfo<T>::name(), SIMD_WIDTH, (void *) p);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu(T *const p, const Vec<T, SIMD_WIDTH> &)
{
  printf("storeu<%s,%zu>(%p,V)\n", TypeInfo<T>::name(), SIMD_WIDTH, (void *) p);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void stream_store(T *const p, const Vec<T, SIMD_WIDTH> &)
{
  printf("stream_store<%s,%zu>(%p,V)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
}

static SIMD_INLINE void lfence()
{
  printf("lfence\n");
}

static SIMD_INLINE void sfence()
{
  printf("sfence\n");
}

static SIMD_INLINE void mfence()
{
  printf("mfence\n");
}

template <size_t INDEX, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T extract(const Vec<T, SIMD_WIDTH> &)
{
  printf("extract<%zu,%s,%zu>(V)\n", INDEX, TypeInfo<T>::name(), SIMD_WIDTH);
  return T(0);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> add(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("add<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> adds(const Vec<T, SIMD_WIDTH> &,
                                           const Vec<T, SIMD_WIDTH> &)
{
  printf("adds<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sub(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("sub<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> subs(const Vec<T, SIMD_WIDTH> &,
                                           const Vec<T, SIMD_WIDTH> &)
{
  printf("subs<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> neg(const Vec<T, SIMD_WIDTH> &)
{
  printf("neg<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mul(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("mul<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> div(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("div<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> ceil(const Vec<T, SIMD_WIDTH> &)
{
  printf("ceil<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> floor(const Vec<T, SIMD_WIDTH> &)
{
  printf("floor<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> round(const Vec<T, SIMD_WIDTH> &)
{
  printf("round<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> truncate(const Vec<T, SIMD_WIDTH> &)
{
  printf("truncate<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> rcp(const Vec<T, SIMD_WIDTH> &)
{
  printf("rcp<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> rsqrt(const Vec<T, SIMD_WIDTH> &)
{
  printf("rsqrt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sqrt(const Vec<T, SIMD_WIDTH> &)
{
  printf("sqrt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> min(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("min<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> max(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("max<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> abs(const Vec<T, SIMD_WIDTH> &)
{
  printf("abs<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// unpack NUM_ELEMS elements of type T
// PART=0: low half of input vectors,
// PART=1: high half of input vectors
template <size_t PART, size_t BYTES, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> unpack(const Vec<T, SIMD_WIDTH> &,
                                             const Vec<T, SIMD_WIDTH> &,
                                             Part<PART>, Bytes<BYTES>)
{
  printf("unpack<PART=%zu,BYTES=%zu,%s,%zu>(V,V)\n", PART, BYTES,
         TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// unpack16
template <size_t PART, size_t BYTES, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> unpack16(const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               Part<PART>, Bytes<BYTES>)
{
  printf("unpack16<PART=%zu,BYTES=%zu,%s,%zu>(V,V)\n", PART, BYTES,
         TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// extractLane
template <size_t LANE_INDEX, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, SIMD_WIDTH> &)
{
  printf("extractLane<IMM=%zu,%s,%zu>(V)\n", LANE_INDEX, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, 16>();
}

// zip (2 x unpack) NUM_ELEMS elements of type T
template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void zip(const Vec<T, SIMD_WIDTH>, const Vec<T, SIMD_WIDTH>,
                            Vec<T, SIMD_WIDTH> &, Vec<T, SIMD_WIDTH> &)
{
  printf("zip<NUM_ELEMS=%zu,%s,%zu>(V,V,V&,V&)\n", NUM_ELEMS,
         TypeInfo<T>::name(), SIMD_WIDTH);
}

// zip16
template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void zip16(const Vec<T, SIMD_WIDTH>,
                              const Vec<T, SIMD_WIDTH>, Vec<T, SIMD_WIDTH> &,
                              Vec<T, SIMD_WIDTH> &)
{
  printf("zip16<NUM_ELEMS=%zu,%s,%zu>(V,V,V&,V&)\n", NUM_ELEMS,
         TypeInfo<T>::name(), SIMD_WIDTH);
}

// unzip (inverse of zip)
template <size_t BYTES, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unzip(const Vec<T, SIMD_WIDTH>,
                              const Vec<T, SIMD_WIDTH>, Vec<T, SIMD_WIDTH> &,
                              Vec<T, SIMD_WIDTH> &, Bytes<BYTES>)
{
  printf("unzip<BYTES=%zu,%s,%zu>(V,V,V&,V&)\n", BYTES, TypeInfo<T>::name(),
         SIMD_WIDTH);
}

// unifies packs and packus, depending on Tout
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> &,
                                               const Vec<Tin, SIMD_WIDTH> &,
                                               OutputType<Tout>)
{
  printf("packs<%s,%s,%zu>(V,V)\n", TypeInfo<Tout>::name(),
         TypeInfo<Tin>::name(), SIMD_WIDTH);
  return Vec<Tout, SIMD_WIDTH>();
}

// generalized version of unpack-based type conversion: includes
// multistage extension (zero-extend or sign-extend, depending on
// type)
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void extend(
  const Vec<Tin, SIMD_WIDTH> &,
  Vec<Tout, SIMD_WIDTH>[sizeof(Tout) / sizeof(Tin)])
{
  printf("extend<%s,%s,%zu>>(V,V*)\n", TypeInfo<Tout>::name(),
         TypeInfo<Tin>::name(), SIMD_WIDTH);
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srai(const Vec<T, SIMD_WIDTH> &)
{
  printf("srai<%zu,%s,%zu>(V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srli(const Vec<T, SIMD_WIDTH> &)
{
  printf("srli<%zu,%s,%zu>(V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> slli(const Vec<T, SIMD_WIDTH> &)
{
  printf("slli<%zu,%s,%zu>(V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// 12. Jan 23 (Jonas Keller): added sra, srl and sll functions

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sra(const Vec<T, SIMD_WIDTH> &,
                                          const uint8_t)
{
  printf("sra<%s,%zu>(V, U8)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srl(const Vec<T, SIMD_WIDTH> &,
                                          const uint8_t)
{
  printf("srl<%s,%zu>(V, U8)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sll(const Vec<T, SIMD_WIDTH> &,
                                          const uint8_t)
{
  printf("sll<%s,%zu>(V, U8)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// conversion without changes in the number of vector elements
// saturated version
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> cvts(const Vec<Tin, SIMD_WIDTH> &,
                                              OutputType<Tout>)
{
  printf("cvts<%s,%s,%zu>(V)\n", TypeInfo<Tout>::name(), TypeInfo<Tin>::name(),
         SIMD_WIDTH);
  return Vec<Tout, SIMD_WIDTH>();
}

// horizontal addition
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadd(const Vec<T, SIMD_WIDTH> &,
                                           const Vec<T, SIMD_WIDTH> &)
{
  printf("hadd<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// horizontal addition (with saturation)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadds(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("hadds<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// horizontal subtraction
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hsub(const Vec<T, SIMD_WIDTH> &,
                                           const Vec<T, SIMD_WIDTH> &)
{
  printf("hsub<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// horizontal subtraction (with saturation)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hsubs(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("hsubs<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// logical right shift by n elements, filling in zeros
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srle(const Vec<T, SIMD_WIDTH> &)
{
  printf("srle<%zu,%s,%zu>(V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// logical left shift by n elements, filling in zeros
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> slle(const Vec<T, SIMD_WIDTH> &)
{
  printf("slle<%zu,%s,%zu>(V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// alignr (with number of elements in imm):
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> alignre(const Vec<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &)
{
  printf("alignre<%zu,%s,%zu>(V,V)\n", COUNT, TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// swizzle (AoS to SoA), in-place, with template parameter N
template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle(Vec<T, SIMD_WIDTH>[N], Integer<N>)
{
  printf("swizzle<%zu,%s,%zu>(V,V*)\n", N, TypeInfo<T>::name(), SIMD_WIDTH);
}

// ifelse
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> ifelse(const Vec<T, SIMD_WIDTH> &,
                                             const Vec<T, SIMD_WIDTH> &,
                                             const Vec<T, SIMD_WIDTH> &)
{
  printf("ifelse<%s,%zu>(V,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare <
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmplt(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("cmplt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare <=
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmple(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("cmple<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare ==
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpeq(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("cmpeq<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare >=
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpge(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("cmpge<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare >
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpgt(const Vec<T, SIMD_WIDTH> &,
                                            const Vec<T, SIMD_WIDTH> &)
{
  printf("cmpgt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// compare !=
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpneq(const Vec<T, SIMD_WIDTH> &,
                                             const Vec<T, SIMD_WIDTH> &)
{
  printf("cmpneq<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// bitwise and
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_and(const Vec<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &)
{
  printf("and<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// bitwise or
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_or(const Vec<T, SIMD_WIDTH> &,
                                             const Vec<T, SIMD_WIDTH> &)
{
  printf("or<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// bitwise andnot: (not a) and b
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_andnot(const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("bit_andnot<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// bitwise xor
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_xor(const Vec<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &)
{
  printf("xor<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// bitwise not (work-arounds required)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_not(const Vec<T, SIMD_WIDTH> &)
{
  printf("not<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avg(const Vec<T, SIMD_WIDTH> &,
                                          const Vec<T, SIMD_WIDTH> &)
{
  printf("avg<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// test if all bits are zeros (false)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool test_all_zeros(const Vec<T, SIMD_WIDTH> &)
{
  printf("test_all_zeros<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return 0;
}

// test if all bits are ones (true)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool test_all_ones(const Vec<T, SIMD_WIDTH> &)
{
  printf("test_all_ones<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return 0;
}

// TODO: not level 0, should this be in the sandbox?
// negate a, where b is negative, Float only
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sign(const Vec<T, SIMD_WIDTH> &,
                                           const Vec<T, SIMD_WIDTH> &)
{
  printf("sign<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// TODO: not level 0, should this be in the sandbox?
// Computes elementwise absolute difference of vectors
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> absDiff(const Vec<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &)
{
  printf("absDiff<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// reverse order
template <typename T, size_t SIMD_WIDTH>
static Vec<T, SIMD_WIDTH> reverse(const Vec<T, SIMD_WIDTH> &)
{
  printf("reverse<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// 27. Aug 22 (Jonas Keller): added msb2int
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE uint64_t msb2int(const Vec<T, SIMD_WIDTH> &)
{
  printf("msb2int<%s,%zu>(V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return 0;
}

// 09. Oct 22 (Jonas Keller): added int2msb
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> int2msb(const uint64_t, OutputType<T>,
                                              Integer<SIMD_WIDTH>)
{
  printf("int2msb<%s,%zu>(U64)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// 09. Oct 22 (Jonas Keller): added int2bits
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> int2bits(const uint64_t, OutputType<T>,
                                               Integer<SIMD_WIDTH>)
{
  printf("int2bits<%s,%zu>(U64)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// 30. Jan 23 (Jonas Keller): added iota

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> iota(OutputType<T>, Integer<SIMD_WIDTH>)
{
  printf("iota<%s,%zu>()\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}
} // namespace base
} // namespace internal
} // namespace simd

#endif // SIMDVEC_SANDBOX

#endif // SIMD_VEC_BASE_IMPL_SANDBOX_H_

#include <cstdint>
#include <type_traits>

namespace simd {
/**
 * @ingroup group_type_conversion
 * @brief Reinterprets a given Vec as a Vec with a different element
 * type.
 *
 * @tparam Tout element type of the resulting Vec
 * @tparam Tin element type of the given Vec
 * @param a Vec to reinterpret
 * @return reinterpreted Vec
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> reinterpret(
  const Vec<Tin, SIMD_WIDTH> &a)
{
  return internal::base::reinterpret(a, internal::OutputType<Tout>());
}

/**
 * @ingroup group_init
 * @brief Returns a Vec with all elements set to zero.
 * @return Vec with all elements set to zero
 */
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setzero()
{
  return internal::base::setzero(internal::OutputType<T>(),
                                 internal::Integer<SIMD_WIDTH>());
}

/**
 * @ingroup group_init
 * @brief Returns a Vec with all elements set to the same value.
 * @param a value to set all elements to
 * @return Vec with all elements set to the same value
 */
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE Vec<T, SIMD_WIDTH> set1(T a)
{
  return internal::base::set1(a, internal::Integer<SIMD_WIDTH>());
}

// 30. Jan 23 (Jonas Keller): added iota

/**
 * @ingroup group_init
 * @brief Creates a Vec with sequentially increasing numbers, starting
 * with 0.
 *
 * The sequence starts from the lowest element of the Vec, i.e. the lowest
 * element of the Vec is set to 0, the next element to 1, and so on.
 *
 * @return Vec containing sequentially increasing numbers
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> iota()
{
  return internal::base::iota(internal::OutputType<T>(),
                              internal::Integer<SIMD_WIDTH>());
}

/**
 * @ingroup group_cmp
 * @brief Selects elements from two Vec's based on a condition Vec.
 *
 * The element type of the condition Vec must have the same size as the
 * element type of the true and false value Vec's.
 *
 * @param cond condition Vec, each element must be either all 1 bits or all 0
 * bits, representing true or false, respectively
 * @param trueVal Vec to select from if the condition is true
 * @param falseVal Vec to select from if the condition is false
 * @return Vec containing the selected elements
 */
template <typename Tcond, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> ifelse(const Vec<Tcond, SIMD_WIDTH> &cond,
                                             const Vec<T, SIMD_WIDTH> &trueVal,
                                             const Vec<T, SIMD_WIDTH> &falseVal)
{
  static_assert(sizeof(Tcond) == sizeof(T),
                "condition and value types must have the same size");
  return internal::base::ifelse(reinterpret<T>(cond), trueVal, falseVal);
}

// 27. Aug 22 (Jonas Keller): added msb2int

/**
 * @ingroup group_init
 * @brief Collects the most significant bit of each element of a Vec into
 * an integer.
 *
 * @param a Vec
 * @return an integer containing the most significant bit of each element of the
 * input Vec padded with zeros to 64 bits
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE uint64_t msb2int(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::msb2int(a);
}

// 09. Oct 22 (Jonas Keller): added int2msb

/**
 * @ingroup group_init
 * @brief Sets the most significant bit of each element of a Vec to the
 * corresponding bit of an integer.
 *
 * The bottom n bits of the input integer are used, where n is the number of
 * elements in the Vec. All other bits of the input are ignored.
 *
 * All other bits of the output elements are set to zero.
 *
 * @param a integer
 * @return Vec with the most significant bit of each element set to the
 * corresponding bit of the input integer
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> int2msb(const uint64_t a)
{
  return internal::base::int2msb(a, internal::OutputType<T>(),
                                 internal::Integer<SIMD_WIDTH>());
}

// 09. Oct 22 (Jonas Keller): added int2bits

/**
 * @ingroup group_init
 * @brief Sets all bits of each element of a Vec to the corresponding bit
 * of an integer.
 *
 * The bottom n bits of the input integer are used, where n is the number of
 * elements in the Vec. All other bits of the input are ignored.
 *
 * @param a integer
 * @return Vec with all bits of each element set to the corresponding bit of the
 * input integer
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> int2bits(const uint64_t a)
{
  return internal::base::int2bits(a, internal::OutputType<T>(),
                                  internal::Integer<SIMD_WIDTH>());
}

/**
 * @ingroup group_memory_load
 * @brief Loads a Vec from aligned memory.
 *
 * The memory location must be aligned to the @p SIMD_WIDTH.
 *
 * @note The template argument order of @p T and @p SIMD_WIDTH is reversed so
 * that @p T can be omitted in instantiation.
 *
 * @param[in] p pointer to the aligned memory location to load from
 * @return Vec with the loaded values
 */
template <size_t SIMD_WIDTH_DEFAULT_NATIVE, typename T>
static SIMD_INLINE Vec<T, SIMD_WIDTH> load(const T *const p)
{
  // 08. Apr 23 (Jonas Keller):
  // added alignment check (if SIMD_ALIGN_CHK is defined)
  SIMD_CHECK_ALIGNMENT(p, SIMD_WIDTH);
  return internal::base::load(p, internal::Integer<SIMD_WIDTH>());
}

/**
 * @ingroup group_memory_load
 * @brief Loads a Vec from unaligned memory.
 *
 * In contrast to load(const T *const p), the memory location does not need to
 * be aligned to any boundary.
 *
 * @note The template argument order of @p T and @p SIMD_WIDTH is reversed so
 * that @p T can be omitted in instantiation.
 *
 * @param[in] p pointer to the memory location to load from
 * @return Vec with the loaded values
 */
template <size_t SIMD_WIDTH_DEFAULT_NATIVE, typename T>
static SIMD_INLINE Vec<T, SIMD_WIDTH> loadu(const T *const p)
{
  return internal::base::loadu(p, internal::Integer<SIMD_WIDTH>());
}

/**
 * @ingroup group_memory_store
 * @brief Stores a Vec to aligned memory.
 *
 * The memory location must be aligned to the SIMD_WIDTH.
 *
 * @param[out] p pointer to the aligned memory location to store to
 * @param a Vec to store
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store(T *const p, const Vec<T, SIMD_WIDTH> &a)
{
  // 08. Apr 23 (Jonas Keller):
  // added alignment check (if SIMD_ALIGN_CHK is defined)
  SIMD_CHECK_ALIGNMENT(p, SIMD_WIDTH);
  internal::base::store(p, a);
}

/**
 * @ingroup group_memory_store
 * @brief Stores a Vec to unaligned memory.
 *
 * In contrast to store(), the memory location does not need to be aligned to
 * any boundary.
 *
 * @param[out] p pointer to the memory location to store to
 * @param a Vec to store
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu(T *const p, const Vec<T, SIMD_WIDTH> &a)
{
  internal::base::storeu(p, a);
}

/**
 * @ingroup group_memory_store
 * @brief Stores a Vec to aligned memory using a non-temporal memory
 * hint.
 *
 * This function uses the @c _mm*_stream_* intrinsics on Intel and regular
 * store intrinsics on NEON. A call to sfence() may be required in order for
 * other threads/processors to see the stored values. This function may
 * improve performance on some architectures compared to store().
 *
 * The memory location must be aligned to the @p SIMD_WIDTH.
 *
 * @param[out] p pointer to the aligned memory location to store to
 * @param a Vec to store
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void stream_store(T *const p, const Vec<T, SIMD_WIDTH> &a)
{
  // 08. Apr 23 (Jonas Keller):
  // added alignment check (if SIMD_ALIGN_CHK is defined)
  SIMD_CHECK_ALIGNMENT(p, SIMD_WIDTH);
  internal::base::stream_store(p, a);
}

/**
 * @addtogroup group_memory
 * @{
 */

/**
 * @brief Load fence.
 *
 * Forces strong memory ordering (serialization) between load instructions
 * preceding this instruction and load instructions following this
 * instruction, ensuring the system completes all previous loads before
 * executing subsequent loads.
 *
 * @note May be implemented as a full memory barrier on some architectures.
 */
static SIMD_INLINE void lfence()
{
  internal::base::lfence();
}

/**
 * @brief Store fence.
 *
 * Forces strong memory ordering (serialization) between store instructions
 * preceding this instruction and store instructions following this
 * instruction, ensuring the system completes all previous stores before
 * executing subsequent stores.
 *
 * @note May be implemented as a full memory barrier on some architectures.
 */
static SIMD_INLINE void sfence()
{
  internal::base::sfence();
}

/**
 * @brief Full memory fence.
 *
 * Forces strong memory ordering (serialization) between load and store
 * instructions preceding this instruction and load and store instructions
 * following this instruction, ensuring that the system completes all
 * previous memory accesses before executing subsequent memory accesses.
 */
static SIMD_INLINE void mfence()
{
  internal::base::mfence();
}

/** @} */

/**
 * @addtogroup group_arithmetic
 * @{
 */

/**
 * @brief Adds the elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the sums of the elements of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> add(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::add(a, b);
}

/**
 * @brief Adds the elements of two Vec's using saturated arithmetic.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the saturated sums of the elements of the two input
 * Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> adds(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::adds(a, b);
}

/**
 * @brief Subtracts the elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the differences of the elements of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sub(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::sub(a, b);
}

/**
 * @brief Subtracts the elements of two Vec's using saturated arithmetic.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the saturated differences of the elements of the two
 * input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> subs(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::subs(a, b);
}

/**
 * @brief Multiplies the elements of two Vec's.
 *
 * @note This function is only available for floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the products of the elements of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mul(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  static_assert(TypeInfo<T>::isFloatingPoint,
                "mul() is only available for floating point types");
  return internal::base::mul(a, b);
}

/**
 * @brief Divides the elements of two Vec's.
 *
 * @note This function is only available for floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the quotients of the elements of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> div(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  static_assert(TypeInfo<T>::isFloatingPoint,
                "div() is only available for floating point types");
  return internal::base::div(a, b);
}

/**
 * @brief Computes the average of the elements of two Vec's, rounded up.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the rounded up average of the elements of the two
 * input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avg(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::avg(a, b);
}

/** @} */

/**
 * @addtogroup group_horizontal
 * @{
 */

/**
 * @brief Horizontally adds adjacent elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the results of the horizontal additions
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadd(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::hadd(a, b);
}

/**
 * @brief Horizontally adds adjacent elements of two Vec's with saturation.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the results of the horizontal saturated additions
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadds(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::hadds(a, b);
}

/**
 * @brief Horizontally subtracts adjacent elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the results of the horizontal subtractions
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hsub(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::hsub(a, b);
}

/**
 * @brief Horizontally subtracts adjacent elements of two Vec's with saturation.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the results of the horizontal saturated subtractions
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hsubs(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::hsubs(a, b);
}

/** @} */

/**
 * @addtogroup group_math_functions
 * @{
 */

/**
 * @brief Computes the approximate reciprocal of the elements of a
 * Vec.
 *
 * This function is only available for floating point types.
 *
 * @param a Vec to compute the reciprocal of
 * @return Vec containing the approximate reciprocal of the elements of the
 * input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> rcp(const Vec<T, SIMD_WIDTH> &a)
{
  static_assert(TypeInfo<T>::isFloatingPoint,
                "rcp() is only available for floating point types");
  return internal::base::rcp(a);
}

/**
 * @brief Computes the approximate reciprocal square root of the elements of a
 * Vec.
 *
 * This function is only available for floating point types.
 *
 * @param a Vec to compute the reciprocal square root of
 * @return Vec containing the approximate reciprocal square root of the elements
 * of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> rsqrt(const Vec<T, SIMD_WIDTH> &a)
{
  static_assert(TypeInfo<T>::isFloatingPoint,
                "rsqrt() is only available for floating point types");
  return internal::base::rsqrt(a);
}

/**
 * @brief Computes the square root of the elements of a Vec.
 *
 * This function is only available for floating point types.
 *
 * @note This function may only compute an approximation on some
 * architectures.
 *
 * @param a Vec to compute the square root of
 * @return Vec containing the square root of the elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sqrt(const Vec<T, SIMD_WIDTH> &a)
{
  static_assert(TypeInfo<T>::isFloatingPoint,
                "sqrt() is only available for floating point types");
  return internal::base::sqrt(a);
}

/** @} */

/**
 * @addtogroup group_math_operations
 * @{
 */

/**
 * @brief Computes the minimum of the elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the minimum of the elements of the input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> min(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::min(a, b);
}

/**
 * @brief Computes the maximum of the elements of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the maximum of the elements of the input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> max(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::max(a, b);
}

/**
 * @brief Negates the elements of a Vec.
 *
 * This function is only available for signed types.
 *
 * @param a Vec to negate
 * @return Vec containing the negated elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> neg(const Vec<T, SIMD_WIDTH> &a)
{
  static_assert(TypeInfo<T>::isSigned,
                "neg() is only available for signed types");
  return internal::base::neg(a);
}

// 25. Mar 23 (Jonas Keller): added integer version of ceil, floor, round,
// truncate and unsigned version of abs

/**
 * @brief Computes the absolute value of the elements of a Vec.
 *
 * For unsigned types, this function simply returns the input Vec.
 *
 * @param a Vec to compute the absolute value of
 * @return Vec containing the absolute value of the elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> abs(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::abs(a);
}

/**
 * @brief Rounds the elements of a Vec up to the nearest integer.
 *
 * For integer types, this function simply returns the input Vec.
 *
 * @param a Vec to round
 * @return Vec containing the rounded elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> ceil(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::ceil(a);
}

/**
 * @brief Rounds the elements of a Vec down to the nearest
 * integer.
 *
 * For integer types, this function simply returns the input Vec.
 *
 * @param a Vec to round
 * @return Vec containing the rounded elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> floor(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::floor(a);
}

/**
 * @brief Rounds the elements of a Vec to the nearest integer.
 *
 * For integer types, this function simply returns the input Vec.
 *
 * @param a Vec to round
 * @return Vec containing the rounded elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> round(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::round(a);
}

/**
 * @brief Truncates the elements of a Vec to the nearest integer
 * i.e. rounds towards zero.
 *
 * For integer types, this function simply returns the input Vec.
 *
 * @param a Vec to truncate
 * @return Vec containing the truncated elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> truncate(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::truncate(a);
}

/** @} */

/**
 * @addtogroup group_logic
 * @{
 */

/**
 * @brief Computes the bitwise AND of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the bitwise AND of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_and(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::bit_and(a, b);
}

/**
 * @brief Computes the bitwise OR of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the bitwise OR of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_or(const Vec<T, SIMD_WIDTH> &a,
                                             const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::bit_or(a, b);
}

/**
 * @brief Computes the bitwise ANDNOT of two Vec's.
 *
 * The result is computed as (not @p a ) and @p b .
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the bitwise ANDNOT of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_andnot(const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::bit_andnot(a, b);
}

/**
 * @brief Computes the bitwise XOR of two Vec's.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the bitwise XOR of the two input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_xor(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::bit_xor(a, b);
}

/**
 * @brief Computes the bitwise NOT of a Vec.
 *
 * @param a Vec
 * @return Vec containing the bitwise NOT of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> bit_not(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::bit_not(a);
}

/** @} */

/**
 * @addtogroup group_shift
 * @{
 */

/**
 * @brief Shifts the elements of a Vec right by a constant
 * number of bits while shifting in the sign bit.
 *
 * @sa sra()
 *
 * @tparam COUNT number of bits to shift by
 * @param a Vec to shift
 * @return Vec containing the shifted elements of the input Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srai(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::srai<COUNT>(a);
}

/**
 * @brief Shifts the elements of a Vec right by a constant
 * number of bits while shifting in zeros.
 *
 * @sa srl()
 *
 * @tparam COUNT number of bits to shift by
 * @param a Vec to shift
 * @return Vec containing the shifted elements of the input Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srli(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::srli<COUNT>(a);
}

/**
 * @brief Shifts the elements of a Vec left by a constant
 * number of bits while shifting in zeros.
 *
 * @sa sll()
 *
 * @tparam COUNT number of bits to shift by
 * @param a Vec to shift
 * @return Vec containing the shifted elements of the input Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> slli(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::slli<COUNT>(a);
}

// 12. Jan 23 (Jonas Keller): added sra, srl and sll functions

/**
 * @brief Shifts the elements of a Vec right by a variable
 * number of bits while shifting in the sign bit.
 *
 * @sa srai()
 *
 * @param a Vec to shift
 * @param count number of bits to shift by
 * @return Vec containing the shifted elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sra(const Vec<T, SIMD_WIDTH> &a,
                                          const uint8_t count)
{
  return internal::base::sra(a, count);
}

/**
 * @brief Shifts the elements of a Vec right by a variable
 * number of bits while shifting in zeros.
 *
 * @sa srli()
 *
 * @param a Vec to shift
 * @param count number of bits to shift by
 * @return Vec containing the shifted elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srl(const Vec<T, SIMD_WIDTH> &a,
                                          const uint8_t count)
{
  return internal::base::srl(a, count);
}

/**
 * @brief Shifts the elements of a Vec left by a variable
 * number of bits while shifting in zeros.
 *
 * @sa slli()
 *
 * @param a Vec to shift
 * @param count number of bits to shift by
 * @return Vec containing the shifted elements of the input Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sll(const Vec<T, SIMD_WIDTH> &a,
                                          const uint8_t count)
{
  return internal::base::sll(a, count);
}

/** @} */

/**
 * @addtogroup group_cmp
 * @{
 */

/**
 * @brief Compares corresponding elements of two Vec's for less-than
 * ( @c < ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmplt(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmplt(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmplt(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmplt(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * less-than-or-equal ( @c <= ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmple(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmple(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmple(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmple(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * equality ( @c == ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmpeq(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmpeq(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpeq(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmpeq(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * greater-than-or-equal ( @c >= ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmpge(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmpge(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpge(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmpge(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * greater-than ( @c > ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmpgt(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmpgt(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpgt(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmpgt(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * inequality ( @c != ).
 *
 * @param a first Vec to compare
 * @param b second Vec to compare
 * @return Vec containing the results of the comparison, where each element is
 * all 1 bits or all 0 bits, depending on whether the corresponding comparison
 * returned true or false, respectively
 * @sa mask_cmpne(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 * @sa mask_cmpne(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> cmpneq(const Vec<T, SIMD_WIDTH> &a,
                                             const Vec<T, SIMD_WIDTH> &b)
{
  return internal::base::cmpneq(a, b);
}

/**
 * @brief Tests if all bits of a Vec are zero.
 *
 * @param a Vec to test
 * @return true if all bits are zero, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool test_all_zeros(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::test_all_zeros(a);
}

/**
 * @brief Tests if all bits of a Vec are one.
 *
 * @param a Vec to test
 * @return true if all bits are one, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool test_all_ones(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::test_all_ones(a);
}

/** @} */

/**
 * @ingroup group_extract
 * @brief Extracts a single value from a Vec.
 *
 * Returns 0 if @p INDEX is out of range.
 *
 * @tparam INDEX index of the value to extract
 * @param a Vec to extract from
 * @return extracted value
 */
template <size_t INDEX, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T extract(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::extract<INDEX>(a);
}

/**
 * @ingroup group_extract
 * @brief Extracts a 16-byte lane from a Vec as a Vec < T, 16 >.
 *
 * @tparam LANE_INDEX lane to extract, must be less than @p SIMD_WIDTH / 16
 * @param a Vec to extract a lane from
 * @return extracted lane as a Vec < T, 16 >
 */
template <size_t LANE_INDEX, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, 16> extractLane(const Vec<T, SIMD_WIDTH> &a)
{
  static_assert(LANE_INDEX < SIMD_WIDTH / 16,
                "LANE_INDEX must be less than SIMD_WIDTH / 16");
  return internal::base::extractLane<LANE_INDEX>(a);
}

/**
 * @ingroup group_reordering
 * @brief Reverses the order of the elements of a Vec.
 *
 * @param a Vec to reverse
 * @return Vec containing the elements of the input Vec in reverse order
 */
template <typename T, size_t SIMD_WIDTH>
static Vec<T, SIMD_WIDTH> reverse(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::reverse(a);
}

/**
 * @ingroup group_swizzle
 * @brief Swizzle/de-interleave/convert from AoS to SoA multiple Vec's in-place.
 *
 * This function swizzles/de-interleaves/converts from AoS (Array of Structs) to
 * SoA (Struct of Arrays) multiple Vec's in-place.
 *
 * <h4>Example:</h4>
 * Example for a swizzle distance of 3 with 3 Vec's of 8 elements each:
 *
 * input stream (structures indicated by curly brackets):
 * @code
 * {0 1 2} {3 4 5} {6 7 8} {9 10 11} ... {21 22 23}
 * @endcode
 * input vectors:
 * @code
 * v[0] =  0  1  2  3  4  5  6  7
 * v[1] =  8  9 10 11 12 13 14 15
 * v[2] = 16 17 18 19 20 21 22 23
 * @endcode
 * output vectors:
 * @code
 * v[0] =  0  3  6  9 12 15 18 21
 * v[1] =  1  4  7 10 13 16 19 22
 * v[2] =  2  5  8 11 14 17 20 23
 * @endcode
 *
 * @tparam N swizzle distance, must be between 1 and 5
 * @param[in,out] v array of Vec's to swizzle
 *
 * @sa swizzle2(): %swizzle function that takes double the number of Vec's with
 * potentially better performance
 * @sa unswizzle()
 */
template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle(Vec<T, SIMD_WIDTH> v[N])
{
  static_assert(N >= 1 && N <= 5, "N must be between 1 and 5");
  internal::base::swizzle(v, internal::Integer<N>());
}

/**
 * @ingroup group_zip_unpack
 * @brief Interleaves blocks of elements from the high or low half of two
 * Vec's.
 *
 * This function interleaves blocks of elements from the high or low
 * half of two Vec's, starting with the lowest block of the
 * selected half of the first input Vec.
 *
 * To get both halves of the input Vec's interleaved, use zip().
 *
 * Example: TODO?
 *
 * @tparam PART selects which half of the input Vec's to use. A value of 0
 * selects the low half, a value of 1 selects the high half
 * @tparam NUM_ELEMS number of elements in a block, this must be a power of two
 * (including 1) and be at most half of one Vec
 * @param a first input Vec
 * @param b second input Vec
 * @return interleaved Vec
 */
template <size_t PART, size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> unpack(const Vec<T, SIMD_WIDTH> &a,
                                             const Vec<T, SIMD_WIDTH> &b)
{
  static_assert(PART == 0 || PART == 1, "PART must be 0 or 1");
  static_assert(NUM_ELEMS <= Vec<T, SIMD_WIDTH>::elements / 2,
                "NUM_ELEMS must be at most half of one Vec");
  static_assert(NUM_ELEMS > 0 && (NUM_ELEMS & (NUM_ELEMS - 1)) == 0,
                "NUM_ELEMS must be a power of two");
  return internal::base::unpack(a, b, internal::Part<PART>(),
                                internal::Bytes<NUM_ELEMS * sizeof(T)>());
}

/**
 * @ingroup group_zip_unpack
 * @brief Interleaves blocks of elements from the high or low half of each
 * 16-byte lane of two Vec's.
 *
 * This function interleaves blocks of elements from the high or low
 * half of each 16-byte lane of two Vec's, starting with the
 * lowest block of the selected half of the first input Vec.
 *
 * This function is the lane-oriented equivalent of unpack().
 *
 * If the blocks of elements to be interleaved are larger than half of a 16-byte
 * lane the behavior of this function is undefined.
 *
 * To get both halves of the input Vec's interleaved, use zip16().
 *
 * Example: TODO?
 *
 * @tparam PART selects which half of the lanes of the input Vec's to use. A
 * value of 0 selects the low half, a value of 1 selects the high half
 * @tparam NUM_ELEMS number of elements in a block, must be a power of two
 * (including 1) and be at most half of one 16-byte lane of one Vec (i.e. at
 * most 8 bytes)
 * @param a first input Vec
 * @param b second input Vec
 * @return interleaved Vec
 */
template <size_t PART, size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> unpack16(const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  static_assert(PART == 0 || PART == 1, "PART must be 0 or 1");
  static_assert(NUM_ELEMS <= Vec<T, SIMD_WIDTH>::elements / 2,
                "NUM_ELEMS must be at most half of one Vec");
  static_assert(NUM_ELEMS > 0 && (NUM_ELEMS & (NUM_ELEMS - 1)) == 0,
                "NUM_ELEMS must be a power of two");
  return internal::base::unpack16(a, b, internal::Part<PART>(),
                                  internal::Bytes<NUM_ELEMS * sizeof(T)>());
}

/**
 * @ingroup group_zip_unpack
 * @brief Interleaves blocks of elements of two Vec's.
 *
 * This function interleaves blocks of elements from two
 * Vec's, starting with the lowest block of the first input
 * Vec. The interleaved blocks are returned in two output
 * Vec's.
 *
 * This function is semantically equivalent to calling unpack() twice for
 * both parts, but may be more efficient on some platforms (such as arm
 * NEON).
 *
 * Example: TODO?
 *
 * @sa unpack()
 * @sa unzip()
 *
 * @tparam NUM_ELEMS number of elements in a block, must be a power of two
 * (including 1) and be at most half of one Vec
 * @param a first input Vec
 * @param b second input Vec
 * @param[out] l output Vec containing the low half of the interleaved blocks
 * @param[out] h output Vec containing the high half of the interleaved blocks
 */
template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void zip(const Vec<T, SIMD_WIDTH> a,
                            const Vec<T, SIMD_WIDTH> b, Vec<T, SIMD_WIDTH> &l,
                            Vec<T, SIMD_WIDTH> &h)
{
  static_assert(NUM_ELEMS <= Vec<T, SIMD_WIDTH>::elements / 2,
                "NUM_ELEMS must be at most half of one Vec");
  static_assert(NUM_ELEMS > 0 && (NUM_ELEMS & (NUM_ELEMS - 1)) == 0,
                "NUM_ELEMS must be a power of two");
  return internal::base::zip<NUM_ELEMS>(a, b, l, h);
}

/**
 * @ingroup group_zip_unpack
 * @brief Interleaves blocks of elements of each 16-byte lane of two
 * Vec's.
 *
 * This function interleaves blocks of elements from the high or low
 * half of each 16-byte lane of two Vec's, starting with the
 * lowest block of the selected half of the first input Vec. The
 * interleaved blocks are returned in two output Vec's.
 *
 * This function is the lane-oriented equivalent of zip().
 *
 * This function is semantically equivalent to calling unpack16() twice for
 * both parts, but may be more efficient on some platforms (such as arm
 * NEON).
 *
 * Example: TODO?
 *
 * @sa unpack16()
 *
 * @tparam NUM_ELEMS number of elements in a block, must be a power of two
 * (including 1) and be at most half of one Vec
 * @param a first input Vec
 * @param b second input Vec
 * @param[out] l output Vec containing the low half of the interleaved blocks
 * @param[out] h output Vec containing the high half of the interleaved blocks
 */
template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void zip16(const Vec<T, SIMD_WIDTH> a,
                              const Vec<T, SIMD_WIDTH> b, Vec<T, SIMD_WIDTH> &l,
                              Vec<T, SIMD_WIDTH> &h)
{
  static_assert(NUM_ELEMS <= Vec<T, SIMD_WIDTH>::elements / 2,
                "NUM_ELEMS must be at most half of one Vec");
  static_assert(NUM_ELEMS > 0 && (NUM_ELEMS & (NUM_ELEMS - 1)) == 0,
                "NUM_ELEMS must be a power of two");
  return internal::base::zip16<NUM_ELEMS>(a, b, l, h);
}

/**
 * @ingroup group_zip_unpack
 * @brief Deinterleaves blocks of elements two Vec's.
 *
 * This function deinterleaves blocks of elements from two
 * Vec's. This is the inverse of zip().
 *
 * Example: TODO?
 *
 * @sa zip()
 *
 * @tparam NUM_ELEMS number of elements in a block, must be a power of two
 * (including 1) and be at most half of one Vec
 * @param a first input Vec
 * @param b second input Vec
 * @param[out] l output Vec containing the low half of the deinterleaved blocks
 * @param[out] h output Vec containing the high half of the deinterleaved blocks
 */
template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unzip(const Vec<T, SIMD_WIDTH> a,
                              const Vec<T, SIMD_WIDTH> b, Vec<T, SIMD_WIDTH> &l,
                              Vec<T, SIMD_WIDTH> &h)
{
  static_assert(NUM_ELEMS <= Vec<T, SIMD_WIDTH>::elements / 2,
                "NUM_ELEMS must be at most half of one Vec");
  static_assert(NUM_ELEMS > 0 && (NUM_ELEMS & (NUM_ELEMS - 1)) == 0,
                "NUM_ELEMS must be a power of two");
  return internal::base::unzip(a, b, l, h,
                               internal::Bytes<NUM_ELEMS * sizeof(T)>());
}

/**
 * @ingroup group_elementwise_shift
 * @brief Shifts a Vec right by a constant number of elements,
 * shifting in zero elements.
 *
 * @tparam COUNT number of elements to shift by
 * @param a Vec to shift
 * @return shifted Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> srle(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::srle<COUNT>(a);
}

/**
 * @ingroup group_elementwise_shift
 * @brief Shifts a Vec left by a constant number of elements,
 * shifting in zero elements.
 *
 * @tparam COUNT number of elements to shift by
 * @param a Vec to shift
 * @return shifted Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> slle(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::base::slle<COUNT>(a);
}

/**
 * @ingroup group_elementwise_shift
 * @brief Concatenates two Vec's, shifts the result right by a
 * constant number of elements, and returns the low half of the
 * result.
 *
 * @tparam COUNT number of elements to shift by
 * @param h first Vec The high half of the concatenated Vec
 * @param l second Vec The low half of the concatenated Vec
 * @return low half of the shifted concatenated Vec
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> alignre(const Vec<T, SIMD_WIDTH> &h,
                                              const Vec<T, SIMD_WIDTH> &l)
{
  return internal::base::alignre<COUNT>(h, l);
}

/**
 * @addtogroup group_type_conversion
 * @{
 */

/**
 * @brief Packs two Vec's into one by converting the elements
 * into the next smaller type with saturation.
 *
 * Float is converted to an integer type, as there
 * is no 16-bit floating point type.
 *
 * @sa extend()
 *
 * @tparam Tout type of the resulting Vec, must be the next smaller type of the
 * elements of the input Vec's
 * @tparam Tin type of the input Vec's
 * @param a, b The input Vec's.
 * @return Vec with the elements of a and b packed into one
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> &a,
                                               const Vec<Tin, SIMD_WIDTH> &b)
{
  return internal::base::packs(a, b, internal::OutputType<Tout>());
}

/**
 * @brief Extends the elements of a Vec to a larger or equally
 * sized type.
 *
 * The values are zero-extended or sign-extended, depending on the type.
 *
 * When converting from a signed to an unsigned type, negative values are
 * saturated to zero.
 *
 * Multiple output Vec's are produced, where the amount of output
 * Vec's is <tt>sizeof(Tout) / sizeof(Tin)</tt>.
 *
 * @sa packs()
 *
 * @tparam Tout type to extend to
 * @tparam Tin type to extend from
 * @param vIn input Vec
 * @param[out] vOut output Vec's
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void extend(
  const Vec<Tin, SIMD_WIDTH> &vIn,
  Vec<Tout, SIMD_WIDTH> vOut[sizeof(Tout) / sizeof(Tin)])
{
  return internal::base::extend(vIn, vOut);
}

/**
 * @brief Converts the elements of a Vec between integer and floating point
 * types of the same size.
 *
 * The conversion from floating point type to integer is saturated.
 *
 * @tparam Tout type of the resulting Vec, must be the same size as @p Tin
 * and must be a floating point type if @p Tin is an integer type, and vice
 * versa
 * @tparam Tin type of the input Vec, must be the same size as @p Tout
 * and must be a floating point type if @p Tout is an integer type, and vice
 * versa
 * @param a input Vec
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> cvts(const Vec<Tin, SIMD_WIDTH> &a)
{
  static_assert(sizeof(Tout) == sizeof(Tin),
                "Tout and Tin must be the same size");
  static_assert(std::is_floating_point<Tout>::value !=
                  std::is_floating_point<Tin>::value,
                "exactly one of Tout and Tin must be a floating point type");
  return internal::base::cvts(a, internal::OutputType<Tout>());
}

/** @} */
} // namespace simd

#endif // SIMD_VEC_BASE_H_

// generic templates for extension commands
// ===========================================================================
//
// SIMDVecExt.H --
// extension commands combining multiple 1st-level vector template functions
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace

// 03. Mar 23 (Jonas Keller): removed hsub and hsubs extensions, as they do not
// do something useful

// 09. Mar 23 (Jonas Keller): added doxygen documentation

// 03. Aug 23 (Jonas Keller): renamed all swizzle2, unswizzle and transpose
// versions to sequential names <base_name>_a, <base_name>_b, <base_name>_c,
// etc. and moved them into the internal namespace and added a hub function
// <base_name> that wraps the fastest version

// 13. May 23 (Jonas Keller): added Double support

#ifndef SIMD_VEC_EXT_H_
#define SIMD_VEC_EXT_H_

// ===========================================================================
//
// transpose_inplace_autogen.H --
// auto-generated transpose functions with in-place processing
// DO NOT EDIT!!!
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SIMD_VEC_EXT_TRANSPOSE_AUTOGEN_H_
#define SIMD_VEC_EXT_TRANSPOSE_AUTOGEN_H_

#include <utility>

namespace simd {
namespace internal {
namespace ext {
// ==========================================================
// transpose1inplc
// ==========================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<2>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  std::swap(outRows[1], outRows[2]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  std::swap(outRows[1], outRows[4]);
  std::swap(outRows[3], outRows[6]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  std::swap(outRows[1], outRows[8]);
  std::swap(outRows[2], outRows[4]);
  std::swap(outRows[3], outRows[12]);
  std::swap(outRows[5], outRows[10]);
  std::swap(outRows[7], outRows[14]);
  std::swap(outRows[11], outRows[13]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip<1>(inRows[16], inRows[17], outRows[16], outRows[17]);
  zip<1>(inRows[18], inRows[19], outRows[18], outRows[19]);
  zip<1>(inRows[20], inRows[21], outRows[20], outRows[21]);
  zip<1>(inRows[22], inRows[23], outRows[22], outRows[23]);
  zip<1>(inRows[24], inRows[25], outRows[24], outRows[25]);
  zip<1>(inRows[26], inRows[27], outRows[26], outRows[27]);
  zip<1>(inRows[28], inRows[29], outRows[28], outRows[29]);
  zip<1>(inRows[30], inRows[31], outRows[30], outRows[31]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<4>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip<4>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip<4>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip<4>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip<4>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip<4>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip<4>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip<4>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip<8>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip<8>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip<8>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip<8>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip<8>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip<8>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip<8>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip<8>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip<16>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<16>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<16>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<16>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<16>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<16>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<16>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<16>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<16>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<16>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<16>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<16>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<16>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<16>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<16>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<16>(outRows[15], outRows[31], outRows[15], outRows[31]);
  std::swap(outRows[1], outRows[16]);
  std::swap(outRows[2], outRows[8]);
  std::swap(outRows[3], outRows[24]);
  std::swap(outRows[5], outRows[20]);
  std::swap(outRows[6], outRows[12]);
  std::swap(outRows[7], outRows[28]);
  std::swap(outRows[9], outRows[18]);
  std::swap(outRows[11], outRows[26]);
  std::swap(outRows[13], outRows[22]);
  std::swap(outRows[15], outRows[30]);
  std::swap(outRows[19], outRows[25]);
  std::swap(outRows[23], outRows[29]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<64>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip<1>(inRows[16], inRows[17], outRows[16], outRows[17]);
  zip<1>(inRows[18], inRows[19], outRows[18], outRows[19]);
  zip<1>(inRows[20], inRows[21], outRows[20], outRows[21]);
  zip<1>(inRows[22], inRows[23], outRows[22], outRows[23]);
  zip<1>(inRows[24], inRows[25], outRows[24], outRows[25]);
  zip<1>(inRows[26], inRows[27], outRows[26], outRows[27]);
  zip<1>(inRows[28], inRows[29], outRows[28], outRows[29]);
  zip<1>(inRows[30], inRows[31], outRows[30], outRows[31]);
  zip<1>(inRows[32], inRows[33], outRows[32], outRows[33]);
  zip<1>(inRows[34], inRows[35], outRows[34], outRows[35]);
  zip<1>(inRows[36], inRows[37], outRows[36], outRows[37]);
  zip<1>(inRows[38], inRows[39], outRows[38], outRows[39]);
  zip<1>(inRows[40], inRows[41], outRows[40], outRows[41]);
  zip<1>(inRows[42], inRows[43], outRows[42], outRows[43]);
  zip<1>(inRows[44], inRows[45], outRows[44], outRows[45]);
  zip<1>(inRows[46], inRows[47], outRows[46], outRows[47]);
  zip<1>(inRows[48], inRows[49], outRows[48], outRows[49]);
  zip<1>(inRows[50], inRows[51], outRows[50], outRows[51]);
  zip<1>(inRows[52], inRows[53], outRows[52], outRows[53]);
  zip<1>(inRows[54], inRows[55], outRows[54], outRows[55]);
  zip<1>(inRows[56], inRows[57], outRows[56], outRows[57]);
  zip<1>(inRows[58], inRows[59], outRows[58], outRows[59]);
  zip<1>(inRows[60], inRows[61], outRows[60], outRows[61]);
  zip<1>(inRows[62], inRows[63], outRows[62], outRows[63]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip<2>(outRows[32], outRows[34], outRows[32], outRows[34]);
  zip<2>(outRows[33], outRows[35], outRows[33], outRows[35]);
  zip<2>(outRows[36], outRows[38], outRows[36], outRows[38]);
  zip<2>(outRows[37], outRows[39], outRows[37], outRows[39]);
  zip<2>(outRows[40], outRows[42], outRows[40], outRows[42]);
  zip<2>(outRows[41], outRows[43], outRows[41], outRows[43]);
  zip<2>(outRows[44], outRows[46], outRows[44], outRows[46]);
  zip<2>(outRows[45], outRows[47], outRows[45], outRows[47]);
  zip<2>(outRows[48], outRows[50], outRows[48], outRows[50]);
  zip<2>(outRows[49], outRows[51], outRows[49], outRows[51]);
  zip<2>(outRows[52], outRows[54], outRows[52], outRows[54]);
  zip<2>(outRows[53], outRows[55], outRows[53], outRows[55]);
  zip<2>(outRows[56], outRows[58], outRows[56], outRows[58]);
  zip<2>(outRows[57], outRows[59], outRows[57], outRows[59]);
  zip<2>(outRows[60], outRows[62], outRows[60], outRows[62]);
  zip<2>(outRows[61], outRows[63], outRows[61], outRows[63]);
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<4>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip<4>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip<4>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip<4>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip<4>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip<4>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip<4>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip<4>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip<4>(outRows[32], outRows[36], outRows[32], outRows[36]);
  zip<4>(outRows[34], outRows[38], outRows[34], outRows[38]);
  zip<4>(outRows[33], outRows[37], outRows[33], outRows[37]);
  zip<4>(outRows[35], outRows[39], outRows[35], outRows[39]);
  zip<4>(outRows[40], outRows[44], outRows[40], outRows[44]);
  zip<4>(outRows[42], outRows[46], outRows[42], outRows[46]);
  zip<4>(outRows[41], outRows[45], outRows[41], outRows[45]);
  zip<4>(outRows[43], outRows[47], outRows[43], outRows[47]);
  zip<4>(outRows[48], outRows[52], outRows[48], outRows[52]);
  zip<4>(outRows[50], outRows[54], outRows[50], outRows[54]);
  zip<4>(outRows[49], outRows[53], outRows[49], outRows[53]);
  zip<4>(outRows[51], outRows[55], outRows[51], outRows[55]);
  zip<4>(outRows[56], outRows[60], outRows[56], outRows[60]);
  zip<4>(outRows[58], outRows[62], outRows[58], outRows[62]);
  zip<4>(outRows[57], outRows[61], outRows[57], outRows[61]);
  zip<4>(outRows[59], outRows[63], outRows[59], outRows[63]);
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip<8>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip<8>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip<8>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip<8>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip<8>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip<8>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip<8>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip<8>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip<8>(outRows[32], outRows[40], outRows[32], outRows[40]);
  zip<8>(outRows[36], outRows[44], outRows[36], outRows[44]);
  zip<8>(outRows[34], outRows[42], outRows[34], outRows[42]);
  zip<8>(outRows[38], outRows[46], outRows[38], outRows[46]);
  zip<8>(outRows[33], outRows[41], outRows[33], outRows[41]);
  zip<8>(outRows[37], outRows[45], outRows[37], outRows[45]);
  zip<8>(outRows[35], outRows[43], outRows[35], outRows[43]);
  zip<8>(outRows[39], outRows[47], outRows[39], outRows[47]);
  zip<8>(outRows[48], outRows[56], outRows[48], outRows[56]);
  zip<8>(outRows[52], outRows[60], outRows[52], outRows[60]);
  zip<8>(outRows[50], outRows[58], outRows[50], outRows[58]);
  zip<8>(outRows[54], outRows[62], outRows[54], outRows[62]);
  zip<8>(outRows[49], outRows[57], outRows[49], outRows[57]);
  zip<8>(outRows[53], outRows[61], outRows[53], outRows[61]);
  zip<8>(outRows[51], outRows[59], outRows[51], outRows[59]);
  zip<8>(outRows[55], outRows[63], outRows[55], outRows[63]);
  zip<16>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<16>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<16>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<16>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<16>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<16>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<16>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<16>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<16>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<16>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<16>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<16>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<16>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<16>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<16>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<16>(outRows[15], outRows[31], outRows[15], outRows[31]);
  zip<16>(outRows[32], outRows[48], outRows[32], outRows[48]);
  zip<16>(outRows[40], outRows[56], outRows[40], outRows[56]);
  zip<16>(outRows[36], outRows[52], outRows[36], outRows[52]);
  zip<16>(outRows[44], outRows[60], outRows[44], outRows[60]);
  zip<16>(outRows[34], outRows[50], outRows[34], outRows[50]);
  zip<16>(outRows[42], outRows[58], outRows[42], outRows[58]);
  zip<16>(outRows[38], outRows[54], outRows[38], outRows[54]);
  zip<16>(outRows[46], outRows[62], outRows[46], outRows[62]);
  zip<16>(outRows[33], outRows[49], outRows[33], outRows[49]);
  zip<16>(outRows[41], outRows[57], outRows[41], outRows[57]);
  zip<16>(outRows[37], outRows[53], outRows[37], outRows[53]);
  zip<16>(outRows[45], outRows[61], outRows[45], outRows[61]);
  zip<16>(outRows[35], outRows[51], outRows[35], outRows[51]);
  zip<16>(outRows[43], outRows[59], outRows[43], outRows[59]);
  zip<16>(outRows[39], outRows[55], outRows[39], outRows[55]);
  zip<16>(outRows[47], outRows[63], outRows[47], outRows[63]);
  zip<32>(outRows[0], outRows[32], outRows[0], outRows[32]);
  zip<32>(outRows[16], outRows[48], outRows[16], outRows[48]);
  zip<32>(outRows[8], outRows[40], outRows[8], outRows[40]);
  zip<32>(outRows[24], outRows[56], outRows[24], outRows[56]);
  zip<32>(outRows[4], outRows[36], outRows[4], outRows[36]);
  zip<32>(outRows[20], outRows[52], outRows[20], outRows[52]);
  zip<32>(outRows[12], outRows[44], outRows[12], outRows[44]);
  zip<32>(outRows[28], outRows[60], outRows[28], outRows[60]);
  zip<32>(outRows[2], outRows[34], outRows[2], outRows[34]);
  zip<32>(outRows[18], outRows[50], outRows[18], outRows[50]);
  zip<32>(outRows[10], outRows[42], outRows[10], outRows[42]);
  zip<32>(outRows[26], outRows[58], outRows[26], outRows[58]);
  zip<32>(outRows[6], outRows[38], outRows[6], outRows[38]);
  zip<32>(outRows[22], outRows[54], outRows[22], outRows[54]);
  zip<32>(outRows[14], outRows[46], outRows[14], outRows[46]);
  zip<32>(outRows[30], outRows[62], outRows[30], outRows[62]);
  zip<32>(outRows[1], outRows[33], outRows[1], outRows[33]);
  zip<32>(outRows[17], outRows[49], outRows[17], outRows[49]);
  zip<32>(outRows[9], outRows[41], outRows[9], outRows[41]);
  zip<32>(outRows[25], outRows[57], outRows[25], outRows[57]);
  zip<32>(outRows[5], outRows[37], outRows[5], outRows[37]);
  zip<32>(outRows[21], outRows[53], outRows[21], outRows[53]);
  zip<32>(outRows[13], outRows[45], outRows[13], outRows[45]);
  zip<32>(outRows[29], outRows[61], outRows[29], outRows[61]);
  zip<32>(outRows[3], outRows[35], outRows[3], outRows[35]);
  zip<32>(outRows[19], outRows[51], outRows[19], outRows[51]);
  zip<32>(outRows[11], outRows[43], outRows[11], outRows[43]);
  zip<32>(outRows[27], outRows[59], outRows[27], outRows[59]);
  zip<32>(outRows[7], outRows[39], outRows[7], outRows[39]);
  zip<32>(outRows[23], outRows[55], outRows[23], outRows[55]);
  zip<32>(outRows[15], outRows[47], outRows[15], outRows[47]);
  zip<32>(outRows[31], outRows[63], outRows[31], outRows[63]);
  std::swap(outRows[1], outRows[32]);
  std::swap(outRows[2], outRows[16]);
  std::swap(outRows[3], outRows[48]);
  std::swap(outRows[4], outRows[8]);
  std::swap(outRows[5], outRows[40]);
  std::swap(outRows[6], outRows[24]);
  std::swap(outRows[7], outRows[56]);
  std::swap(outRows[9], outRows[36]);
  std::swap(outRows[10], outRows[20]);
  std::swap(outRows[11], outRows[52]);
  std::swap(outRows[13], outRows[44]);
  std::swap(outRows[14], outRows[28]);
  std::swap(outRows[15], outRows[60]);
  std::swap(outRows[17], outRows[34]);
  std::swap(outRows[19], outRows[50]);
  std::swap(outRows[21], outRows[42]);
  std::swap(outRows[22], outRows[26]);
  std::swap(outRows[23], outRows[58]);
  std::swap(outRows[25], outRows[38]);
  std::swap(outRows[27], outRows[54]);
  std::swap(outRows[29], outRows[46]);
  std::swap(outRows[31], outRows[62]);
  std::swap(outRows[35], outRows[49]);
  std::swap(outRows[37], outRows[41]);
  std::swap(outRows[39], outRows[57]);
  std::swap(outRows[43], outRows[53]);
  std::swap(outRows[47], outRows[61]);
  std::swap(outRows[55], outRows[59]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  transpose1inplc(inRows, outRows, Elements<Vec<T, SIMD_WIDTH>::elements>());
}

// ==========================================================
// transpose1inplcLane
// ==========================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<16>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  std::swap(outRows[1], outRows[8]);
  std::swap(outRows[2], outRows[4]);
  std::swap(outRows[3], outRows[12]);
  std::swap(outRows[5], outRows[10]);
  std::swap(outRows[7], outRows[14]);
  std::swap(outRows[11], outRows[13]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  std::swap(outRows[1], outRows[4]);
  std::swap(outRows[3], outRows[6]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  std::swap(outRows[1], outRows[2]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<2>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>,
  Bytes<32>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<1>(inRows[16], inRows[17], outRows[16], outRows[17]);
  zip16<1>(inRows[18], inRows[19], outRows[18], outRows[19]);
  zip16<1>(inRows[20], inRows[21], outRows[20], outRows[21]);
  zip16<1>(inRows[22], inRows[23], outRows[22], outRows[23]);
  zip16<1>(inRows[24], inRows[25], outRows[24], outRows[25]);
  zip16<1>(inRows[26], inRows[27], outRows[26], outRows[27]);
  zip16<1>(inRows[28], inRows[29], outRows[28], outRows[29]);
  zip16<1>(inRows[30], inRows[31], outRows[30], outRows[31]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip16<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip16<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip16<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip16<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip16<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip16<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip16<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<4>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<4>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<4>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<4>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<4>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<4>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<4>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<4>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip16<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip16<8>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip16<8>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip16<8>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip16<8>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip16<8>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip16<8>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip16<8>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip16<8>(outRows[23], outRows[31], outRows[23], outRows[31]);
  std::swap(outRows[1], outRows[8]);
  std::swap(outRows[2], outRows[4]);
  std::swap(outRows[3], outRows[12]);
  std::swap(outRows[5], outRows[10]);
  std::swap(outRows[7], outRows[14]);
  std::swap(outRows[11], outRows[13]);
  std::swap(outRows[17], outRows[24]);
  std::swap(outRows[18], outRows[20]);
  std::swap(outRows[19], outRows[28]);
  std::swap(outRows[21], outRows[26]);
  std::swap(outRows[23], outRows[30]);
  std::swap(outRows[27], outRows[29]);
  // correction steps follow below (if required)
  zip<16>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<16>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<16>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<16>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<16>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<16>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<16>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<16>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<16>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<16>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<16>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<16>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<16>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<16>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<16>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<16>(outRows[15], outRows[31], outRows[15], outRows[31]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<32>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  std::swap(outRows[1], outRows[4]);
  std::swap(outRows[3], outRows[6]);
  std::swap(outRows[9], outRows[12]);
  std::swap(outRows[11], outRows[14]);
  // correction steps follow below (if required)
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<32>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  std::swap(outRows[1], outRows[2]);
  std::swap(outRows[5], outRows[6]);
  // correction steps follow below (if required)
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>, Bytes<32>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  // correction steps follow below (if required)
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<64>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<1>(inRows[16], inRows[17], outRows[16], outRows[17]);
  zip16<1>(inRows[18], inRows[19], outRows[18], outRows[19]);
  zip16<1>(inRows[20], inRows[21], outRows[20], outRows[21]);
  zip16<1>(inRows[22], inRows[23], outRows[22], outRows[23]);
  zip16<1>(inRows[24], inRows[25], outRows[24], outRows[25]);
  zip16<1>(inRows[26], inRows[27], outRows[26], outRows[27]);
  zip16<1>(inRows[28], inRows[29], outRows[28], outRows[29]);
  zip16<1>(inRows[30], inRows[31], outRows[30], outRows[31]);
  zip16<1>(inRows[32], inRows[33], outRows[32], outRows[33]);
  zip16<1>(inRows[34], inRows[35], outRows[34], outRows[35]);
  zip16<1>(inRows[36], inRows[37], outRows[36], outRows[37]);
  zip16<1>(inRows[38], inRows[39], outRows[38], outRows[39]);
  zip16<1>(inRows[40], inRows[41], outRows[40], outRows[41]);
  zip16<1>(inRows[42], inRows[43], outRows[42], outRows[43]);
  zip16<1>(inRows[44], inRows[45], outRows[44], outRows[45]);
  zip16<1>(inRows[46], inRows[47], outRows[46], outRows[47]);
  zip16<1>(inRows[48], inRows[49], outRows[48], outRows[49]);
  zip16<1>(inRows[50], inRows[51], outRows[50], outRows[51]);
  zip16<1>(inRows[52], inRows[53], outRows[52], outRows[53]);
  zip16<1>(inRows[54], inRows[55], outRows[54], outRows[55]);
  zip16<1>(inRows[56], inRows[57], outRows[56], outRows[57]);
  zip16<1>(inRows[58], inRows[59], outRows[58], outRows[59]);
  zip16<1>(inRows[60], inRows[61], outRows[60], outRows[61]);
  zip16<1>(inRows[62], inRows[63], outRows[62], outRows[63]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip16<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip16<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip16<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip16<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip16<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip16<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip16<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip16<2>(outRows[32], outRows[34], outRows[32], outRows[34]);
  zip16<2>(outRows[33], outRows[35], outRows[33], outRows[35]);
  zip16<2>(outRows[36], outRows[38], outRows[36], outRows[38]);
  zip16<2>(outRows[37], outRows[39], outRows[37], outRows[39]);
  zip16<2>(outRows[40], outRows[42], outRows[40], outRows[42]);
  zip16<2>(outRows[41], outRows[43], outRows[41], outRows[43]);
  zip16<2>(outRows[44], outRows[46], outRows[44], outRows[46]);
  zip16<2>(outRows[45], outRows[47], outRows[45], outRows[47]);
  zip16<2>(outRows[48], outRows[50], outRows[48], outRows[50]);
  zip16<2>(outRows[49], outRows[51], outRows[49], outRows[51]);
  zip16<2>(outRows[52], outRows[54], outRows[52], outRows[54]);
  zip16<2>(outRows[53], outRows[55], outRows[53], outRows[55]);
  zip16<2>(outRows[56], outRows[58], outRows[56], outRows[58]);
  zip16<2>(outRows[57], outRows[59], outRows[57], outRows[59]);
  zip16<2>(outRows[60], outRows[62], outRows[60], outRows[62]);
  zip16<2>(outRows[61], outRows[63], outRows[61], outRows[63]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<4>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<4>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<4>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<4>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<4>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<4>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<4>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<4>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip16<4>(outRows[32], outRows[36], outRows[32], outRows[36]);
  zip16<4>(outRows[34], outRows[38], outRows[34], outRows[38]);
  zip16<4>(outRows[33], outRows[37], outRows[33], outRows[37]);
  zip16<4>(outRows[35], outRows[39], outRows[35], outRows[39]);
  zip16<4>(outRows[40], outRows[44], outRows[40], outRows[44]);
  zip16<4>(outRows[42], outRows[46], outRows[42], outRows[46]);
  zip16<4>(outRows[41], outRows[45], outRows[41], outRows[45]);
  zip16<4>(outRows[43], outRows[47], outRows[43], outRows[47]);
  zip16<4>(outRows[48], outRows[52], outRows[48], outRows[52]);
  zip16<4>(outRows[50], outRows[54], outRows[50], outRows[54]);
  zip16<4>(outRows[49], outRows[53], outRows[49], outRows[53]);
  zip16<4>(outRows[51], outRows[55], outRows[51], outRows[55]);
  zip16<4>(outRows[56], outRows[60], outRows[56], outRows[60]);
  zip16<4>(outRows[58], outRows[62], outRows[58], outRows[62]);
  zip16<4>(outRows[57], outRows[61], outRows[57], outRows[61]);
  zip16<4>(outRows[59], outRows[63], outRows[59], outRows[63]);
  zip16<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip16<8>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip16<8>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip16<8>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip16<8>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip16<8>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip16<8>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip16<8>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip16<8>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip16<8>(outRows[32], outRows[40], outRows[32], outRows[40]);
  zip16<8>(outRows[36], outRows[44], outRows[36], outRows[44]);
  zip16<8>(outRows[34], outRows[42], outRows[34], outRows[42]);
  zip16<8>(outRows[38], outRows[46], outRows[38], outRows[46]);
  zip16<8>(outRows[33], outRows[41], outRows[33], outRows[41]);
  zip16<8>(outRows[37], outRows[45], outRows[37], outRows[45]);
  zip16<8>(outRows[35], outRows[43], outRows[35], outRows[43]);
  zip16<8>(outRows[39], outRows[47], outRows[39], outRows[47]);
  zip16<8>(outRows[48], outRows[56], outRows[48], outRows[56]);
  zip16<8>(outRows[52], outRows[60], outRows[52], outRows[60]);
  zip16<8>(outRows[50], outRows[58], outRows[50], outRows[58]);
  zip16<8>(outRows[54], outRows[62], outRows[54], outRows[62]);
  zip16<8>(outRows[49], outRows[57], outRows[49], outRows[57]);
  zip16<8>(outRows[53], outRows[61], outRows[53], outRows[61]);
  zip16<8>(outRows[51], outRows[59], outRows[51], outRows[59]);
  zip16<8>(outRows[55], outRows[63], outRows[55], outRows[63]);
  std::swap(outRows[1], outRows[8]);
  std::swap(outRows[2], outRows[4]);
  std::swap(outRows[3], outRows[12]);
  std::swap(outRows[5], outRows[10]);
  std::swap(outRows[7], outRows[14]);
  std::swap(outRows[11], outRows[13]);
  std::swap(outRows[17], outRows[24]);
  std::swap(outRows[18], outRows[20]);
  std::swap(outRows[19], outRows[28]);
  std::swap(outRows[21], outRows[26]);
  std::swap(outRows[23], outRows[30]);
  std::swap(outRows[27], outRows[29]);
  std::swap(outRows[33], outRows[40]);
  std::swap(outRows[34], outRows[36]);
  std::swap(outRows[35], outRows[44]);
  std::swap(outRows[37], outRows[42]);
  std::swap(outRows[39], outRows[46]);
  std::swap(outRows[43], outRows[45]);
  std::swap(outRows[49], outRows[56]);
  std::swap(outRows[50], outRows[52]);
  std::swap(outRows[51], outRows[60]);
  std::swap(outRows[53], outRows[58]);
  std::swap(outRows[55], outRows[62]);
  std::swap(outRows[59], outRows[61]);
  // correction steps follow below (if required)
  zip<16>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<16>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<16>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<16>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<16>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<16>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<16>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<16>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<16>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<16>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<16>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<16>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<16>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<16>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<16>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<16>(outRows[15], outRows[31], outRows[15], outRows[31]);
  zip<16>(outRows[32], outRows[48], outRows[32], outRows[48]);
  zip<16>(outRows[33], outRows[49], outRows[33], outRows[49]);
  zip<16>(outRows[34], outRows[50], outRows[34], outRows[50]);
  zip<16>(outRows[35], outRows[51], outRows[35], outRows[51]);
  zip<16>(outRows[36], outRows[52], outRows[36], outRows[52]);
  zip<16>(outRows[37], outRows[53], outRows[37], outRows[53]);
  zip<16>(outRows[38], outRows[54], outRows[38], outRows[54]);
  zip<16>(outRows[39], outRows[55], outRows[39], outRows[55]);
  zip<16>(outRows[40], outRows[56], outRows[40], outRows[56]);
  zip<16>(outRows[41], outRows[57], outRows[41], outRows[57]);
  zip<16>(outRows[42], outRows[58], outRows[42], outRows[58]);
  zip<16>(outRows[43], outRows[59], outRows[43], outRows[59]);
  zip<16>(outRows[44], outRows[60], outRows[44], outRows[60]);
  zip<16>(outRows[45], outRows[61], outRows[45], outRows[61]);
  zip<16>(outRows[46], outRows[62], outRows[46], outRows[62]);
  zip<16>(outRows[47], outRows[63], outRows[47], outRows[63]);
  zip<32>(outRows[0], outRows[32], outRows[0], outRows[32]);
  zip<32>(outRows[1], outRows[33], outRows[1], outRows[33]);
  zip<32>(outRows[2], outRows[34], outRows[2], outRows[34]);
  zip<32>(outRows[3], outRows[35], outRows[3], outRows[35]);
  zip<32>(outRows[4], outRows[36], outRows[4], outRows[36]);
  zip<32>(outRows[5], outRows[37], outRows[5], outRows[37]);
  zip<32>(outRows[6], outRows[38], outRows[6], outRows[38]);
  zip<32>(outRows[7], outRows[39], outRows[7], outRows[39]);
  zip<32>(outRows[8], outRows[40], outRows[8], outRows[40]);
  zip<32>(outRows[9], outRows[41], outRows[9], outRows[41]);
  zip<32>(outRows[10], outRows[42], outRows[10], outRows[42]);
  zip<32>(outRows[11], outRows[43], outRows[11], outRows[43]);
  zip<32>(outRows[12], outRows[44], outRows[12], outRows[44]);
  zip<32>(outRows[13], outRows[45], outRows[13], outRows[45]);
  zip<32>(outRows[14], outRows[46], outRows[14], outRows[46]);
  zip<32>(outRows[15], outRows[47], outRows[15], outRows[47]);
  zip<32>(outRows[16], outRows[48], outRows[16], outRows[48]);
  zip<32>(outRows[17], outRows[49], outRows[17], outRows[49]);
  zip<32>(outRows[18], outRows[50], outRows[18], outRows[50]);
  zip<32>(outRows[19], outRows[51], outRows[19], outRows[51]);
  zip<32>(outRows[20], outRows[52], outRows[20], outRows[52]);
  zip<32>(outRows[21], outRows[53], outRows[21], outRows[53]);
  zip<32>(outRows[22], outRows[54], outRows[22], outRows[54]);
  zip<32>(outRows[23], outRows[55], outRows[23], outRows[55]);
  zip<32>(outRows[24], outRows[56], outRows[24], outRows[56]);
  zip<32>(outRows[25], outRows[57], outRows[25], outRows[57]);
  zip<32>(outRows[26], outRows[58], outRows[26], outRows[58]);
  zip<32>(outRows[27], outRows[59], outRows[27], outRows[59]);
  zip<32>(outRows[28], outRows[60], outRows[28], outRows[60]);
  zip<32>(outRows[29], outRows[61], outRows[29], outRows[61]);
  zip<32>(outRows[30], outRows[62], outRows[30], outRows[62]);
  zip<32>(outRows[31], outRows[63], outRows[31], outRows[63]);
  std::swap(outRows[16], outRows[32]);
  std::swap(outRows[17], outRows[33]);
  std::swap(outRows[18], outRows[34]);
  std::swap(outRows[19], outRows[35]);
  std::swap(outRows[20], outRows[36]);
  std::swap(outRows[21], outRows[37]);
  std::swap(outRows[22], outRows[38]);
  std::swap(outRows[23], outRows[39]);
  std::swap(outRows[24], outRows[40]);
  std::swap(outRows[25], outRows[41]);
  std::swap(outRows[26], outRows[42]);
  std::swap(outRows[27], outRows[43]);
  std::swap(outRows[28], outRows[44]);
  std::swap(outRows[29], outRows[45]);
  std::swap(outRows[30], outRows[46]);
  std::swap(outRows[31], outRows[47]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<1>(inRows[16], inRows[17], outRows[16], outRows[17]);
  zip16<1>(inRows[18], inRows[19], outRows[18], outRows[19]);
  zip16<1>(inRows[20], inRows[21], outRows[20], outRows[21]);
  zip16<1>(inRows[22], inRows[23], outRows[22], outRows[23]);
  zip16<1>(inRows[24], inRows[25], outRows[24], outRows[25]);
  zip16<1>(inRows[26], inRows[27], outRows[26], outRows[27]);
  zip16<1>(inRows[28], inRows[29], outRows[28], outRows[29]);
  zip16<1>(inRows[30], inRows[31], outRows[30], outRows[31]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip16<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip16<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip16<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip16<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip16<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip16<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip16<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip16<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<4>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<4>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<4>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<4>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<4>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<4>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<4>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<4>(outRows[27], outRows[31], outRows[27], outRows[31]);
  std::swap(outRows[1], outRows[4]);
  std::swap(outRows[3], outRows[6]);
  std::swap(outRows[9], outRows[12]);
  std::swap(outRows[11], outRows[14]);
  std::swap(outRows[17], outRows[20]);
  std::swap(outRows[19], outRows[22]);
  std::swap(outRows[25], outRows[28]);
  std::swap(outRows[27], outRows[30]);
  // correction steps follow below (if required)
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip<8>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip<8>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip<8>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip<8>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip<8>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip<8>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip<8>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip<8>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip<16>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<16>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<16>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<16>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<16>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<16>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<16>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<16>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<16>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<16>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<16>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<16>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<16>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<16>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<16>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<16>(outRows[15], outRows[31], outRows[15], outRows[31]);
  std::swap(outRows[8], outRows[16]);
  std::swap(outRows[9], outRows[17]);
  std::swap(outRows[10], outRows[18]);
  std::swap(outRows[11], outRows[19]);
  std::swap(outRows[12], outRows[20]);
  std::swap(outRows[13], outRows[21]);
  std::swap(outRows[14], outRows[22]);
  std::swap(outRows[15], outRows[23]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  zip16<1>(inRows[8], inRows[9], outRows[8], outRows[9]);
  zip16<1>(inRows[10], inRows[11], outRows[10], outRows[11]);
  zip16<1>(inRows[12], inRows[13], outRows[12], outRows[13]);
  zip16<1>(inRows[14], inRows[15], outRows[14], outRows[15]);
  zip16<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  std::swap(outRows[1], outRows[2]);
  std::swap(outRows[5], outRows[6]);
  std::swap(outRows[9], outRows[10]);
  std::swap(outRows[13], outRows[14]);
  // correction steps follow below (if required)
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<4>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<4>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<4>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<4>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<8>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<8>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<8>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<8>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<8>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<8>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<8>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<8>(outRows[7], outRows[15], outRows[7], outRows[15]);
  std::swap(outRows[4], outRows[8]);
  std::swap(outRows[5], outRows[9]);
  std::swap(outRows[6], outRows[10]);
  std::swap(outRows[7], outRows[11]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<64>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  zip16<1>(inRows[2], inRows[3], outRows[2], outRows[3]);
  zip16<1>(inRows[4], inRows[5], outRows[4], outRows[5]);
  zip16<1>(inRows[6], inRows[7], outRows[6], outRows[7]);
  // correction steps follow below (if required)
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<4>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<4>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<4>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<4>(outRows[3], outRows[7], outRows[3], outRows[7]);
  std::swap(outRows[2], outRows[4]);
  std::swap(outRows[3], outRows[5]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose1inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  transpose1inplcLane(inRows, outRows, Elements<Vec<T, SIMD_WIDTH>::elements>(),
                      Bytes<SIMD_WIDTH>());
}

// ==========================================================
// transpose2inplc
// ==========================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<2>)
{
  zip<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>)
{
  zip<1>(inRows[0], inRows[2], outRows[0], outRows[2]);
  zip<1>(inRows[1], inRows[3], outRows[1], outRows[3]);
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>)
{
  zip<1>(inRows[0], inRows[4], outRows[0], outRows[4]);
  zip<1>(inRows[1], inRows[5], outRows[1], outRows[5]);
  zip<1>(inRows[2], inRows[6], outRows[2], outRows[6]);
  zip<1>(inRows[3], inRows[7], outRows[3], outRows[7]);
  zip<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>)
{
  zip<1>(inRows[0], inRows[8], outRows[0], outRows[8]);
  zip<1>(inRows[1], inRows[9], outRows[1], outRows[9]);
  zip<1>(inRows[2], inRows[10], outRows[2], outRows[10]);
  zip<1>(inRows[3], inRows[11], outRows[3], outRows[11]);
  zip<1>(inRows[4], inRows[12], outRows[4], outRows[12]);
  zip<1>(inRows[5], inRows[13], outRows[5], outRows[13]);
  zip<1>(inRows[6], inRows[14], outRows[6], outRows[14]);
  zip<1>(inRows[7], inRows[15], outRows[7], outRows[15]);
  zip<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>)
{
  zip<1>(inRows[0], inRows[16], outRows[0], outRows[16]);
  zip<1>(inRows[1], inRows[17], outRows[1], outRows[17]);
  zip<1>(inRows[2], inRows[18], outRows[2], outRows[18]);
  zip<1>(inRows[3], inRows[19], outRows[3], outRows[19]);
  zip<1>(inRows[4], inRows[20], outRows[4], outRows[20]);
  zip<1>(inRows[5], inRows[21], outRows[5], outRows[21]);
  zip<1>(inRows[6], inRows[22], outRows[6], outRows[22]);
  zip<1>(inRows[7], inRows[23], outRows[7], outRows[23]);
  zip<1>(inRows[8], inRows[24], outRows[8], outRows[24]);
  zip<1>(inRows[9], inRows[25], outRows[9], outRows[25]);
  zip<1>(inRows[10], inRows[26], outRows[10], outRows[26]);
  zip<1>(inRows[11], inRows[27], outRows[11], outRows[27]);
  zip<1>(inRows[12], inRows[28], outRows[12], outRows[28]);
  zip<1>(inRows[13], inRows[29], outRows[13], outRows[29]);
  zip<1>(inRows[14], inRows[30], outRows[14], outRows[30]);
  zip<1>(inRows[15], inRows[31], outRows[15], outRows[31]);
  zip<1>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<1>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip<1>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<1>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip<1>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<1>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip<1>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<1>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip<1>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<1>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip<1>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<1>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip<1>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<1>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip<1>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip<1>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<1>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip<1>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<1>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip<1>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<1>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip<1>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<1>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip<1>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<1>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<1>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<1>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<1>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<1>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<1>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<1>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<1>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<1>(outRows[16], outRows[17], outRows[16], outRows[17]);
  zip<1>(outRows[18], outRows[19], outRows[18], outRows[19]);
  zip<1>(outRows[20], outRows[21], outRows[20], outRows[21]);
  zip<1>(outRows[22], outRows[23], outRows[22], outRows[23]);
  zip<1>(outRows[24], outRows[25], outRows[24], outRows[25]);
  zip<1>(outRows[26], outRows[27], outRows[26], outRows[27]);
  zip<1>(outRows[28], outRows[29], outRows[28], outRows[29]);
  zip<1>(outRows[30], outRows[31], outRows[30], outRows[31]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<64>)
{
  zip<1>(inRows[0], inRows[32], outRows[0], outRows[32]);
  zip<1>(inRows[1], inRows[33], outRows[1], outRows[33]);
  zip<1>(inRows[2], inRows[34], outRows[2], outRows[34]);
  zip<1>(inRows[3], inRows[35], outRows[3], outRows[35]);
  zip<1>(inRows[4], inRows[36], outRows[4], outRows[36]);
  zip<1>(inRows[5], inRows[37], outRows[5], outRows[37]);
  zip<1>(inRows[6], inRows[38], outRows[6], outRows[38]);
  zip<1>(inRows[7], inRows[39], outRows[7], outRows[39]);
  zip<1>(inRows[8], inRows[40], outRows[8], outRows[40]);
  zip<1>(inRows[9], inRows[41], outRows[9], outRows[41]);
  zip<1>(inRows[10], inRows[42], outRows[10], outRows[42]);
  zip<1>(inRows[11], inRows[43], outRows[11], outRows[43]);
  zip<1>(inRows[12], inRows[44], outRows[12], outRows[44]);
  zip<1>(inRows[13], inRows[45], outRows[13], outRows[45]);
  zip<1>(inRows[14], inRows[46], outRows[14], outRows[46]);
  zip<1>(inRows[15], inRows[47], outRows[15], outRows[47]);
  zip<1>(inRows[16], inRows[48], outRows[16], outRows[48]);
  zip<1>(inRows[17], inRows[49], outRows[17], outRows[49]);
  zip<1>(inRows[18], inRows[50], outRows[18], outRows[50]);
  zip<1>(inRows[19], inRows[51], outRows[19], outRows[51]);
  zip<1>(inRows[20], inRows[52], outRows[20], outRows[52]);
  zip<1>(inRows[21], inRows[53], outRows[21], outRows[53]);
  zip<1>(inRows[22], inRows[54], outRows[22], outRows[54]);
  zip<1>(inRows[23], inRows[55], outRows[23], outRows[55]);
  zip<1>(inRows[24], inRows[56], outRows[24], outRows[56]);
  zip<1>(inRows[25], inRows[57], outRows[25], outRows[57]);
  zip<1>(inRows[26], inRows[58], outRows[26], outRows[58]);
  zip<1>(inRows[27], inRows[59], outRows[27], outRows[59]);
  zip<1>(inRows[28], inRows[60], outRows[28], outRows[60]);
  zip<1>(inRows[29], inRows[61], outRows[29], outRows[61]);
  zip<1>(inRows[30], inRows[62], outRows[30], outRows[62]);
  zip<1>(inRows[31], inRows[63], outRows[31], outRows[63]);
  zip<1>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip<1>(outRows[32], outRows[48], outRows[32], outRows[48]);
  zip<1>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip<1>(outRows[33], outRows[49], outRows[33], outRows[49]);
  zip<1>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip<1>(outRows[34], outRows[50], outRows[34], outRows[50]);
  zip<1>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip<1>(outRows[35], outRows[51], outRows[35], outRows[51]);
  zip<1>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip<1>(outRows[36], outRows[52], outRows[36], outRows[52]);
  zip<1>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip<1>(outRows[37], outRows[53], outRows[37], outRows[53]);
  zip<1>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip<1>(outRows[38], outRows[54], outRows[38], outRows[54]);
  zip<1>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip<1>(outRows[39], outRows[55], outRows[39], outRows[55]);
  zip<1>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip<1>(outRows[40], outRows[56], outRows[40], outRows[56]);
  zip<1>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip<1>(outRows[41], outRows[57], outRows[41], outRows[57]);
  zip<1>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip<1>(outRows[42], outRows[58], outRows[42], outRows[58]);
  zip<1>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip<1>(outRows[43], outRows[59], outRows[43], outRows[59]);
  zip<1>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip<1>(outRows[44], outRows[60], outRows[44], outRows[60]);
  zip<1>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip<1>(outRows[45], outRows[61], outRows[45], outRows[61]);
  zip<1>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip<1>(outRows[46], outRows[62], outRows[46], outRows[62]);
  zip<1>(outRows[15], outRows[31], outRows[15], outRows[31]);
  zip<1>(outRows[47], outRows[63], outRows[47], outRows[63]);
  zip<1>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip<1>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip<1>(outRows[32], outRows[40], outRows[32], outRows[40]);
  zip<1>(outRows[48], outRows[56], outRows[48], outRows[56]);
  zip<1>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip<1>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip<1>(outRows[33], outRows[41], outRows[33], outRows[41]);
  zip<1>(outRows[49], outRows[57], outRows[49], outRows[57]);
  zip<1>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip<1>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip<1>(outRows[34], outRows[42], outRows[34], outRows[42]);
  zip<1>(outRows[50], outRows[58], outRows[50], outRows[58]);
  zip<1>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip<1>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip<1>(outRows[35], outRows[43], outRows[35], outRows[43]);
  zip<1>(outRows[51], outRows[59], outRows[51], outRows[59]);
  zip<1>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip<1>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip<1>(outRows[36], outRows[44], outRows[36], outRows[44]);
  zip<1>(outRows[52], outRows[60], outRows[52], outRows[60]);
  zip<1>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip<1>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip<1>(outRows[37], outRows[45], outRows[37], outRows[45]);
  zip<1>(outRows[53], outRows[61], outRows[53], outRows[61]);
  zip<1>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip<1>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip<1>(outRows[38], outRows[46], outRows[38], outRows[46]);
  zip<1>(outRows[54], outRows[62], outRows[54], outRows[62]);
  zip<1>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip<1>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip<1>(outRows[39], outRows[47], outRows[39], outRows[47]);
  zip<1>(outRows[55], outRows[63], outRows[55], outRows[63]);
  zip<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip<1>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip<1>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip<1>(outRows[32], outRows[36], outRows[32], outRows[36]);
  zip<1>(outRows[40], outRows[44], outRows[40], outRows[44]);
  zip<1>(outRows[48], outRows[52], outRows[48], outRows[52]);
  zip<1>(outRows[56], outRows[60], outRows[56], outRows[60]);
  zip<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip<1>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip<1>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip<1>(outRows[33], outRows[37], outRows[33], outRows[37]);
  zip<1>(outRows[41], outRows[45], outRows[41], outRows[45]);
  zip<1>(outRows[49], outRows[53], outRows[49], outRows[53]);
  zip<1>(outRows[57], outRows[61], outRows[57], outRows[61]);
  zip<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip<1>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip<1>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip<1>(outRows[34], outRows[38], outRows[34], outRows[38]);
  zip<1>(outRows[42], outRows[46], outRows[42], outRows[46]);
  zip<1>(outRows[50], outRows[54], outRows[50], outRows[54]);
  zip<1>(outRows[58], outRows[62], outRows[58], outRows[62]);
  zip<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip<1>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip<1>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip<1>(outRows[35], outRows[39], outRows[35], outRows[39]);
  zip<1>(outRows[43], outRows[47], outRows[43], outRows[47]);
  zip<1>(outRows[51], outRows[55], outRows[51], outRows[55]);
  zip<1>(outRows[59], outRows[63], outRows[59], outRows[63]);
  zip<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<1>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<1>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<1>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<1>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<1>(outRows[32], outRows[34], outRows[32], outRows[34]);
  zip<1>(outRows[36], outRows[38], outRows[36], outRows[38]);
  zip<1>(outRows[40], outRows[42], outRows[40], outRows[42]);
  zip<1>(outRows[44], outRows[46], outRows[44], outRows[46]);
  zip<1>(outRows[48], outRows[50], outRows[48], outRows[50]);
  zip<1>(outRows[52], outRows[54], outRows[52], outRows[54]);
  zip<1>(outRows[56], outRows[58], outRows[56], outRows[58]);
  zip<1>(outRows[60], outRows[62], outRows[60], outRows[62]);
  zip<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<1>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<1>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<1>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<1>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip<1>(outRows[33], outRows[35], outRows[33], outRows[35]);
  zip<1>(outRows[37], outRows[39], outRows[37], outRows[39]);
  zip<1>(outRows[41], outRows[43], outRows[41], outRows[43]);
  zip<1>(outRows[45], outRows[47], outRows[45], outRows[47]);
  zip<1>(outRows[49], outRows[51], outRows[49], outRows[51]);
  zip<1>(outRows[53], outRows[55], outRows[53], outRows[55]);
  zip<1>(outRows[57], outRows[59], outRows[57], outRows[59]);
  zip<1>(outRows[61], outRows[63], outRows[61], outRows[63]);
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<1>(outRows[16], outRows[17], outRows[16], outRows[17]);
  zip<1>(outRows[18], outRows[19], outRows[18], outRows[19]);
  zip<1>(outRows[20], outRows[21], outRows[20], outRows[21]);
  zip<1>(outRows[22], outRows[23], outRows[22], outRows[23]);
  zip<1>(outRows[24], outRows[25], outRows[24], outRows[25]);
  zip<1>(outRows[26], outRows[27], outRows[26], outRows[27]);
  zip<1>(outRows[28], outRows[29], outRows[28], outRows[29]);
  zip<1>(outRows[30], outRows[31], outRows[30], outRows[31]);
  zip<1>(outRows[32], outRows[33], outRows[32], outRows[33]);
  zip<1>(outRows[34], outRows[35], outRows[34], outRows[35]);
  zip<1>(outRows[36], outRows[37], outRows[36], outRows[37]);
  zip<1>(outRows[38], outRows[39], outRows[38], outRows[39]);
  zip<1>(outRows[40], outRows[41], outRows[40], outRows[41]);
  zip<1>(outRows[42], outRows[43], outRows[42], outRows[43]);
  zip<1>(outRows[44], outRows[45], outRows[44], outRows[45]);
  zip<1>(outRows[46], outRows[47], outRows[46], outRows[47]);
  zip<1>(outRows[48], outRows[49], outRows[48], outRows[49]);
  zip<1>(outRows[50], outRows[51], outRows[50], outRows[51]);
  zip<1>(outRows[52], outRows[53], outRows[52], outRows[53]);
  zip<1>(outRows[54], outRows[55], outRows[54], outRows[55]);
  zip<1>(outRows[56], outRows[57], outRows[56], outRows[57]);
  zip<1>(outRows[58], outRows[59], outRows[58], outRows[59]);
  zip<1>(outRows[60], outRows[61], outRows[60], outRows[61]);
  zip<1>(outRows[62], outRows[63], outRows[62], outRows[63]);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplc(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  transpose2inplc(inRows, outRows, Elements<Vec<T, SIMD_WIDTH>::elements>());
}

// ==========================================================
// transpose2inplcLane
// ==========================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<16>)
{
  zip16<1>(inRows[0], inRows[8], outRows[0], outRows[8]);
  zip16<1>(inRows[1], inRows[9], outRows[1], outRows[9]);
  zip16<1>(inRows[2], inRows[10], outRows[2], outRows[10]);
  zip16<1>(inRows[3], inRows[11], outRows[3], outRows[11]);
  zip16<1>(inRows[4], inRows[12], outRows[4], outRows[12]);
  zip16<1>(inRows[5], inRows[13], outRows[5], outRows[13]);
  zip16<1>(inRows[6], inRows[14], outRows[6], outRows[14]);
  zip16<1>(inRows[7], inRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip16<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip16<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip16<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip16<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip16<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip16<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip16<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[4], outRows[0], outRows[4]);
  zip16<1>(inRows[1], inRows[5], outRows[1], outRows[5]);
  zip16<1>(inRows[2], inRows[6], outRows[2], outRows[6]);
  zip16<1>(inRows[3], inRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip16<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip16<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip16<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[2], outRows[0], outRows[2]);
  zip16<1>(inRows[1], inRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip16<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<2>, Bytes<16>)
{
  zip16<1>(inRows[0], inRows[1], outRows[0], outRows[1]);
  // correction steps follow below (if required)
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>,
  Bytes<32>)
{
  zip16<1>(inRows[0], inRows[16], outRows[0], outRows[16]);
  zip16<1>(inRows[1], inRows[17], outRows[1], outRows[17]);
  zip16<1>(inRows[2], inRows[18], outRows[2], outRows[18]);
  zip16<1>(inRows[3], inRows[19], outRows[3], outRows[19]);
  zip16<1>(inRows[4], inRows[20], outRows[4], outRows[20]);
  zip16<1>(inRows[5], inRows[21], outRows[5], outRows[21]);
  zip16<1>(inRows[6], inRows[22], outRows[6], outRows[22]);
  zip16<1>(inRows[7], inRows[23], outRows[7], outRows[23]);
  zip16<1>(inRows[8], inRows[24], outRows[8], outRows[24]);
  zip16<1>(inRows[9], inRows[25], outRows[9], outRows[25]);
  zip16<1>(inRows[10], inRows[26], outRows[10], outRows[26]);
  zip16<1>(inRows[11], inRows[27], outRows[11], outRows[27]);
  zip16<1>(inRows[12], inRows[28], outRows[12], outRows[28]);
  zip16<1>(inRows[13], inRows[29], outRows[13], outRows[29]);
  zip16<1>(inRows[14], inRows[30], outRows[14], outRows[30]);
  zip16<1>(inRows[15], inRows[31], outRows[15], outRows[31]);
  zip16<1>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<1>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip16<1>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<1>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip16<1>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<1>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip16<1>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<1>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip16<1>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<1>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip16<1>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<1>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip16<1>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<1>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip16<1>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<1>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<1>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<1>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<1>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<1>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip16<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<1>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip16<1>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip16<1>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip16<1>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip16<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip16<1>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip16<1>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip16<1>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip16<1>(outRows[29], outRows[31], outRows[29], outRows[31]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<1>(outRows[16], outRows[17], outRows[16], outRows[17]);
  zip<1>(outRows[18], outRows[19], outRows[18], outRows[19]);
  zip<1>(outRows[20], outRows[21], outRows[20], outRows[21]);
  zip<1>(outRows[22], outRows[23], outRows[22], outRows[23]);
  zip<1>(outRows[24], outRows[25], outRows[24], outRows[25]);
  zip<1>(outRows[26], outRows[27], outRows[26], outRows[27]);
  zip<1>(outRows[28], outRows[29], outRows[28], outRows[29]);
  zip<1>(outRows[30], outRows[31], outRows[30], outRows[31]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[2];
    outRows[2]               = outRows[4];
    outRows[4]               = outRows[8];
    outRows[8]               = outRows[16];
    outRows[16]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[6];
    outRows[6]               = outRows[12];
    outRows[12]              = outRows[24];
    outRows[24]              = outRows[17];
    outRows[17]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[5];
    outRows[5]               = outRows[10];
    outRows[10]              = outRows[20];
    outRows[20]              = outRows[9];
    outRows[9]               = outRows[18];
    outRows[18]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[7];
    outRows[7]               = outRows[14];
    outRows[14]              = outRows[28];
    outRows[28]              = outRows[25];
    outRows[25]              = outRows[19];
    outRows[19]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[11];
    outRows[11]              = outRows[22];
    outRows[22]              = outRows[13];
    outRows[13]              = outRows[26];
    outRows[26]              = outRows[21];
    outRows[21]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[15];
    outRows[15]              = outRows[30];
    outRows[30]              = outRows[29];
    outRows[29]              = outRows[27];
    outRows[27]              = outRows[23];
    outRows[23]              = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<32>)
{
  zip16<1>(inRows[0], inRows[8], outRows[0], outRows[8]);
  zip16<1>(inRows[1], inRows[9], outRows[1], outRows[9]);
  zip16<1>(inRows[2], inRows[10], outRows[2], outRows[10]);
  zip16<1>(inRows[3], inRows[11], outRows[3], outRows[11]);
  zip16<1>(inRows[4], inRows[12], outRows[4], outRows[12]);
  zip16<1>(inRows[5], inRows[13], outRows[5], outRows[13]);
  zip16<1>(inRows[6], inRows[14], outRows[6], outRows[14]);
  zip16<1>(inRows[7], inRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<1>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip16<1>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip16<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip16<1>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip16<1>(outRows[13], outRows[15], outRows[13], outRows[15]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[2];
    outRows[2]               = outRows[4];
    outRows[4]               = outRows[8];
    outRows[8]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[6];
    outRows[6]               = outRows[12];
    outRows[12]              = outRows[9];
    outRows[9]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[5];
    outRows[5]               = outRows[10];
    outRows[10]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[7];
    outRows[7]               = outRows[14];
    outRows[14]              = outRows[13];
    outRows[13]              = outRows[11];
    outRows[11]              = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<32>)
{
  zip16<1>(inRows[0], inRows[4], outRows[0], outRows[4]);
  zip16<1>(inRows[1], inRows[5], outRows[1], outRows[5]);
  zip16<1>(inRows[2], inRows[6], outRows[2], outRows[6]);
  zip16<1>(inRows[3], inRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip16<1>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip16<1>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip16<1>(outRows[5], outRows[7], outRows[5], outRows[7]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[2];
    outRows[2]               = outRows[4];
    outRows[4]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[6];
    outRows[6]               = outRows[5];
    outRows[5]               = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<4>, Bytes<32>)
{
  zip16<1>(inRows[0], inRows[2], outRows[0], outRows[2]);
  zip16<1>(inRows[1], inRows[3], outRows[1], outRows[3]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[2];
    outRows[2]               = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<64>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[32], outRows[0], outRows[32]);
  zip16<1>(inRows[1], inRows[33], outRows[1], outRows[33]);
  zip16<1>(inRows[2], inRows[34], outRows[2], outRows[34]);
  zip16<1>(inRows[3], inRows[35], outRows[3], outRows[35]);
  zip16<1>(inRows[4], inRows[36], outRows[4], outRows[36]);
  zip16<1>(inRows[5], inRows[37], outRows[5], outRows[37]);
  zip16<1>(inRows[6], inRows[38], outRows[6], outRows[38]);
  zip16<1>(inRows[7], inRows[39], outRows[7], outRows[39]);
  zip16<1>(inRows[8], inRows[40], outRows[8], outRows[40]);
  zip16<1>(inRows[9], inRows[41], outRows[9], outRows[41]);
  zip16<1>(inRows[10], inRows[42], outRows[10], outRows[42]);
  zip16<1>(inRows[11], inRows[43], outRows[11], outRows[43]);
  zip16<1>(inRows[12], inRows[44], outRows[12], outRows[44]);
  zip16<1>(inRows[13], inRows[45], outRows[13], outRows[45]);
  zip16<1>(inRows[14], inRows[46], outRows[14], outRows[46]);
  zip16<1>(inRows[15], inRows[47], outRows[15], outRows[47]);
  zip16<1>(inRows[16], inRows[48], outRows[16], outRows[48]);
  zip16<1>(inRows[17], inRows[49], outRows[17], outRows[49]);
  zip16<1>(inRows[18], inRows[50], outRows[18], outRows[50]);
  zip16<1>(inRows[19], inRows[51], outRows[19], outRows[51]);
  zip16<1>(inRows[20], inRows[52], outRows[20], outRows[52]);
  zip16<1>(inRows[21], inRows[53], outRows[21], outRows[53]);
  zip16<1>(inRows[22], inRows[54], outRows[22], outRows[54]);
  zip16<1>(inRows[23], inRows[55], outRows[23], outRows[55]);
  zip16<1>(inRows[24], inRows[56], outRows[24], outRows[56]);
  zip16<1>(inRows[25], inRows[57], outRows[25], outRows[57]);
  zip16<1>(inRows[26], inRows[58], outRows[26], outRows[58]);
  zip16<1>(inRows[27], inRows[59], outRows[27], outRows[59]);
  zip16<1>(inRows[28], inRows[60], outRows[28], outRows[60]);
  zip16<1>(inRows[29], inRows[61], outRows[29], outRows[61]);
  zip16<1>(inRows[30], inRows[62], outRows[30], outRows[62]);
  zip16<1>(inRows[31], inRows[63], outRows[31], outRows[63]);
  zip16<1>(outRows[0], outRows[16], outRows[0], outRows[16]);
  zip16<1>(outRows[32], outRows[48], outRows[32], outRows[48]);
  zip16<1>(outRows[1], outRows[17], outRows[1], outRows[17]);
  zip16<1>(outRows[33], outRows[49], outRows[33], outRows[49]);
  zip16<1>(outRows[2], outRows[18], outRows[2], outRows[18]);
  zip16<1>(outRows[34], outRows[50], outRows[34], outRows[50]);
  zip16<1>(outRows[3], outRows[19], outRows[3], outRows[19]);
  zip16<1>(outRows[35], outRows[51], outRows[35], outRows[51]);
  zip16<1>(outRows[4], outRows[20], outRows[4], outRows[20]);
  zip16<1>(outRows[36], outRows[52], outRows[36], outRows[52]);
  zip16<1>(outRows[5], outRows[21], outRows[5], outRows[21]);
  zip16<1>(outRows[37], outRows[53], outRows[37], outRows[53]);
  zip16<1>(outRows[6], outRows[22], outRows[6], outRows[22]);
  zip16<1>(outRows[38], outRows[54], outRows[38], outRows[54]);
  zip16<1>(outRows[7], outRows[23], outRows[7], outRows[23]);
  zip16<1>(outRows[39], outRows[55], outRows[39], outRows[55]);
  zip16<1>(outRows[8], outRows[24], outRows[8], outRows[24]);
  zip16<1>(outRows[40], outRows[56], outRows[40], outRows[56]);
  zip16<1>(outRows[9], outRows[25], outRows[9], outRows[25]);
  zip16<1>(outRows[41], outRows[57], outRows[41], outRows[57]);
  zip16<1>(outRows[10], outRows[26], outRows[10], outRows[26]);
  zip16<1>(outRows[42], outRows[58], outRows[42], outRows[58]);
  zip16<1>(outRows[11], outRows[27], outRows[11], outRows[27]);
  zip16<1>(outRows[43], outRows[59], outRows[43], outRows[59]);
  zip16<1>(outRows[12], outRows[28], outRows[12], outRows[28]);
  zip16<1>(outRows[44], outRows[60], outRows[44], outRows[60]);
  zip16<1>(outRows[13], outRows[29], outRows[13], outRows[29]);
  zip16<1>(outRows[45], outRows[61], outRows[45], outRows[61]);
  zip16<1>(outRows[14], outRows[30], outRows[14], outRows[30]);
  zip16<1>(outRows[46], outRows[62], outRows[46], outRows[62]);
  zip16<1>(outRows[15], outRows[31], outRows[15], outRows[31]);
  zip16<1>(outRows[47], outRows[63], outRows[47], outRows[63]);
  zip16<1>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<1>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip16<1>(outRows[32], outRows[40], outRows[32], outRows[40]);
  zip16<1>(outRows[48], outRows[56], outRows[48], outRows[56]);
  zip16<1>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<1>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip16<1>(outRows[33], outRows[41], outRows[33], outRows[41]);
  zip16<1>(outRows[49], outRows[57], outRows[49], outRows[57]);
  zip16<1>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<1>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip16<1>(outRows[34], outRows[42], outRows[34], outRows[42]);
  zip16<1>(outRows[50], outRows[58], outRows[50], outRows[58]);
  zip16<1>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<1>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip16<1>(outRows[35], outRows[43], outRows[35], outRows[43]);
  zip16<1>(outRows[51], outRows[59], outRows[51], outRows[59]);
  zip16<1>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<1>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip16<1>(outRows[36], outRows[44], outRows[36], outRows[44]);
  zip16<1>(outRows[52], outRows[60], outRows[52], outRows[60]);
  zip16<1>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<1>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip16<1>(outRows[37], outRows[45], outRows[37], outRows[45]);
  zip16<1>(outRows[53], outRows[61], outRows[53], outRows[61]);
  zip16<1>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<1>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip16<1>(outRows[38], outRows[46], outRows[38], outRows[46]);
  zip16<1>(outRows[54], outRows[62], outRows[54], outRows[62]);
  zip16<1>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip16<1>(outRows[39], outRows[47], outRows[39], outRows[47]);
  zip16<1>(outRows[55], outRows[63], outRows[55], outRows[63]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<1>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<1>(outRows[32], outRows[36], outRows[32], outRows[36]);
  zip16<1>(outRows[40], outRows[44], outRows[40], outRows[44]);
  zip16<1>(outRows[48], outRows[52], outRows[48], outRows[52]);
  zip16<1>(outRows[56], outRows[60], outRows[56], outRows[60]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<1>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<1>(outRows[33], outRows[37], outRows[33], outRows[37]);
  zip16<1>(outRows[41], outRows[45], outRows[41], outRows[45]);
  zip16<1>(outRows[49], outRows[53], outRows[49], outRows[53]);
  zip16<1>(outRows[57], outRows[61], outRows[57], outRows[61]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<1>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<1>(outRows[34], outRows[38], outRows[34], outRows[38]);
  zip16<1>(outRows[42], outRows[46], outRows[42], outRows[46]);
  zip16<1>(outRows[50], outRows[54], outRows[50], outRows[54]);
  zip16<1>(outRows[58], outRows[62], outRows[58], outRows[62]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<1>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<1>(outRows[27], outRows[31], outRows[27], outRows[31]);
  zip16<1>(outRows[35], outRows[39], outRows[35], outRows[39]);
  zip16<1>(outRows[43], outRows[47], outRows[43], outRows[47]);
  zip16<1>(outRows[51], outRows[55], outRows[51], outRows[55]);
  zip16<1>(outRows[59], outRows[63], outRows[59], outRows[63]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<1>(outRows[16], outRows[17], outRows[16], outRows[17]);
  zip<1>(outRows[18], outRows[19], outRows[18], outRows[19]);
  zip<1>(outRows[20], outRows[21], outRows[20], outRows[21]);
  zip<1>(outRows[22], outRows[23], outRows[22], outRows[23]);
  zip<1>(outRows[24], outRows[25], outRows[24], outRows[25]);
  zip<1>(outRows[26], outRows[27], outRows[26], outRows[27]);
  zip<1>(outRows[28], outRows[29], outRows[28], outRows[29]);
  zip<1>(outRows[30], outRows[31], outRows[30], outRows[31]);
  zip<1>(outRows[32], outRows[33], outRows[32], outRows[33]);
  zip<1>(outRows[34], outRows[35], outRows[34], outRows[35]);
  zip<1>(outRows[36], outRows[37], outRows[36], outRows[37]);
  zip<1>(outRows[38], outRows[39], outRows[38], outRows[39]);
  zip<1>(outRows[40], outRows[41], outRows[40], outRows[41]);
  zip<1>(outRows[42], outRows[43], outRows[42], outRows[43]);
  zip<1>(outRows[44], outRows[45], outRows[44], outRows[45]);
  zip<1>(outRows[46], outRows[47], outRows[46], outRows[47]);
  zip<1>(outRows[48], outRows[49], outRows[48], outRows[49]);
  zip<1>(outRows[50], outRows[51], outRows[50], outRows[51]);
  zip<1>(outRows[52], outRows[53], outRows[52], outRows[53]);
  zip<1>(outRows[54], outRows[55], outRows[54], outRows[55]);
  zip<1>(outRows[56], outRows[57], outRows[56], outRows[57]);
  zip<1>(outRows[58], outRows[59], outRows[58], outRows[59]);
  zip<1>(outRows[60], outRows[61], outRows[60], outRows[61]);
  zip<1>(outRows[62], outRows[63], outRows[62], outRows[63]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<2>(outRows[32], outRows[34], outRows[32], outRows[34]);
  zip<2>(outRows[36], outRows[38], outRows[36], outRows[38]);
  zip<2>(outRows[40], outRows[42], outRows[40], outRows[42]);
  zip<2>(outRows[44], outRows[46], outRows[44], outRows[46]);
  zip<2>(outRows[48], outRows[50], outRows[48], outRows[50]);
  zip<2>(outRows[52], outRows[54], outRows[52], outRows[54]);
  zip<2>(outRows[56], outRows[58], outRows[56], outRows[58]);
  zip<2>(outRows[60], outRows[62], outRows[60], outRows[62]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  zip<2>(outRows[33], outRows[35], outRows[33], outRows[35]);
  zip<2>(outRows[37], outRows[39], outRows[37], outRows[39]);
  zip<2>(outRows[41], outRows[43], outRows[41], outRows[43]);
  zip<2>(outRows[45], outRows[47], outRows[45], outRows[47]);
  zip<2>(outRows[49], outRows[51], outRows[49], outRows[51]);
  zip<2>(outRows[53], outRows[55], outRows[53], outRows[55]);
  zip<2>(outRows[57], outRows[59], outRows[57], outRows[59]);
  zip<2>(outRows[61], outRows[63], outRows[61], outRows[63]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[4];
    outRows[4]               = outRows[16];
    outRows[16]              = outRows[2];
    outRows[2]               = outRows[8];
    outRows[8]               = outRows[32];
    outRows[32]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[12];
    outRows[12]              = outRows[48];
    outRows[48]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[5];
    outRows[5]               = outRows[20];
    outRows[20]              = outRows[18];
    outRows[18]              = outRows[10];
    outRows[10]              = outRows[40];
    outRows[40]              = outRows[33];
    outRows[33]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[6];
    outRows[6]               = outRows[24];
    outRows[24]              = outRows[34];
    outRows[34]              = outRows[9];
    outRows[9]               = outRows[36];
    outRows[36]              = outRows[17];
    outRows[17]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[7];
    outRows[7]               = outRows[28];
    outRows[28]              = outRows[50];
    outRows[50]              = outRows[11];
    outRows[11]              = outRows[44];
    outRows[44]              = outRows[49];
    outRows[49]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[13];
    outRows[13]              = outRows[52];
    outRows[52]              = outRows[19];
    outRows[19]              = outRows[14];
    outRows[14]              = outRows[56];
    outRows[56]              = outRows[35];
    outRows[35]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[15];
    outRows[15]              = outRows[60];
    outRows[60]              = outRows[51];
    outRows[51]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[21];
    outRows[21]              = outRows[22];
    outRows[22]              = outRows[26];
    outRows[26]              = outRows[42];
    outRows[42]              = outRows[41];
    outRows[41]              = outRows[37];
    outRows[37]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[23];
    outRows[23]              = outRows[30];
    outRows[30]              = outRows[58];
    outRows[58]              = outRows[43];
    outRows[43]              = outRows[45];
    outRows[45]              = outRows[53];
    outRows[53]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[25];
    outRows[25]              = outRows[38];
    outRows[38]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[27];
    outRows[27]              = outRows[46];
    outRows[46]              = outRows[57];
    outRows[57]              = outRows[39];
    outRows[39]              = outRows[29];
    outRows[29]              = outRows[54];
    outRows[54]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[31];
    outRows[31]              = outRows[62];
    outRows[62]              = outRows[59];
    outRows[59]              = outRows[47];
    outRows[47]              = outRows[61];
    outRows[61]              = outRows[55];
    outRows[55]              = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<32>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[16], outRows[0], outRows[16]);
  zip16<1>(inRows[1], inRows[17], outRows[1], outRows[17]);
  zip16<1>(inRows[2], inRows[18], outRows[2], outRows[18]);
  zip16<1>(inRows[3], inRows[19], outRows[3], outRows[19]);
  zip16<1>(inRows[4], inRows[20], outRows[4], outRows[20]);
  zip16<1>(inRows[5], inRows[21], outRows[5], outRows[21]);
  zip16<1>(inRows[6], inRows[22], outRows[6], outRows[22]);
  zip16<1>(inRows[7], inRows[23], outRows[7], outRows[23]);
  zip16<1>(inRows[8], inRows[24], outRows[8], outRows[24]);
  zip16<1>(inRows[9], inRows[25], outRows[9], outRows[25]);
  zip16<1>(inRows[10], inRows[26], outRows[10], outRows[26]);
  zip16<1>(inRows[11], inRows[27], outRows[11], outRows[27]);
  zip16<1>(inRows[12], inRows[28], outRows[12], outRows[28]);
  zip16<1>(inRows[13], inRows[29], outRows[13], outRows[29]);
  zip16<1>(inRows[14], inRows[30], outRows[14], outRows[30]);
  zip16<1>(inRows[15], inRows[31], outRows[15], outRows[31]);
  zip16<1>(outRows[0], outRows[8], outRows[0], outRows[8]);
  zip16<1>(outRows[16], outRows[24], outRows[16], outRows[24]);
  zip16<1>(outRows[1], outRows[9], outRows[1], outRows[9]);
  zip16<1>(outRows[17], outRows[25], outRows[17], outRows[25]);
  zip16<1>(outRows[2], outRows[10], outRows[2], outRows[10]);
  zip16<1>(outRows[18], outRows[26], outRows[18], outRows[26]);
  zip16<1>(outRows[3], outRows[11], outRows[3], outRows[11]);
  zip16<1>(outRows[19], outRows[27], outRows[19], outRows[27]);
  zip16<1>(outRows[4], outRows[12], outRows[4], outRows[12]);
  zip16<1>(outRows[20], outRows[28], outRows[20], outRows[28]);
  zip16<1>(outRows[5], outRows[13], outRows[5], outRows[13]);
  zip16<1>(outRows[21], outRows[29], outRows[21], outRows[29]);
  zip16<1>(outRows[6], outRows[14], outRows[6], outRows[14]);
  zip16<1>(outRows[22], outRows[30], outRows[22], outRows[30]);
  zip16<1>(outRows[7], outRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[23], outRows[31], outRows[23], outRows[31]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[16], outRows[20], outRows[16], outRows[20]);
  zip16<1>(outRows[24], outRows[28], outRows[24], outRows[28]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[17], outRows[21], outRows[17], outRows[21]);
  zip16<1>(outRows[25], outRows[29], outRows[25], outRows[29]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[18], outRows[22], outRows[18], outRows[22]);
  zip16<1>(outRows[26], outRows[30], outRows[26], outRows[30]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  zip16<1>(outRows[19], outRows[23], outRows[19], outRows[23]);
  zip16<1>(outRows[27], outRows[31], outRows[27], outRows[31]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<1>(outRows[16], outRows[17], outRows[16], outRows[17]);
  zip<1>(outRows[18], outRows[19], outRows[18], outRows[19]);
  zip<1>(outRows[20], outRows[21], outRows[20], outRows[21]);
  zip<1>(outRows[22], outRows[23], outRows[22], outRows[23]);
  zip<1>(outRows[24], outRows[25], outRows[24], outRows[25]);
  zip<1>(outRows[26], outRows[27], outRows[26], outRows[27]);
  zip<1>(outRows[28], outRows[29], outRows[28], outRows[29]);
  zip<1>(outRows[30], outRows[31], outRows[30], outRows[31]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[16], outRows[18], outRows[16], outRows[18]);
  zip<2>(outRows[20], outRows[22], outRows[20], outRows[22]);
  zip<2>(outRows[24], outRows[26], outRows[24], outRows[26]);
  zip<2>(outRows[28], outRows[30], outRows[28], outRows[30]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  zip<2>(outRows[17], outRows[19], outRows[17], outRows[19]);
  zip<2>(outRows[21], outRows[23], outRows[21], outRows[23]);
  zip<2>(outRows[25], outRows[27], outRows[25], outRows[27]);
  zip<2>(outRows[29], outRows[31], outRows[29], outRows[31]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[4];
    outRows[4]               = outRows[16];
    outRows[16]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[2];
    outRows[2]               = outRows[8];
    outRows[8]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[12];
    outRows[12]              = outRows[18];
    outRows[18]              = outRows[9];
    outRows[9]               = outRows[6];
    outRows[6]               = outRows[24];
    outRows[24]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[5];
    outRows[5]               = outRows[20];
    outRows[20]              = outRows[17];
    outRows[17]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[7];
    outRows[7]               = outRows[28];
    outRows[28]              = outRows[19];
    outRows[19]              = outRows[13];
    outRows[13]              = outRows[22];
    outRows[22]              = outRows[25];
    outRows[25]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[11];
    outRows[11]              = outRows[14];
    outRows[14]              = outRows[26];
    outRows[26]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[15];
    outRows[15]              = outRows[30];
    outRows[30]              = outRows[27];
    outRows[27]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[23];
    outRows[23]              = outRows[29];
    outRows[29]              = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<16>,
  Bytes<64>)
{
  zip16<1>(inRows[0], inRows[8], outRows[0], outRows[8]);
  zip16<1>(inRows[1], inRows[9], outRows[1], outRows[9]);
  zip16<1>(inRows[2], inRows[10], outRows[2], outRows[10]);
  zip16<1>(inRows[3], inRows[11], outRows[3], outRows[11]);
  zip16<1>(inRows[4], inRows[12], outRows[4], outRows[12]);
  zip16<1>(inRows[5], inRows[13], outRows[5], outRows[13]);
  zip16<1>(inRows[6], inRows[14], outRows[6], outRows[14]);
  zip16<1>(inRows[7], inRows[15], outRows[7], outRows[15]);
  zip16<1>(outRows[0], outRows[4], outRows[0], outRows[4]);
  zip16<1>(outRows[8], outRows[12], outRows[8], outRows[12]);
  zip16<1>(outRows[1], outRows[5], outRows[1], outRows[5]);
  zip16<1>(outRows[9], outRows[13], outRows[9], outRows[13]);
  zip16<1>(outRows[2], outRows[6], outRows[2], outRows[6]);
  zip16<1>(outRows[10], outRows[14], outRows[10], outRows[14]);
  zip16<1>(outRows[3], outRows[7], outRows[3], outRows[7]);
  zip16<1>(outRows[11], outRows[15], outRows[11], outRows[15]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<1>(outRows[8], outRows[9], outRows[8], outRows[9]);
  zip<1>(outRows[10], outRows[11], outRows[10], outRows[11]);
  zip<1>(outRows[12], outRows[13], outRows[12], outRows[13]);
  zip<1>(outRows[14], outRows[15], outRows[14], outRows[15]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[8], outRows[10], outRows[8], outRows[10]);
  zip<2>(outRows[12], outRows[14], outRows[12], outRows[14]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  zip<2>(outRows[9], outRows[11], outRows[9], outRows[11]);
  zip<2>(outRows[13], outRows[15], outRows[13], outRows[15]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[4];
    outRows[4]               = outRows[2];
    outRows[2]               = outRows[8];
    outRows[8]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[12];
    outRows[12]              = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[5];
    outRows[5]               = outRows[6];
    outRows[6]               = outRows[10];
    outRows[10]              = outRows[9];
    outRows[9]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[7];
    outRows[7]               = outRows[14];
    outRows[14]              = outRows[11];
    outRows[11]              = outRows[13];
    outRows[13]              = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems], Elements<8>, Bytes<64>)
{
  zip16<1>(inRows[0], inRows[4], outRows[0], outRows[4]);
  zip16<1>(inRows[1], inRows[5], outRows[1], outRows[5]);
  zip16<1>(inRows[2], inRows[6], outRows[2], outRows[6]);
  zip16<1>(inRows[3], inRows[7], outRows[3], outRows[7]);
  // correction steps follow below (if required)
  zip<1>(outRows[0], outRows[1], outRows[0], outRows[1]);
  zip<1>(outRows[2], outRows[3], outRows[2], outRows[3]);
  zip<1>(outRows[4], outRows[5], outRows[4], outRows[5]);
  zip<1>(outRows[6], outRows[7], outRows[6], outRows[7]);
  zip<2>(outRows[0], outRows[2], outRows[0], outRows[2]);
  zip<2>(outRows[4], outRows[6], outRows[4], outRows[6]);
  zip<2>(outRows[1], outRows[3], outRows[1], outRows[3]);
  zip<2>(outRows[5], outRows[7], outRows[5], outRows[7]);
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[1];
    outRows[1]               = outRows[4];
    outRows[4]               = vec_v;
  }
  {
    Vec<T, SIMD_WIDTH> vec_v = outRows[3];
    outRows[3]               = outRows[6];
    outRows[6]               = vec_v;
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose2inplcLane(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  transpose2inplcLane(inRows, outRows, Elements<Vec<T, SIMD_WIDTH>::elements>(),
                      Bytes<SIMD_WIDTH>());
}

} // namespace ext
} // namespace internal
} // namespace simd

#endif // SIMD_VEC_EXT_TRANSPOSE_AUTOGEN_H_

#include <cassert>
#include <cmath>
#include <cstddef>
#include <cstdio>
#include <string>
#include <type_traits>

namespace simd {
namespace internal {
namespace ext {
// https://stackoverflow.com/questions/23781506/compile-time-computing-of-number-of-bits-needed-to-encode-n-different-states
template <typename T>
static constexpr SIMD_INLINE T floorlog2(T x)
{
  static_assert(std::is_integral<T>::value, "");
  return x == 1 ? 0 : 1 + floorlog2(x >> 1);
}
} // namespace ext
} // namespace internal

// determine NATIVE_SIMD_REG_COUNT
// https://stackoverflow.com/questions/62419256/how-can-i-determine-how-many-avx-registers-my-processor-has

// exclude from doxygen (until endcond)
/// @cond

#ifdef __x86_64__
#ifdef __AVX512VL__
#define NATIVE_SIMD_REG_COUNT 32
#else
#define NATIVE_SIMD_REG_COUNT 16
#endif
#else
#define NATIVE_SIMD_REG_COUNT 8
#endif
/// @endcond

// ===========================================================================
// print functions (for tests)
// ===========================================================================

// 04. Aug 22 (Jonas Keller):
// removed treatZero(), not needed anymore because of change below
//
// // integer types don't have negative zero
// template <typename T>
// static SIMD_INLINE T
// treatZero(T in)
// {
//   return in;
// }
//
// // Float: map -0.0f to 0.0f
// static SIMD_INLINE Float
// treatZero(Float in)
// {
//   return (in == -0.0f) ? 0.0f : in;
// }

/**
 * @addtogroup group_print
 * @{
 */

/**
 * @brief Writes the formatted elements of a Vec to a file.
 *
 * The elements are formatted using the format string and written to
 * the file using the fprintf function of the C standard library in the order
 * they are stored in memory.
 *
 * @param f file to write to
 * @param format format string Must be a valid format string to print a single
 * element of the Vec using the fprintf function of the C standard library
 * @param vec Vec to print
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void fprint(FILE *f, const char *format,
                               const Vec<T, SIMD_WIDTH> &vec)
{
  // buffer
  // 19. Jul 16 (rm)
  const auto elems = Vec<T, SIMD_WIDTH>::elems; // SIMD_WIDTH/sizeof(T)
  // T buf[SIMD_WIDTH];
  T buf[elems];
  // store vector (unaligned, not time-critical)
  storeu(buf, vec);
  // print elements of vector to f
  for (size_t i = 0; i < elems; i++)
    // 04. Aug 22 (Jonas Keller):
    // removed mapping from -0.0f to 0.0f,
    // for debugging you want to see -0.0f
    // fprintf(f, format, treatZero(buf[i]));
    fprintf(f, format, buf[i]);
}

/**
 * @brief Writes the formatted elements of a Vec to stdout.
 *
 * Equivalent to calling fprint(FILE *f, const char *format,
 * const Vec<T,SIMD_WIDTH> &vec) with stdout as the file.
 *
 * @param format format string Must be a valid format string to print a single
 * element of the Vec using the fprintf function of the C standard library
 * @param vec Vec to print
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void print(const char *format, const Vec<T, SIMD_WIDTH> &vec)
{
  fprint(stdout, format, vec);
}

/**
 * @brief Writes the formatted elements of a Vec to a file separated
 * by a separator string.
 *
 * Equivalent to calling fprint(FILE *f, const char *format,
 * const Vec<T,SIMD_WIDTH> &vec) with a format string
 * consisting of the format string and the separator string.
 *
 * @param f file to write to
 * @param format format string Must be a valid format string to print a single
 * element of the Vec using the fprintf function of the C standard library
 * @param separator separator string
 * @param vec Vec to print
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void fprint(FILE *f, const char *format,
                               const char *separator,
                               const Vec<T, SIMD_WIDTH> &vec)
{
  // 09. Jan 23 (Jonas Keller): used std::string instead of strcpy and strcat
  // to avoid potential buffer overflows
  // char fmtSep[256];
  // strcat(strcpy(fmtSep, format), separator);
  std::string fmtSep = std::string(format) + std::string(separator);
  fprint(f, fmtSep.c_str(), vec);
}

/**
 * @brief Writes the formatted elements of a Vec to stdout separated
 * by a separator string.
 *
 * Equivalent to calling fprint(FILE *f, const char *format,
 * const char *separator, const Vec<T,SIMD_WIDTH> &vec) with stdout as
 * the file.
 *
 * @param format format string Must be a valid format string to print a single
 * element of the Vec using the fprintf function of the C standard library
 * @param separator separator string
 * @param vec Vec to print
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void print(const char *format, const char *separator,
                              const Vec<T, SIMD_WIDTH> &vec)
{
  fprint(stdout, format, separator, vec);
}

/** @} */

// ===========================================================================
// multi-vector store and load
// ===========================================================================

/**
 * @ingroup group_memory_load
 * @brief Loads multiple successive Vec's from aligned memory.
 *
 * The memory location must be aligned to the SIMD_WIDTH.
 *
 * @param[in] p pointer to the aligned memory location to load from
 * @param[out] inVecs array of Vec's to store the loaded values in
 * @param numInVecs number of Vec's to load
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void load(const T *const p, Vec<T, SIMD_WIDTH> inVecs[],
                             size_t numInVecs)
{
  for (size_t i = 0; i < numInVecs; i++) {
    inVecs[i] = load<SIMD_WIDTH>(&p[i * Vec<T, SIMD_WIDTH>::elems]);
  }
}

/**
 * @ingroup group_memory_load
 * @brief Loads multiple successive Vec's from unaligned memory.
 *
 * In contrast to load(const T *const, Vec<T, SIMD_WIDTH>[], size_t), the
 * memory location does not need to be aligned to any boundary.
 *
 * @param[in] p pointer to the memory location to load from
 * @param[out] inVecs array of Vec's to store the loaded values in
 * @param numInVecs number of Vec's to load
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void loadu(const T *const p, Vec<T, SIMD_WIDTH> inVecs[],
                              size_t numInVecs)
{
  for (size_t i = 0; i < numInVecs; i++) {
    inVecs[i] = loadu<SIMD_WIDTH>(&p[i * Vec<T, SIMD_WIDTH>::elems]);
  }
}

/**
 * @ingroup group_memory_store
 * @brief Stores multiple successive Vec's to aligned memory.
 *
 * The memory location must be aligned to the SIMD_WIDTH.
 *
 * @param[out] p pointer to the aligned memory location to store to
 * @param[in] outVecs array of Vec's to store
 * @param numOutVecs number of Vec's to store
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store(T *const p, const Vec<T, SIMD_WIDTH> outVecs[],
                              size_t numOutVecs)
{
  for (size_t i = 0; i < numOutVecs; i++) {
    store(&p[i * Vec<T, SIMD_WIDTH>::elems], outVecs[i]);
  }
}

/**
 * @ingroup group_memory_store
 * @brief Stores multiple successive Vec's to unaligned memory.
 *
 * In contrast to store(T *const, const Vec<T, SIMD_WIDTH>[], size_t), the
 * memory location does not need to be aligned to any boundary.
 *
 * @param[out] p pointer to the memory location to store to
 * @param[in] outVecs array of Vec's to store
 * @param numOutVecs number of Vec's to store
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu(T *const p, const Vec<T, SIMD_WIDTH> outVecs[],
                               size_t numOutVecs)
{
  for (size_t i = 0; i < numOutVecs; i++) {
    storeu(&p[i * Vec<T, SIMD_WIDTH>::elems], outVecs[i]);
  }
}

/**
 * @ingroup group_memory_store
 * @brief Stores a single Vec multiple times to aligned memory.
 *
 * The memory location must be aligned to the SIMD_WIDTH.
 *
 * @param[out] p pointer to the aligned memory location to store to
 * @param[in] outVec Vec to store
 * @param numOutVecs number of times to store the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store(T *const p, const Vec<T, SIMD_WIDTH> &outVec,
                              size_t numOutVecs)
{
  for (size_t i = 0; i < numOutVecs; i++) {
    store(&p[i * Vec<T, SIMD_WIDTH>::elems], outVec);
  }
}

/**
 * @ingroup group_memory_store
 * @brief Stores a single Vec multiple times to unaligned memory.
 *
 * In contrast to store(T *const p, const Vec<T, SIMD_WIDTH> &outVec,
 * size_t numOutVecs), the memory location does not need to be aligned to
 * any boundary.
 *
 * @param[out] p pointer to the memory location to store to
 * @param[in] outVec Vec to store
 * @param numOutVecs number of times to store the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu(T *const p, const Vec<T, SIMD_WIDTH> &outVec,
                               size_t numOutVecs)
{
  for (size_t i = 0; i < numOutVecs; i++) {
    storeu(&p[i * Vec<T, SIMD_WIDTH>::elems], outVec);
  }
}

// -------------------- different store functions ----------------------------
namespace internal {
namespace ext {
template <typename T, size_t SIMD_WIDTH>
struct Store
{
  static SIMD_INLINE void _store(T *const p, const Vec<T, SIMD_WIDTH> &outVec)
  {
    return store(p, outVec);
  }
};

template <typename T, size_t SIMD_WIDTH>
struct StoreU
{
  static SIMD_INLINE void _store(T *const p, const Vec<T, SIMD_WIDTH> &outVec)
  {
    return storeu(p, outVec);
  }
};

// ---------------------------------------------------------------------------
// Meta Template Class Store16
// used to store matrix after transposed with Transpose<Unpack16>
//
// TODO: Currently storing complete quadratic matrix. Integrate numOutVecs?
//
// gcc error with inline template function: inlining failed in call to
// always_inline ‘void storeu16(..) [..]’: recursive inlining
// ---------------------------------------------------------------------------

template <template <typename, size_t> class Store, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW, size_t STORE_STOP,
          size_t STORE_WIDTH, size_t SRC_OFF, size_t DST_OFF>
struct Store16
{
  static SIMD_INLINE void _store16(
    T *const p, const Vec<T, SIMD_WIDTH> outVecs[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("STORE_WIDTH=%d, SRC_OFFSET=%d, DST_OFFSET=%d\n",
    // STORE_WIDTH, SRC_OFFSET, DST_OFFSET);
    Store16<Store, T, SIMD_WIDTH, NUMROWS, ROW, STORE_STOP, STORE_WIDTH / 2,
            SRC_OFF, 2 * DST_OFF>::_store16(p, outVecs);
    Store16<Store, T, SIMD_WIDTH, NUMROWS, ROW, STORE_STOP, STORE_WIDTH / 2,
            SRC_OFF + SIMD_WIDTH / STORE_WIDTH,
            2 * DST_OFF + STORE_STOP>::_store16(p, outVecs);
  }
};

template <template <typename, size_t> class Store, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW, size_t STORE_STOP,
          size_t SRC_OFF, size_t DST_OFF>
struct Store16<Store, T, SIMD_WIDTH, NUMROWS, ROW, STORE_STOP, 16, SRC_OFF,
               DST_OFF>
{
  static constexpr auto STEP = SIMD_WIDTH / 16;
  static constexpr auto VO   = SRC_OFF + ROW * STEP;
  static constexpr auto OFF  = (DST_OFF + ROW) * NUMROWS;

  static SIMD_INLINE void _store16(
    T *const p, const Vec<T, SIMD_WIDTH> outVecs[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("VO=%d\n", OFF=%d\n", VO, OFF);
    Store<T, SIMD_WIDTH>::_store(p + OFF, outVecs[VO]);
    Store16<Store, T, SIMD_WIDTH, NUMROWS, ROW + 1, STORE_STOP, 16, SRC_OFF,
            DST_OFF>::_store16(p, outVecs);
  }
};

template <template <typename, size_t> class Store, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t STORE_STOP, size_t SRC_OFF,
          size_t DST_OFF>
struct Store16<Store, T, SIMD_WIDTH, NUMROWS, STORE_STOP, STORE_STOP, 16,
               SRC_OFF, DST_OFF>
{
  static SIMD_INLINE void _store16(
    T *const, const Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems])
  {}
};

// -------------------- store16 functions ------------------------------------

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store16(
  T *const p, const Vec<T, SIMD_WIDTH> outVecs[Vec<T, SIMD_WIDTH>::elems])
{
  const auto numRows   = SIMD_WIDTH / sizeof(T);
  const auto storeStop = 16 / sizeof(T);
  internal::ext::Store16<internal::ext::Store, T, SIMD_WIDTH, numRows, 0,
                         storeStop, SIMD_WIDTH, 0, 0>::_store16(p, outVecs);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu16(
  T *const p, const Vec<T, SIMD_WIDTH> outVecs[Vec<T, SIMD_WIDTH>::elems])
{
  const auto numRows   = SIMD_WIDTH / sizeof(T);
  const auto storeStop = 16 / sizeof(T);
  internal::ext::Store16<internal::ext::StoreU, T, SIMD_WIDTH, numRows, 0,
                         storeStop, SIMD_WIDTH, 0, 0>::_store16(p, outVecs);
}

} // namespace ext
} // namespace internal

// ===========================================================================
// copy (load and store)
// ===========================================================================

/**
 * @addtogroup group_memory
 * @{
 */

/**
 * @brief Copies a single Vec from one aligned memory location to
 * another aligned memory location.
 *
 * Both memory locations must be aligned to the SIMD_WIDTH.
 *
 * @param[in] src pointer to the aligned source memory location
 * @param[out] dst pointer to the aligned destination memory location
 */
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE void load_store(const T *const src, T *const dst)
{
  Vec<T, SIMD_WIDTH> copy = load<SIMD_WIDTH>(src);
  store(dst, copy);
}

/**
 * @brief Copies a single Vec from one unaligned memory location
 * to another aligned memory location.
 *
 * The destination memory location must be aligned to the SIMD_WIDTH, the
 * source memory location does not have to be aligned to any boundary.
 *
 * @param[in] src pointer to the unaligned source memory location
 * @param[out] dst pointer to the aligned destination memory location
 */
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE void loadu_store(const T *const src, T *const dst)
{
  Vec<T, SIMD_WIDTH> copy = loadu<SIMD_WIDTH>(src);
  store(dst, copy);
}

/**
 * @brief Copies a single Vec from one aligned memory location to
 * another unaligned memory location.
 *
 * The source memory location must be aligned to the SIMD_WIDTH, the
 * destination memory location does not have to be aligned to any boundary.
 *
 * @param[in] src pointer to the aligned source memory location
 * @param[out] dst pointer to the unaligned destination memory location
 */
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE void load_storeu(const T *const src, T *const dst)
{
  Vec<T, SIMD_WIDTH> copy = load<SIMD_WIDTH>(src);
  storeu(dst, copy);
}

/**
 * @brief Copies a single Vec from one unaligned memory location
 * to another unaligned memory location.
 *
 * Both memory locations do not have to be aligned to any boundary.
 *
 * @param[in] src pointer to the unaligned source memory location
 * @param[out] dst pointer to the unaligned destination memory location
 */
template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE void loadu_storeu(const T *const src, T *const dst)
{
  Vec<T, SIMD_WIDTH> copy = loadu<SIMD_WIDTH>(src);
  storeu(dst, copy);
}

/** @} */

// ===========================================================================
// generalized packs
// ===========================================================================

// input is only signed
// same-size input and output is allowed

namespace internal {
namespace ext {

// no stage

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> packs(const Vec<T, SIMD_WIDTH> a[1],
                                            OutputType<T>, Compression<1>)
{
  return a[0];
}

// single stage

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> a[2],
                                               OutputType<Tout>, Compression<2>)
{
  return packs<Tout>(a[0], a[1]);
}

// two stages

// via Short if Tout is Byte or SignedByte
template <typename Tout, typename Tin, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(sizeof(Tout) == 1)>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> a[4],
                                               OutputType<Tout>, Compression<4>)
{
  return packs<Tout>(packs<Short>(a[0], a[1]), packs<Short>(a[2], a[3]));
}

// via Int if Tout is Word or Short
template <typename Tout, typename Tin, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(sizeof(Tout) == 2), typename = void>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> a[4],
                                               OutputType<Tout>, Compression<4>)
{
  return packs<Tout>(packs<Int>(a[0], a[1]), packs<Int>(a[2], a[3]));
}

// two stages from Double

template <typename Tout, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(
  const Vec<Double, SIMD_WIDTH> a[4], OutputType<Tout>, Compression<4>)
{
  // always via Int
  return packs<Tout>(packs<Int>(a[0], a[1]), packs<Int>(a[2], a[3]));
}

// three stages

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> a[8],
                                               OutputType<Tout>, Compression<8>)
{
  // always via Short
  return packs<Tout>(packs<Short>(a), packs<Short>(a + 4));
}

// special cases: int <-> float, long <-> double

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(const Vec<Tin, SIMD_WIDTH> a[1],
                                               OutputType<Tout>, Compression<1>)
{
  static_assert(sizeof(Tin) == sizeof(Tout), "");
  static_assert(std::is_floating_point<Tin>::value !=
                  std::is_floating_point<Tout>::value,
                "");
  return cvts<Tout>(a[0]);
}

} // namespace ext
} // namespace internal

// generalized version of packs: includes multistage packing

/**
 * @ingroup group_type_conversion
 * @brief Packs multiple Vec's into a single Vec by converting the
 * elements into smaller or equally sized types.
 *
 * TODO: allowed types
 *
 * In contrast to packs(const Vec<Tin, SIMD_WIDTH>&, const Vec<Tin,
 * SIMD_WIDTH>&), this function can handle multistage packing, i.e. the
 * number of input can be different from 2.
 *
 * @sa packs(const Vec<Tin, SIMD_WIDTH>&, const Vec<Tin,
 * SIMD_WIDTH>&)
 *
 * @tparam Tout type of the resulting Vec
 * @tparam Tin type of the input Vec's
 * @param[in] a array of input Vec's
 * @return Vec with the packed elements
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(
  const Vec<Tin, SIMD_WIDTH> a[sizeof(Tin) / sizeof(Tout)])
{
  return internal::ext::packs(
    a, internal::OutputType<Tout>(),
    internal::Compression<sizeof(Tin) / sizeof(Tout)>());
}

// ===========================================================================
// generalized convert (using extend and packs)
// ===========================================================================

namespace internal {
namespace ext {
template <typename Tout, typename Tin, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(sizeof(Tout) < sizeof(Tin))>
static SIMD_INLINE void convert(
  const Vec<Tin, SIMD_WIDTH> inVecs[sizeof(Tin) / sizeof(Tout)],
  Vec<Tout, SIMD_WIDTH> outVecs[1])
{
  outVecs[0] = packs<Tout>(inVecs);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(sizeof(Tout) >= sizeof(Tin)), typename = void>
static SIMD_INLINE void convert(const Vec<Tin, SIMD_WIDTH> inVecs[1],
                                Vec<Tout, SIMD_WIDTH> outVecs[1])
{
  extend(inVecs[0], outVecs);
}
} // namespace ext
} // namespace internal

/**
 * @ingroup group_type_conversion
 * @brief Converts (potentially multiple) Vec's between different types.
 *
 * Combines extend() and packs().
 *
 * TODO: allowed types
 *
 * If the input and output types are of the same size, both the input and
 * output are just one Vec. If the types are of different sizes, the input or
 * output consists of multiple Vec's, such that the number of input elements
 * is equal to the number of output elements.
 *
 * @sa extend()
 * @sa packs()
 *
 * @tparam Tout element type of the output Vec's
 * @tparam Tin element type of the input Vec's
 * @param[in] inVecs input Vec's
 * @param[out] outVecs output Vec's
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void convert(
  const Vec<Tin, SIMD_WIDTH> inVecs[numInVecs<Tout, Tin>()],
  Vec<Tout, SIMD_WIDTH> outVecs[numOutVecs<Tout, Tin>()])
{
  internal::ext::convert(inVecs, outVecs);
}

// ===========================================================================
// float-based operations on arbitrary input and output types
// ===========================================================================

// internal helper functions for float-based operations:
namespace internal {
namespace ext {
template <typename Tin, typename Tout, typename Tfloat>
static constexpr SIMD_INLINE size_t numCalcVecs()
{
  static_assert(numInVecs<Tout, Tin>() * sizeof(Tfloat) / sizeof(Tin) ==
                  numOutVecs<Tout, Tin>() * sizeof(Tfloat) / sizeof(Tout),
                "numCalcVecs() must be equal for input and output");
  return numInVecs<Tout, Tin>() * sizeof(Tfloat) / sizeof(Tin);
}

template <typename Tin, typename Tout, typename Tfloat, size_t SIMD_WIDTH>
static SIMD_INLINE void extendInToFloat(
  const Vec<Tin, SIMD_WIDTH> inVecs[numInVecs<Tout, Tin>()],
  Vec<Tfloat, SIMD_WIDTH> floatVecs[numCalcVecs<Tout, Tin, Tfloat>()])
{
  for (size_t i = 0; i < numInVecs<Tout, Tin>(); ++i) {
    extend(inVecs[i], &floatVecs[i * sizeof(Tfloat) / sizeof(Tin)]);
  }
}

template <typename Tin, typename Tout, typename Tfloat, size_t SIMD_WIDTH>
static SIMD_INLINE void packsOutFromFloat(
  const Vec<Tfloat, SIMD_WIDTH> floatVecs[numCalcVecs<Tout, Tin, Tfloat>()],
  Vec<Tout, SIMD_WIDTH> outVecs[numOutVecs<Tout, Tin>()])
{
  for (size_t i = 0; i < numOutVecs<Tout, Tin>(); ++i) {
    outVecs[i] = packs<Tout>(&floatVecs[i * sizeof(Tfloat) / sizeof(Tout)]);
  }
}
} // namespace ext
} // namespace internal

/**
 * @addtogroup group_fops
 * @{
 */

// ---------------------------------------------------------------------------
// divide then multiply with float constant in float arithmetic
// ---------------------------------------------------------------------------

// TODO: fdivmul: better fmuldiv = first multiply then divide?

// 15. Mar 23 (Jonas Keller): fused the three cases

/**
 * @brief Divides Vec's element-wise, then multiplies with a constant factor in
 * floating point arithmetic.
 *
 * @tparam Tfloat the floating point type to perform the calculation in.
 * Defaults to @ref BigEnoughFloat<Tout, Tin>
 * @param[in] vecsNum numerator Vec's
 * @param[in] vecsDenom denominator Vec's
 * @param fac factor to multiply with
 * @param[out] vecsOut output Vec's
 * @sa numInVecs(), numOutVecs()
 */
template <typename Tout, typename Tin,
          typename Tfloat = BigEnoughFloat<Tout, Tin>, size_t SIMD_WIDTH>
static SIMD_INLINE void fdivmul(
  const Vec<Tin, SIMD_WIDTH> vecsNum[numInVecs<Tout, Tin>()],
  const Vec<Tin, SIMD_WIDTH> vecsDenom[numInVecs<Tout, Tin>()],
  dont_deduce<Tfloat> fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Tfloat), "");
  static_assert(sizeof(Tout) <= sizeof(Tfloat), "");
  constexpr auto nFloatVecs = internal::ext::numCalcVecs<Tout, Tin, Tfloat>();
  const auto facVec         = set1<Tfloat, SIMD_WIDTH>(fac);
  Vec<Tfloat, SIMD_WIDTH> numF[nFloatVecs], denomF[nFloatVecs],
    resF[nFloatVecs];
  internal::ext::extendInToFloat<Tin, Tout>(vecsNum, numF);
  internal::ext::extendInToFloat<Tin, Tout>(vecsDenom, denomF);
  for (size_t i = 0; i < nFloatVecs; i++) {
    resF[i] = mul(div(numF[i], denomF[i]), facVec);
  }
  internal::ext::packsOutFromFloat<Tin, Tout>(resF, vecsOut);
}

// ---------------------------------------------------------------------------
// divide, apply multidimensional sigmoid and then multiply with float
// constant in float arithmetic (derived from fdivmul)
// sigmoid(x) = ((y(x,a)/(1-y(x,a)**4)**0.25))+1)/2)
// y(x,a) = sum_d(a*w[d]*(x[d]-w0[d]))
// a = -0.433 from fitting this to 1/(1+exp(y(x,1))
// ---------------------------------------------------------------------------

namespace internal {
namespace ext {
template <size_t DIM, size_t NVEC, typename Tout, typename Tin,
          size_t SIMD_WIDTH, SIMD_ENABLE_IF(sizeof(Tout) < sizeof(Tin))>
static SIMD_INLINE void fdivMsigmoidmul(
  const Vec<Tin, SIMD_WIDTH> vecsNum[DIM][NVEC],
  const Vec<Tin, SIMD_WIDTH> vecsDenom[DIM][NVEC], const double w[DIM],
  const double w0[DIM], double fac, Vec<Tout, SIMD_WIDTH> vecsOut[1])
{
  const auto nIn   = sizeof(Tin) / sizeof(Tout);
  const auto fanIn = sizeof(Float) / sizeof(Tin);
  const auto facF  = set1<Float, SIMD_WIDTH>(fac / 2.0);
  const auto oneF  = set1<Float, SIMD_WIDTH>(1.0f);
  Vec<Float, SIMD_WIDTH> wF[DIM], w0F[DIM], numF[DIM][fanIn],
    denomF[DIM][fanIn], resF[nIn * fanIn];
  for (size_t d = 0; d < DIM; d++) {
    wF[d]  = set1<Float, SIMD_WIDTH>(-0.433 * w[d]);
    w0F[d] = set1<Float, SIMD_WIDTH>(w0[d]);
  }
  // i: index of input vector
  // j: index of extended input vector
  // k: index of output vectors
  // TODO: sometimes i < nIn does not work with -O2 is always true?
  for (size_t i = 0, k = 0; i < nIn; i++) {
    for (size_t d = 0; d < DIM; d++) {
      extend(vecsNum[d][i], numF[d]);
      extend(vecsDenom[d][i], denomF[d]);
    }
    for (size_t j = 0; j < fanIn; j++, k++) {
      auto yF = setzero<Float, SIMD_WIDTH>();
      for (size_t d = 0; d < DIM; d++) {
        yF = add(yF, mul(wF[d], sub(div(numF[d][j], denomF[d][j]), w0F[d])));
      }
      auto y4F = mul(yF, yF);
      y4F      = mul(y4F, y4F);
      resF[k]  = mul(add(div(yF, sqrt(sqrt(add(oneF, y4F)))), oneF), facF);
    }
  }
  vecsOut[0] = packs<Tout>(resF);
}

template <size_t DIM, size_t NVEC, typename Tout, typename Tin,
          size_t SIMD_WIDTH, SIMD_ENABLE_IF(sizeof(Tout) > sizeof(Tin)),
          typename = void>
static SIMD_INLINE void fdivMsigmoidmul(
  const Vec<Tin, SIMD_WIDTH> vecsNum[DIM][NVEC],
  const Vec<Tin, SIMD_WIDTH> vecsDenom[DIM][NVEC], const double w[DIM],
  const double w0[DIM], double fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[sizeof(Tout) / sizeof(Tin)])
{
  const auto nOut   = sizeof(Tout) / sizeof(Tin);
  const auto fanOut = sizeof(Float) / sizeof(Tout);
  const auto facF   = set1<Float, SIMD_WIDTH>(fac / 2.0);
  const auto oneF   = set1<Float, SIMD_WIDTH>(1.0f);
  Vec<Float, SIMD_WIDTH> wF[DIM], w0F[DIM], numF[DIM][nOut * fanOut],
    denomF[DIM][nOut * fanOut], resF[fanOut];
  for (size_t d = 0; d < DIM; d++) {
    wF[d]  = set1<Float, SIMD_WIDTH>(-0.433 * w[d]);
    w0F[d] = set1<Float, SIMD_WIDTH>(w0[d]);
    extend(*vecsNum[d], numF[d]);
    extend(*vecsDenom[d], denomF[d]);
  }
  // i: index of output vector
  // j: index of partial output vectors
  // k: index of input vector
  for (size_t i = 0, k = 0; i < nOut; i++) {
    for (size_t j = 0; j < fanOut; j++, k++) {
      auto yF = setzero<Float, SIMD_WIDTH>();
      for (size_t d = 0; d < DIM; d++) {
        yF = add(yF, mul(wF[d], sub(div(numF[d][k], denomF[d][k]), w0F[d])));
      }
      auto y4F = mul(yF, yF);
      y4F      = mul(y4F, y4F);
      resF[j]  = mul(add(div(yF, sqrt(sqrt(add(oneF, y4F)))), oneF), facF);
    }
    vecsOut[i] = packs<Tout>(resF);
  }
}

template <size_t DIM, size_t NVEC, typename Tout, typename Tin,
          size_t SIMD_WIDTH, SIMD_ENABLE_IF(sizeof(Tout) == sizeof(Tin)),
          typename = void, typename = void>
static SIMD_INLINE void fdivMsigmoidmul(
  const Vec<Tin, SIMD_WIDTH> vecsNum[DIM][NVEC],
  const Vec<Tin, SIMD_WIDTH> vecsDenom[DIM][NVEC], const double w[DIM],
  const double w0[DIM], double fac, Vec<Tout, SIMD_WIDTH> vecsOut[1])
{
  const auto fanInOut = sizeof(Float) / sizeof(Tin);
  const auto facF     = set1<Float, SIMD_WIDTH>(fac / 2.0);
  const auto oneF     = set1<Float, SIMD_WIDTH>(1.0f);
  Vec<Float, SIMD_WIDTH> wF[DIM], w0F[DIM], numF[DIM][fanInOut],
    denomF[DIM][fanInOut], resF[fanInOut];
  for (size_t d = 0; d < DIM; d++) {
    wF[d]  = set1<Float, SIMD_WIDTH>(-0.433 * w[d]);
    w0F[d] = set1<Float, SIMD_WIDTH>(w0[d]);
    extend(*vecsNum[d], numF[d]);
    extend(*vecsDenom[d], denomF[d]);
  }
  // j: index of extended input/output vector
  for (size_t j = 0; j < fanInOut; j++) {
    auto yF = setzero<Float, SIMD_WIDTH>();
    for (size_t d = 0; d < DIM; d++) {
      yF = add(yF, mul(wF[d], sub(div(numF[d][j], denomF[d][j]), w0F[d])));
    }
    auto y4F = mul(yF, yF);
    y4F      = mul(y4F, y4F);
    resF[j]  = mul(add(div(yF, sqrt(sqrt(add(oneF, y4F)))), oneF), facF);
  }
  vecsOut[0] = packs<Tout>(resF);
}
} // namespace ext
} // namespace internal

/**
 * @brief Special function used in MinWarping.
 *
 * @param[in] vecsNum Numerator vectors.
 * @param[in] vecsDenom Denominator vectors.
 * @param[in] w Weights.
 * @param[in] w0 Weights.
 * @param fac Factor.
 * @param[out] vecsOut Output vectors.
 */
template <size_t DIM, size_t NVEC, typename Tout, typename Tin,
          size_t SIMD_WIDTH>
static SIMD_INLINE void fdivMsigmoidmul(
  const Vec<Tin, SIMD_WIDTH> vecsNum[DIM][NVEC],
  const Vec<Tin, SIMD_WIDTH> vecsDenom[DIM][NVEC], const double w[DIM],
  const double w0[DIM], double fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Float), "");
  static_assert(sizeof(Tout) <= sizeof(Float), "");
  internal::ext::fdivMsigmoidmul<DIM, NVEC>(vecsNum, vecsDenom, w, w0, fac,
                                            vecsOut);
}

// ---------------------------------------------------------------------------
// multiply with float constant in float arithmetic
// ---------------------------------------------------------------------------

// 15. Mar 23 (Jonas Keller): fused the three cases

/**
 * @brief Multiplies Vec's element-wise with a floating point constant in
 * floating point arithmetic.
 *
 * @tparam Tfloat the floating point type to perform the calculation in.
 * Defaults to @ref BigEnoughFloat<Tout, Tin>
 * @param[in] vecsIn input Vec's
 * @param fac factor
 * @param[out] vecsOut output Vec's
 */
template <typename Tout, typename Tin,
          typename Tfloat = BigEnoughFloat<Tout, Tin>, size_t SIMD_WIDTH>
static SIMD_INLINE void fmul(
  const Vec<Tin, SIMD_WIDTH> vecsIn[numInVecs<Tout, Tin>()],
  dont_deduce<Tfloat> fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Tfloat), "");
  static_assert(sizeof(Tout) <= sizeof(Tfloat), "");
  constexpr auto nFloatVecs = internal::ext::numCalcVecs<Tout, Tin, Tfloat>();
  const auto facVec         = set1<Tfloat, SIMD_WIDTH>(fac);
  Vec<Tfloat, SIMD_WIDTH> inF[nFloatVecs], resF[nFloatVecs];
  internal::ext::extendInToFloat<Tin, Tout>(vecsIn, inF);
  for (size_t i = 0; i < nFloatVecs; i++) { resF[i] = mul(inF[i], facVec); }
  internal::ext::packsOutFromFloat<Tin, Tout>(resF, vecsOut);
}

// ---------------------------------------------------------------------------
// add then multiply with float constant in float arithmetic
// ---------------------------------------------------------------------------

// 15. Mar 23 (Jonas Keller): fused the three cases

/**
 * @brief Adds a floating point constant to the elements of Vec's, then
 * multiplies with a floating point constant in floating point arithmetic
 *
 * @tparam Tfloat the floating point type to perform the calculation in.
 * Defaults to @ref BigEnoughFloat<Tout, Tin>
 * @param[in] vecsIn input Vec's
 * @param off float constant to add
 * @param fac float constant to multiply with
 * @param[out] vecsOut output Vec's
 */
template <typename Tout, typename Tin,
          typename Tfloat = BigEnoughFloat<Tout, Tin>, size_t SIMD_WIDTH>
static SIMD_INLINE void faddmul(
  const Vec<Tin, SIMD_WIDTH> vecsIn[numInVecs<Tout, Tin>()],
  dont_deduce<Tfloat> off, dont_deduce<Tfloat> fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Tfloat), "");
  static_assert(sizeof(Tout) <= sizeof(Tfloat), "");
  constexpr auto nFloatVecs = internal::ext::numCalcVecs<Tout, Tin, Tfloat>();
  const auto offVec         = set1<Tfloat, SIMD_WIDTH>(off);
  const auto facVec         = set1<Tfloat, SIMD_WIDTH>(fac);
  Vec<Tfloat, SIMD_WIDTH> inF[nFloatVecs], resF[nFloatVecs];
  internal::ext::extendInToFloat<Tin, Tout>(vecsIn, inF);
  for (size_t i = 0; i < nFloatVecs; i++) {
    resF[i] = mul(add(inF[i], offVec), facVec);
  }
  internal::ext::packsOutFromFloat<Tin, Tout>(resF, vecsOut);
}

// ---------------------------------------------------------------------------
// multiply then add with float constant in float arithmetic
// ---------------------------------------------------------------------------

// better for conversion of zero-centered data to unsigned pixel format

// 15. Mar 23 (Jonas Keller): fused the three cases

/**
 * @brief Multiplies the elements of Vec's with a floating point constant, then
 * adds a floating point constant in floating point arithmetic
 *
 * @tparam Tfloat the floating point type to perform the calculation in.
 * Defaults to @ref BigEnoughFloat<Tout, Tin>
 * @param[in] vecsIn input Vec's
 * @param fac float constant to multiply with
 * @param off float constant to add
 * @param[out] vecsOut output Vec's
 */
template <typename Tout, typename Tin,
          typename Tfloat = BigEnoughFloat<Tout, Tin>, size_t SIMD_WIDTH>
static SIMD_INLINE void fmuladd(
  const Vec<Tin, SIMD_WIDTH> vecsIn[numInVecs<Tout, Tin>()],
  dont_deduce<Tfloat> fac, dont_deduce<Tfloat> off,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Tfloat), "");
  static_assert(sizeof(Tout) <= sizeof(Tfloat), "");
  constexpr auto nFloatVecs = internal::ext::numCalcVecs<Tout, Tin, Tfloat>();
  const auto facVec         = set1<Tfloat, SIMD_WIDTH>(fac);
  const auto offVec         = set1<Tfloat, SIMD_WIDTH>(off);
  Vec<Tfloat, SIMD_WIDTH> inF[nFloatVecs], resF[nFloatVecs];
  internal::ext::extendInToFloat<Tin, Tout>(vecsIn, inF);
  for (size_t i = 0; i < nFloatVecs; i++) {
    resF[i] = add(mul(inF[i], facVec), offVec);
  }
  internal::ext::packsOutFromFloat<Tin, Tout>(resF, vecsOut);
}

// ---------------------------------------------------------------------------
// multiply with float constant in float arithmetic
// ---------------------------------------------------------------------------

// 15. Mar 23 (Jonas Keller): fused the three cases

// fac * [v2 + w * (v1 - v2)] = fac * [w * v1 + (1 - w) * v2], w in [0,1]
// w: weight factor (in [0,1])
// fac: scale factor

/**
 * @brief Linearly interpolates Vec's element-wise with a constant weight and
 * then scales by a constant factor in floating point arithmetic.
 *
 * The result is calculated as:
 * <tt>out = fac * (w * v1 + (1 - w) * v2)</tt>
 * (implemented as <tt>out = fac * (v2 + w * (v1 - v2))</tt>).
 *
 * @tparam Tfloat the floating point type to perform the calculation in.
 * Defaults to @ref BigEnoughFloat<Tout, Tin>
 * @param[in] vecsIn1 first input Vec's
 * @param[in] vecsIn2 second input Vec's
 * @param w interpolation weight
 * @param fac scaling factor
 * @param[out] vecsOut output Vec's
 */
template <typename Tout, typename Tin,
          typename Tfloat = BigEnoughFloat<Tout, Tin>, size_t SIMD_WIDTH>
static SIMD_INLINE void fwaddmul(
  const Vec<Tin, SIMD_WIDTH> vecsIn1[numInVecs<Tout, Tin>()],
  const Vec<Tin, SIMD_WIDTH> vecsIn2[numInVecs<Tout, Tin>()],
  dont_deduce<Tfloat> w, dont_deduce<Tfloat> fac,
  Vec<Tout, SIMD_WIDTH> vecsOut[numOutVecs<Tout, Tin>()])
{
  static_assert(sizeof(Tin) <= sizeof(Tfloat), "");
  static_assert(sizeof(Tout) <= sizeof(Tfloat), "");
  constexpr auto nFloatVecs = internal::ext::numCalcVecs<Tout, Tin, Tfloat>();
  const auto wVec           = set1<Tfloat, SIMD_WIDTH>(w);
  const auto facVec         = set1<Tfloat, SIMD_WIDTH>(fac);
  Vec<Tfloat, SIMD_WIDTH> inF1[nFloatVecs], inF2[nFloatVecs], resF[nFloatVecs];
  internal::ext::extendInToFloat<Tin, Tout>(vecsIn1, inF1);
  internal::ext::extendInToFloat<Tin, Tout>(vecsIn2, inF2);
  for (size_t i = 0; i < nFloatVecs; i++) {
    resF[i] = mul(facVec, add(inF2[i], mul(wVec, sub(inF1[i], inF2[i]))));
  }
  internal::ext::packsOutFromFloat<Tin, Tout>(resF, vecsOut);
}

/** @} */

/**
 * @addtogroup group_horizontal
 * @{
 */

// ===========================================================================
// horizontal add/adds/sub/subs: generic form for multiple vector inputs
// ===========================================================================

// TODO: is there an easy way to implement multivec horizontal min/max?
// TODO: (Hackers delight: min/max via doz = hsubs?)

namespace internal {
namespace ext {
// primary template
// num: number of elements processed
// i0, i1: indices of lowest elements of block
template <typename T, size_t SIMD_WIDTH, size_t num, size_t i0, size_t i1>
struct Horizontal
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadd(
    const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
  {
    return hadd(Horizontal<T, SIMD_WIDTH, num / 2, i0, i0 + num / 4>::_hadd(v),
                Horizontal<T, SIMD_WIDTH, num / 2, i1, i1 + num / 4>::_hadd(v));
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadds(
    const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
  {
    return hadds(
      Horizontal<T, SIMD_WIDTH, num / 2, i0, i0 + num / 4>::_hadds(v),
      Horizontal<T, SIMD_WIDTH, num / 2, i1, i1 + num / 4>::_hadds(v));
  }
};

// partial specialization to end the recursion
template <typename T, size_t SIMD_WIDTH, size_t i0, size_t i1>
struct Horizontal<T, SIMD_WIDTH, 2, i0, i1>
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadd(
    const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
  {
    return hadd(v[i0], v[i1]);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadds(
    const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
  {
    return hadds(v[i0], v[i1]);
  }
};
} // namespace ext
} // namespace internal

// function template

/**
 * @brief Sums the elements of multiple Vec's independently and returns a
 * Vec with the results.
 *
 * @param[in] v array of Vec's to be summed. The number of Vec's must be equal
 * to the number of elements in a Vec
 * @return Vec with the results of the horizontal sums
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadd(
  const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
{
  return internal::ext::Horizontal<T, SIMD_WIDTH, Vec<T, SIMD_WIDTH>::elems, 0,
                                   (Vec<T, SIMD_WIDTH>::elems) / 2>::_hadd(v);
}

// function template

/**
 * @brief Sums the elements of multiple Vec's independently using
 * saturated arithmetic and returns a Vec with the results.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param[in] v array of Vec's to be summed
 * @return Vec with the results of the saturated horizontal sums
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadds(
  const Vec<T, SIMD_WIDTH> v[Vec<T, SIMD_WIDTH>::elems])
{
  return internal::ext::Horizontal<T, SIMD_WIDTH, Vec<T, SIMD_WIDTH>::elems, 0,
                                   (Vec<T, SIMD_WIDTH>::elems) / 2>::_hadds(v);
}

// ===========================================================================
// horizontal operations (generic form for single vector input)
// ===========================================================================

// these operations are not fully parallel!

// example: SIMD_WIDTH = 16, T = float
// extract<0>(Horizontal1<float,16,2>::_hadd(v));
//       u = Horizontal1<float,16,1>::_hadd(v);
//           hadd(v, v)
//       hadd(u, u)

namespace internal {
namespace ext {
template <typename T, size_t SIMD_WIDTH, size_t NUM>
struct Horizontal1
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadd(const Vec<T, SIMD_WIDTH> &v)
  {
    Vec<T, SIMD_WIDTH> u = Horizontal1<T, SIMD_WIDTH, NUM / 2>::_hadd(v);
    return hadd(u, u);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadds(const Vec<T, SIMD_WIDTH> &v)
  {
    Vec<T, SIMD_WIDTH> u = Horizontal1<T, SIMD_WIDTH, NUM / 2>::_hadds(v);
    return hadds(u, u);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hmin(const Vec<T, SIMD_WIDTH> &v)
  {
    return Horizontal1<T, SIMD_WIDTH, NUM / 2>::_hmin(min(srle<NUM>(v), v));
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hmax(const Vec<T, SIMD_WIDTH> &v)
  {
    return Horizontal1<T, SIMD_WIDTH, NUM / 2>::_hmax(max(srle<NUM>(v), v));
  }
};

template <typename T, size_t SIMD_WIDTH>
struct Horizontal1<T, SIMD_WIDTH, 1>
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadd(const Vec<T, SIMD_WIDTH> &v)
  {
    return hadd(v, v);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hadds(const Vec<T, SIMD_WIDTH> &v)
  {
    return hadds(v, v);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hmin(const Vec<T, SIMD_WIDTH> &v)
  {
    return min(srle<1>(v), v);
  }

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _hmax(const Vec<T, SIMD_WIDTH> &v)
  {
    return max(srle<1>(v), v);
  }
};
} // namespace ext
} // namespace internal

/**
 * @brief Adds all elements of a Vec.
 *
 * @param v input Vec
 * @return sum of all elements of the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T hadd(const Vec<T, SIMD_WIDTH> &v)
{
  return extract<0>(
    internal::ext::Horizontal1<T, SIMD_WIDTH,
                               SIMD_WIDTH / sizeof(T) / 2>::_hadd(v));
}

/**
 * @brief Adds all elements of a Vec using saturated arithmetic.
 *
 * @note Does not use saturated arithmetic with floating point types.
 *
 * @param v Vec to add all elements of
 * @return saturated sum of all elements of the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T hadds(const Vec<T, SIMD_WIDTH> &v)
{
  return extract<0>(
    internal::ext::Horizontal1<T, SIMD_WIDTH,
                               SIMD_WIDTH / sizeof(T) / 2>::_hadds(v));
}

/**
 * @brief Calculates the minimum of all elements of a Vec.
 *
 * @param v Vec to calculate the minimum of all elements of
 * @return minimum of all elements of the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T hmin(const Vec<T, SIMD_WIDTH> &v)
{
  return extract<0>(
    internal::ext::Horizontal1<T, SIMD_WIDTH,
                               SIMD_WIDTH / sizeof(T) / 2>::_hmin(v));
}

/**
 * @brief Calculates the maximum of all elements of a Vec.
 *
 * @param v Vec to calculate the maximum of all elements of
 * @return maximum of all elements of the Vec
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T hmax(const Vec<T, SIMD_WIDTH> &v)
{
  return extract<0>(
    internal::ext::Horizontal1<T, SIMD_WIDTH,
                               SIMD_WIDTH / sizeof(T) / 2>::_hmax(v));
}

/** @} */

// ===========================================================================
// iterative horizontal accumulation
// ===========================================================================

/**
 * @addtogroup group_iter_hor_acc
 * @{
 */

// 04. Aug 23 (Jonas Keller): added classes for iterative horizontal
// accumulation

/**
 * @brief Iterative horizontal accumulator. Calculates the horizontal
 * accumulation of multiple (Vec<T, SIMD_WIDTH>::elems) Vec's into a single Vec
 * in parallel with the Vec's to be accumulated pushed one by one.
 *
 * @tparam HOp horizontal accumulation operation, one of HAdd, HAdds, HMin, HMax
 */
template <class HOp, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
struct HAcc
{
private:
  size_t count    = 0;
  size_t stackTop = 0;
  Vec<T, SIMD_WIDTH> stack[internal::ext::floorlog2(Vec<T, SIMD_WIDTH>::elems)];

public:
  /**
   * @brief Checks if the horizontal accumulation is empty, i.e. if no Vec has
   * been pushed yet.
   *
   * @return whether the horizontal accumulation is empty
   */
  bool isEmpty() const { return count == 0; }

  /**
   * @brief Checks if the horizontal accumulation is done.
   *
   * @return whether the horizontal accumulation is done
   */
  bool isDone() const { return count == Vec<T, SIMD_WIDTH>::elems; }

  /**
   * @brief Pushes the next Vec to be horizontally accumulated. Does nothing if
   * the horizontal accumulation is already done.
   *
   * @param v Vec to be pushed
   */
  void push(const Vec<T, SIMD_WIDTH> &v)
  {
    if (isDone()) { return; }
    auto acc = v;
    for (size_t i = 0; count & (1 << i); i++) {
      stackTop--;
      acc = HOp::apply(stack[stackTop], acc);
    }
    stack[stackTop] = acc;
    stackTop++;
    count++;
  }

  /**
   * @brief Finishes the horizontal accumulation by pushing neutral values until
   * the horizontal accumulation is done.
   */
  void finish()
  {
    while (!isDone()) {
      push(set1<T, SIMD_WIDTH>(HOp::template neutralValue<T>()));
    }
  }

  /**
   * @brief Gets the result of the horizontal accumulation. Finishes the
   * horizontal accumulation if it is not done yet.
   *
   * @return result of the horizontal accumulation
   */
  Vec<T, SIMD_WIDTH> get()
  {
    finish();
    return stack[0];
  }

  /**
   * @brief Resets the horizontal accumulation.
   */
  void reset()
  {
    count    = 0;
    stackTop = 0;
  }
};

/**
 * @brief Iterative horizontal accumulator with store of the result. Calculates
 * the horizontal accumulation of multiple Vec's in parallel with the Vec's to
 * be accumulated pushed one by one. Stores the result of the horizontal
 * accumulation every Vec<T, SIMD_WIDTH>::elems Vec's into memory.
 *
 * @tparam HOp horizontal accumulation operation, one of HAdd, HAdds, HMin, HMax
 */
template <class HOp, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
class HAccStore
{
private:
  T *const ptr;
  size_t index = 0;
  HAcc<HOp, T, SIMD_WIDTH> hacc;

public:
  /**
   * @brief Constructs a new HAccStore object.
   *
   * @param p pointer to the memory to store the result of the horizontal
   * accumulation
   */
  HAccStore(T *const p) : ptr(p) {}

  ~HAccStore() { finish(); }

  /**
   * @brief Pushes the next Vec to be horizontally accumulated. Stores the
   * result of the horizontal accumulation every Vec<T, SIMD_WIDTH>::elems Vec's
   * into memory.
   *
   * @param v Vec to be pushed
   */
  void push(Vec<T, SIMD_WIDTH> v)
  {
    hacc.push(v);
    if (hacc.isDone()) {
      // TODO: aligned/unaligned?
      storeu(&ptr[index], hacc.get());
      index += Vec<T, SIMD_WIDTH>::elems;
      hacc.reset();
    }
  }

  /**
   * @brief Finishes the horizontal accumulation and stores the result of the
   * horizontal accumulation into memory.
   */
  void finish()
  {
    // if hacc is not empty, we have to finish and store;
    // if hacc was reset after last push, nothing happens
    if (hacc.isEmpty()) return;

    hacc.finish();
    storeu(&ptr[index], hacc.get());
    index = 0;
    hacc.reset();
  }
};

/**
 * @brief Horizontal addition class for iterative horizontal accumulation.
 * @sa HAcc, HAccStore
 */
struct HAdd
{
  // exclude from doxygen (until endcond)
  /// @cond
  template <typename T, size_t SIMD_WIDTH>
  static SIMD_INLINE Vec<T, SIMD_WIDTH> apply(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
  {
    return hadd(a, b);
  }

  template <typename T>
  static SIMD_INLINE T neutralValue()
  {
    return T(0);
  }
  /// @endcond
};

/**
 * @brief Horizontal saturated addition class for iterative horizontal
 * accumulation.
 * @sa HAcc, HAccStore
 */
struct HAdds
{
  // exclude from doxygen (until endcond)
  /// @cond
  template <typename T, size_t SIMD_WIDTH>
  static SIMD_INLINE Vec<T, SIMD_WIDTH> apply(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
  {
    return hadds(a, b);
  }

  template <typename T>
  static SIMD_INLINE T neutralValue()
  {
    return T(0);
  }
  /// @endcond
};

/**
 * @brief Horizontal minimum class for iterative horizontal accumulation.
 * @sa HAcc, HAccStore
 */
struct HMin
{
  // exclude from doxygen (until endcond)
  /// @cond
  template <typename T, size_t SIMD_WIDTH>
  static SIMD_INLINE Vec<T, SIMD_WIDTH> apply(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
  {
    return min(a, b);
  }

  template <typename T>
  static SIMD_INLINE T neutralValue()
  {
    return std::numeric_limits<T>::max();
  }
  /// @endcond
};

/**
 * @brief Horizontal maximum class for iterative horizontal accumulation.
 * @sa HAcc, HAccStore
 */
struct HMax
{
  // exclude from doxygen (until endcond)
  /// @cond
  template <typename T, size_t SIMD_WIDTH>
  static SIMD_INLINE Vec<T, SIMD_WIDTH> apply(const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
  {
    return max(a, b);
  }

  template <typename T>
  static SIMD_INLINE T neutralValue()
  {
    return std::numeric_limits<T>::lowest();
  }
  /// @endcond
};

/** @} */

/**
 * @addtogroup group_arithmetic
 * @{
 */

// ===========================================================================
// avgru: synonym f. average with rounding up
// ===========================================================================

// this is just a synonym for avg which is compatible with the auxiliary avgrd

/**
 * @copybrief avg(const Vec<T, SIMD_WIDTH> &,
 * const Vec<T, SIMD_WIDTH> &)
 *
 * Equivalent to avg(const Vec<T, SIMD_WIDTH> &, const
 * Vec<T, SIMD_WIDTH> &).
 *
 * @copydetails avg(const Vec<T, SIMD_WIDTH> &,
 * const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avgru(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return avg(a, b);
}

// ===========================================================================
// avgrd: average with rounding down
// ===========================================================================

// 30. Jul 17 (rm): removed unnecessary tag dispatching for avgrd()

namespace internal {
namespace ext {
// int types
template <typename T, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avgrd(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  Vec<T, SIMD_WIDTH> one = set1<T, SIMD_WIDTH>(1), as, bs, lsb;
  lsb                    = bit_and(bit_and(a, b), one);
  as                     = div2rd(a);
  bs                     = div2rd(b);
  return add(lsb, add(as, bs));
}

// NOTE: no rounding for floating-point types
template <typename T, size_t SIMD_WIDTH,
          SIMD_ENABLE_IF(std::is_floating_point<T>::value), typename = void>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avgrd(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return mul(add(a, b), set1<T, SIMD_WIDTH>(0.5));
}
} // namespace ext
} // namespace internal

/**
 * @brief Computes the average of the elements of two Vecs, rounding down.
 *
 * @param a first Vec
 * @param b second Vec
 * @return Vec containing the rounded down average of the elements of the two
 * input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> avgrd(const Vec<T, SIMD_WIDTH> &a,
                                            const Vec<T, SIMD_WIDTH> &b)
{
  return internal::ext::avgrd(a, b);
}

// ===========================================================================
// div2r0: integer div. by 2 with round to 0 (for integers)
// ===========================================================================

namespace internal {
namespace ext {
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Byte, SIMD_WIDTH> div2r0(const Vec<Byte, SIMD_WIDTH> &a)
{
  return srli<1>(a);
}

// 16. Oct 22 (Jonas Keller): added missing version for SignedByte
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<SignedByte, SIMD_WIDTH> div2r0(
  const Vec<SignedByte, SIMD_WIDTH> &a)
{
  // add 1 if number is negative
  return srai<1>(add(a, srli<7>(a)));
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Word, SIMD_WIDTH> div2r0(const Vec<Word, SIMD_WIDTH> &a)
{
  return srli<1>(a);
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Short, SIMD_WIDTH> div2r0(
  const Vec<Short, SIMD_WIDTH> &a)
{
  // add 1 if number is negative
  return srai<1>(add(a, srli<15>(a)));
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Int, SIMD_WIDTH> div2r0(const Vec<Int, SIMD_WIDTH> &a)
{
  // add 1 if number is negative
  return srai<1>(add(a, srli<31>(a)));
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Long, SIMD_WIDTH> div2r0(const Vec<Long, SIMD_WIDTH> &a)
{
  // add 1 if number is negative
  return srai<1>(add(a, srli<63>(a)));
}

// NOTE: no rounding for float
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH> div2r0(
  const Vec<Float, SIMD_WIDTH> &a)
{
  return mul(set1<Float, SIMD_WIDTH>(0.5f), a);
}

// NOTE: no rounding for double
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Double, SIMD_WIDTH> div2r0(
  const Vec<Double, SIMD_WIDTH> &a)
{
  return mul(set1<Double, SIMD_WIDTH>(0.5), a);
}
} // namespace ext
} // namespace internal

/**
 * @brief Divides all elements of a Vec by 2 and rounds the result to 0.
 *
 * Only rounds the result to 0 for integer types. For floating point types
 * the result is not rounded.
 *
 * @param a input Vec
 * @return result of the division
 *
 * @sa div2rd()
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> div2r0(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::ext::div2r0(a);
}

// ===========================================================================
// div2rd: integer division by two with rounding down (for integers)
// ===========================================================================

namespace internal {
namespace ext {
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Byte, SIMD_WIDTH> div2rd(const Vec<Byte, SIMD_WIDTH> &a)
{
  return srli<1>(a);
}

// 16. Oct 22 (Jonas Keller): added missing version for SignedByte
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<SignedByte, SIMD_WIDTH> div2rd(
  const Vec<SignedByte, SIMD_WIDTH> &a)
{
  return srai<1>(a);
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Word, SIMD_WIDTH> div2rd(const Vec<Word, SIMD_WIDTH> &a)
{
  return srli<1>(a);
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Short, SIMD_WIDTH> div2rd(
  const Vec<Short, SIMD_WIDTH> &a)
{
  return srai<1>(a);
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Int, SIMD_WIDTH> div2rd(const Vec<Int, SIMD_WIDTH> &a)
{
  return srai<1>(a);
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Long, SIMD_WIDTH> div2rd(const Vec<Long, SIMD_WIDTH> &a)
{
  return srai<1>(a);
}

// NOTE: no rounding for float
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH> div2rd(
  const Vec<Float, SIMD_WIDTH> &a)
{
  return mul(set1<Float, SIMD_WIDTH>(0.5f), a);
}

// NOTE: no rounding for double
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Double, SIMD_WIDTH> div2rd(
  const Vec<Double, SIMD_WIDTH> &a)
{
  return mul(set1<Double, SIMD_WIDTH>(0.5), a);
}
} // namespace ext
} // namespace internal

/**
 * @brief Divides all elements of a Vec by 2 and rounds down the result.
 *
 * Only rounds down the result for integer types. For floating point types
 * the result is not rounded.
 *
 * @param a input Vec
 * @return result of the division
 *
 * @sa div2r0()
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> div2rd(const Vec<T, SIMD_WIDTH> &a)
{
  return internal::ext::div2rd(a);
}

// ===========================================================================
// sign function (Float and Double only)
// ===========================================================================

// contributed by Benedikt Volkmer
// negate a, where b is negative
// note: contrary to IEEE 754, this function considers -0.0f to be negative

/**
 * @brief Negates the elements of a Vec of floating-point numbers where the
 * corresponding element of a second Vec of floating-point numbers is negative.
 * @note Contrary to IEEE 754, this function considers -0.0 to be negative.
 *
 * @param a Vec of floating-point numbers to be negated
 * @param b Vec of floating-point numbers that determines which elements of a
 * are negated
 * @return resulting Vec of floating-point numbers
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> sign(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  static_assert(std::is_floating_point<T>::value,
                "sign() is only available for floating-point types");
  // -0.0F aka. 0x80000000 aka. 1000...b
  return bit_xor(a, bit_and(set1<T, SIMD_WIDTH>(T(-0.0)), b));
}

// ===========================================================================
// absDiff function
// ===========================================================================

// contributed by Benedikt Volkmer
// 23. Mar 22 (rm): removed SFINAE enable_if construct
// (not compatible with C++98)
// Computes elementwise absolute difference of vectors

namespace internal {
namespace ext {

// Use these overloads of the function template if Type is unsigned

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Byte, SIMD_WIDTH> absDiff(
  const Vec<Byte, SIMD_WIDTH> &v1, const Vec<Byte, SIMD_WIDTH> &v2)
{
  // Trick working around non-existing abs() for unsigned Type
  return bit_or(subs(v1, v2), subs(v2, v1));
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Word, SIMD_WIDTH> absDiff(
  const Vec<Word, SIMD_WIDTH> &v1, const Vec<Word, SIMD_WIDTH> &v2)
{
  // Trick working around non-existing abs() for unsigned Type
  return bit_or(subs(v1, v2), subs(v2, v1));
}

// Use these overloads of the function template if Type is signed

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> absDiff(const Vec<T, SIMD_WIDTH> &v1,
                                              const Vec<T, SIMD_WIDTH> &v2)
{
  static_assert(std::is_signed<T>::value, "");
  return abs(sub(v1, v2));
}
} // namespace ext
} // namespace internal

/**
 * @brief Computes the absolute difference of the elements of two Vec's.
 *
 * @param v1 first Vec
 * @param v2 second Vec
 * @return Vec containing the absolute difference of the elements of the two
 * input Vec's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> absDiff(const Vec<T, SIMD_WIDTH> &v1,
                                              const Vec<T, SIMD_WIDTH> &v2)
{
  return internal::ext::absDiff(v1, v2);
}

/** @} */

namespace internal {
namespace ext {

// ===========================================================================
// transpose
// ===========================================================================

// -------------------- different unpack functions ---------------------------

template <size_t PART, size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
struct Unpack
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _unpack(const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
  {
    return unpack<PART, NUM_ELEMS>(a, b);
  }
};

template <size_t PART, size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
struct Unpack16
{
  static SIMD_INLINE Vec<T, SIMD_WIDTH> _unpack(const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
  {
    return unpack16<PART, NUM_ELEMS>(a, b);
  }
};

// ------------------------ transpose a single row ---------------------------

// primary template
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH,
          // INDEX: index of first input element to unpack
          // NLOHI: low/high unpack selector index
          // ELEMS: number of elements to unpack in this stage
          size_t INDEX, size_t NLOHI, size_t ELEMS>
struct Transpose1
{
  static constexpr auto PART = (NLOHI & 0x01);
  static constexpr auto NEXT = (NLOHI >> 1);
  static constexpr auto LIDX = INDEX;
  static constexpr auto RIDX = INDEX + ELEMS;
  static constexpr auto HALF = ELEMS / 2;

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _transpose1(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("_transpose1("
    //       "INDEX=%d NLOHI=%d ELEMS=%d PART=%d LIDX=%d RIDX=%d HALF=%d)\n",
    //       INDEX, NLOHI, ELEMS, PART, LIDX, RIDX, HALF);
    // TODO: T,SIMD_WIDTH necessary or can it be deduced from arguments?
    return Unpack<PART, ELEMS, T, SIMD_WIDTH>::_unpack(
      Transpose1<Unpack, T, SIMD_WIDTH, LIDX, NEXT, HALF>::_transpose1(inRows),
      Transpose1<Unpack, T, SIMD_WIDTH, RIDX, NEXT, HALF>::_transpose1(inRows));
  }
};

// partial specialization to end the iteration (ELEMS=1)
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t INDEX, size_t NLOHI>
struct Transpose1<Unpack, T, SIMD_WIDTH, INDEX, NLOHI, 1>
{
  static constexpr auto PART = (NLOHI & 0x01);

  static SIMD_INLINE Vec<T, SIMD_WIDTH> _transpose1(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("_transpose1(INDEX=%d NLOHI=%d *ELEMS=%d PART=%d)\n",
    // 	   INDEX, NLOHI, 1, PART);
    // TODO: T,SIMD_WIDTH necessary or can it be deduced from arguments?
    return Unpack<PART, 1, T, SIMD_WIDTH>::_unpack(inRows[INDEX],
                                                   inRows[INDEX + 1]);
  }
};

// ----------------------- transpose multiple rows --------------------------

// primary template
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH,
          // NUMROWS: total number of rows
          // NUM_TRANSPOSE_ROWS: number of rows to transpose
          // ROW: index of row to transpose
          size_t NUMROWS, size_t NUM_TRANSPOSE_ROWS, size_t ROW>
struct Transpose
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[NUM_TRANSPOSE_ROWS])
  {
    // printf("\n_transpose(NUMROWS=%d,ROW=%d)\n", NUMROWS, ROW);
    // transpose single row with index ROW
    outRows[ROW] =
      // INDEX=0, NLOWHI=ROW, ELEMS=NUMROWS/2
      Transpose1<Unpack, T, SIMD_WIDTH, 0, ROW, NUMROWS / 2>::_transpose1(
        inRows);
    // transpose next row
    // NUMROWS=NUMROWS, ROW=ROW+1
    Transpose<Unpack, T, SIMD_WIDTH, NUMROWS, NUM_TRANSPOSE_ROWS,
              ROW + 1>::_transpose(inRows, outRows);
  }
};

// partial specialization to end the iteration
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t NUM_TRANSPOSE_ROWS>
struct Transpose<Unpack, T, SIMD_WIDTH, NUMROWS, NUM_TRANSPOSE_ROWS,
                 NUM_TRANSPOSE_ROWS>
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[NUM_TRANSPOSE_ROWS])
  {}
};

// function template: partial transpose
template <size_t NUM_TRANSPOSE_ROWS, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_a_partial(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[NUM_TRANSPOSE_ROWS])
{
  Transpose<Unpack, T, SIMD_WIDTH,
            // NUMROWS, NUM_TRANSPOSE_ROWS, ROW
            SIMD_WIDTH / sizeof(T), NUM_TRANSPOSE_ROWS, 0>::_transpose(inRows,
                                                                       outRows);
}

// function template: full transpose
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_a(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  transpose_a_partial<SIMD_WIDTH / sizeof(T)>(inRows, outRows);
}

// ===========================================================================
// copy matrix
// ===========================================================================

// primary template
template <typename T, size_t SIMD_WIDTH, size_t ROW, size_t ROW_STOP>
struct CopyMatrix
{
  static_assert(ROW < ROW_STOP, "ROW must be less than ROW_STOP");

  static SIMD_INLINE void _copy(Vec<T, SIMD_WIDTH> v[ROW_STOP],
                                Vec<T, SIMD_WIDTH> v2[ROW_STOP])
  {
    v2[ROW] = v[ROW];
    CopyMatrix<T, SIMD_WIDTH, ROW + 1, ROW_STOP>::_copy(v, v2);
  }
};

// partial specialization to end the iteration
template <typename T, size_t SIMD_WIDTH, size_t ROW_STOP>
struct CopyMatrix<T, SIMD_WIDTH, ROW_STOP, ROW_STOP>
{
  static SIMD_INLINE void _copy(Vec<T, SIMD_WIDTH>[ROW_STOP],
                                Vec<T, SIMD_WIDTH>[ROW_STOP])
  {}
};

// ===========================================================================
// Transpose Post-Process
// ===========================================================================

// ------------------------ transpose post-process 16 ------------------------
//            Used to post-process transposed matrix using unpack16

// primary template
template <typename T, size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW,
          size_t ROW_STOP, size_t TRANSPOSE_WIDTH, size_t SRC_OFF,
          size_t DST_OFF>
struct TransposePostprocess16
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("%s", "\nTransposePostprocess16\n");
    // printf("TRANSPOSE_WIDTH=%d\n", TRANSPOSE_WIDTH);
    TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP,
                           TRANSPOSE_WIDTH / 2, SRC_OFF,
                           2 * DST_OFF>::_transpose(inRows, outRows);
    TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP,
                           TRANSPOSE_WIDTH / 2,
                           SRC_OFF + SIMD_WIDTH / TRANSPOSE_WIDTH,
                           2 * DST_OFF + ROW_STOP>::_transpose(inRows, outRows);
  }
};

// partial specialization
template <typename T, size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW,
          size_t ROW_STOP, size_t SRC_OFF, size_t DST_OFF>
struct TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP, 16,
                              SRC_OFF, DST_OFF>
{
  static constexpr auto STEP    = SIMD_WIDTH / 16;
  static constexpr auto SRC_ROW = SRC_OFF + ROW * STEP;
  static constexpr auto DST_ROW = DST_OFF + ROW;

  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("%s", "\nTransposePostprocess16\n");
    // printf("TRANSPOSE_WIDTH=%d\n", 16);
    // printf("SRC_ROW=%d DST_ROW=%d\n", SRC_ROW, DST_ROW);
    outRows[DST_ROW] = inRows[SRC_ROW];
    TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, ROW + 1, ROW_STOP, 16,
                           SRC_OFF, DST_OFF>::_transpose(inRows, outRows);
  }
};

// partial specialization to end the iteration
template <typename T, size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW_STOP,
          size_t SRC_OFF, size_t DST_OFF>
struct TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, ROW_STOP, ROW_STOP, 16,
                              SRC_OFF, DST_OFF>
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems])
  {}
};

// ------------------------ transpose post-process hub -----------------------

// primary template
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH>
struct TransposePostprocess
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems])
  {}
};

// partial specialization to post-process Transpose<Unpack16>
template <typename T, size_t SIMD_WIDTH>
struct TransposePostprocess<Unpack16, T, SIMD_WIDTH>
{
  static constexpr auto NUMROWS  = SIMD_WIDTH / sizeof(T);
  static constexpr auto ROW_STOP = 16 / sizeof(T);

  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("%s", "\nTransposePostprocess\n");
    // printf("SIMD_WIDTH=%d TYPE=%s\n", SIMD_WIDTH, TypeInfo<T>::name());
    TransposePostprocess16<T, SIMD_WIDTH, NUMROWS, 0, ROW_STOP, SIMD_WIDTH, 0,
                           0>::_transpose(inRows, outRows);
  }
};

// ===========================================================================
// transpose_b: Transpose<Unpack16> + post-process
// ===========================================================================

// contributed by Adam Marschall

// function template: full transpose
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_b(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  Vec<T, SIMD_WIDTH> tempRows[Vec<T, SIMD_WIDTH>::elements];
  Transpose<Unpack16, T, SIMD_WIDTH, SIMD_WIDTH / sizeof(T),
            SIMD_WIDTH / sizeof(T), 0>::_transpose(inRows, tempRows);
  TransposePostprocess<Unpack16, T, SIMD_WIDTH>::_transpose(tempRows, outRows);
}

// ===========================================================================
// transpose_c: Transpose<Unpack16> - needs store16
// ===========================================================================

// contributed by Adam Marschall

// function template: full transpose (includes store16)
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_c(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  Vec<T, SIMD_WIDTH> tempOutRows[Vec<T, SIMD_WIDTH>::elems];

  Transpose<Unpack16, T, SIMD_WIDTH, SIMD_WIDTH / sizeof(T),
            SIMD_WIDTH / sizeof(T), 0>::_transpose(inRows, tempOutRows);

  // post-process with store16 ...
  const auto N = SIMD_WIDTH / sizeof(T);
  T outArray[N * N];
  storeu16(outArray, tempOutRows);
  // ... and load to outRows
  loadu(outArray, outRows, N);
}

// ===========================================================================
// Transpose16: Template Class to transpose multiple rows with integrated
// Unpack16 post-process
// Uses Transpose1 to transpose single rows.
// ===========================================================================

// ----------------------- transpose multiple rows --------------------------

// primary template
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH,
          // NUMROWS: total number of rows
          // ROW: index of row to transpose
          size_t NUMROWS, size_t ROW, size_t ROW_STOP, size_t TRANSPOSE_WIDTH,
          size_t SRC_OFF, size_t DST_OFF>
struct Transpose16
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
  {
    Transpose16<Unpack, T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP,
                TRANSPOSE_WIDTH / 2, SRC_OFF, 2 * DST_OFF>::_transpose(inRows,
                                                                       outRows);
    Transpose16<Unpack, T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP,
                TRANSPOSE_WIDTH / 2, SRC_OFF + SIMD_WIDTH / TRANSPOSE_WIDTH,
                2 * DST_OFF + ROW_STOP>::_transpose(inRows, outRows);
  }
};

// partial specialization to end first iteration
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW, size_t ROW_STOP,
          size_t SRC_OFF, size_t DST_OFF>
struct Transpose16<Unpack, T, SIMD_WIDTH, NUMROWS, ROW, ROW_STOP, 16, SRC_OFF,
                   DST_OFF>
{
  static constexpr auto STEP    = SIMD_WIDTH / 16;
  static constexpr auto SRC_ROW = SRC_OFF + ROW * STEP;
  static constexpr auto DST_ROW = DST_OFF + ROW;

  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("\n_transpose_b(SRC=%d,DST=%d)", SRC_ROW, DST_ROW);
    // printf("\n   ROW=%d,OFF=%d,STEP=%d", ROW, TRANSPOSE_OFFSET, STEP);
    // transpose single row with index SRC_ROW
    outRows[DST_ROW] = Transpose1<Unpack, T, SIMD_WIDTH,
                                  // INDEX=0, NLOWHI=SRC_ROW, ELEMS=NUMROWS/2
                                  0, SRC_ROW, NUMROWS / 2>::_transpose1(inRows);
    // transpose next row
    // NUMROWS=NUMROWS, ROW=ROW+1
    Transpose16<Unpack, T, SIMD_WIDTH, NUMROWS, ROW + 1, ROW_STOP, 16, SRC_OFF,
                DST_OFF>::_transpose(inRows, outRows);
  }
};

// partial specialization to end the iteration
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t NUMROWS, size_t ROW_STOP, size_t SRC_OFF,
          size_t DST_OFF>
struct Transpose16<Unpack, T, SIMD_WIDTH, NUMROWS, ROW_STOP, ROW_STOP, 16,
                   SRC_OFF, DST_OFF>
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[Vec<T, SIMD_WIDTH>::elems])
  {}
};

// contributed by Adam Marschall

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_d(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  Transpose16<Unpack16, T, SIMD_WIDTH,
              // NUMROWS, ROW, ROW_STOP
              SIMD_WIDTH / sizeof(T), 0, 16 / sizeof(T),
              // TRANSPOSE_WIDTH, SRC_OFF, DST_OFF
              SIMD_WIDTH, 0, 0>::_transpose(inRows, outRows);
}

// ===========================================================================
// swizzle2_a (deinterleave)
// ===========================================================================

// contributed by Adam Marschall

// generalized from Marat Dukhan's solution referred to at
// https://stackoverflow.com/a/15377386/3852630
// takes 2*N input elements

// TODO: swizzling chunks of multiple elements (useful?)
// TODO: could be possible by starting loop at sizeof(T) and
// TODO: using zip<NUM_ELEMS>

// FINALBLKSIZE template argument is required since function is also
// used for transpose_e
template <size_t N, size_t FINALBLKSIZE, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2_a(Vec<T, SIMD_WIDTH> v[2 * N])
{
  Vec<T, SIMD_WIDTH> v2[2 * N];
  for (size_t blkSize = 1; blkSize <= FINALBLKSIZE; blkSize *= 2) {
    // zip
    for (size_t src = 0, dst = 0; src < N; src++, dst += 2)
      zip<1>(v[src], v[src + N], v2[dst], v2[dst + 1]);
    // copy result back to v
    // TODO: swizzle2_a: check code produced by compiler for copying
    for (size_t i = 0; i < 2 * N; i++) v[i] = v2[i];
  }
}

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2_a(Vec<T, SIMD_WIDTH> v[2 * N])
{
  swizzle2_a<N, Vec<T, SIMD_WIDTH>::elements>(v);
}

// ===========================================================================
// transpose_e
// ===========================================================================

// contributed by Adam Marschall

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_e(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  constexpr auto num = Vec<T, SIMD_WIDTH>::elements;
  for (size_t i = 0; i < num; i++) outRows[i] = inRows[i];
  swizzle2_a<num / 2, num / 2>(outRows);
}

// ===========================================================================
// swizzle2_b (deinterleave)
// ===========================================================================

// contributed by Adam Marschall

// generalized from Marat Dukhan's solution referred to at
// https://stackoverflow.com/a/15377386/3852630
// takes 2*N input elements

// FINALBLKSIZE template argument is required since function is also
// used for transpose_f
template <size_t N, size_t FINALBLKSIZE, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2_b(Vec<T, SIMD_WIDTH> v[2 * N])
{
  Vec<T, SIMD_WIDTH> v2[2 * N];
  const auto origReps  = floorlog2(FINALBLKSIZE) + 1;
  const auto finalReps = origReps / 2;
  // printf("origReps=%d finalReps=%d\n", origReps, finalReps);

  for (size_t rep = 0; rep < finalReps; rep++) {
    // zip there ...
    for (size_t src = 0, dst = 0; src < N; src++, dst += 2)
      zip<1>(v[src], v[src + N], v2[dst], v2[dst + 1]);

    // ... and zip back again
    for (size_t src = 0, dst = 0; src < N; src++, dst += 2)
      zip<1>(v2[src], v2[src + N], v[dst], v[dst + 1]);
  }

  // skip post-amble in case of even origReps
  if (origReps % 2 == 0) return;

  // zip there ...
  for (size_t src = 0, dst = 0; src < N; src++, dst += 2)
    zip<1>(v[src], v[src + N], v2[dst], v2[dst + 1]);

  // ...and copy back again
  for (size_t i = 0; i < 2 * N; i++) v[i] = v2[i];
}

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2_b(Vec<T, SIMD_WIDTH> v[2 * N])
{
  swizzle2_b<N, Vec<T, SIMD_WIDTH>::elements>(v);
}

// ===========================================================================
// transpose_f
// ===========================================================================

// contributed by Adam Marschall

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_f(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  const auto elems = Vec<T, SIMD_WIDTH>::elements;
  for (size_t i = 0; i < elems; i++) outRows[i] = inRows[i];
  swizzle2_b<elems / 2, elems / 2>(outRows);
}

// ===========================================================================
// Swizzle2 meta template
// ===========================================================================

// contributed by Adam Marschall

// -------------------- different zip functions ------------------------------

template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
struct Zip
{
  static SIMD_INLINE void _zip(Vec<T, SIMD_WIDTH> a, Vec<T, SIMD_WIDTH> b,
                               Vec<T, SIMD_WIDTH> &l, Vec<T, SIMD_WIDTH> &h)
  {
    zip<NUM_ELEMS, T>(a, b, l, h);
  }
};

template <size_t NUM_ELEMS, typename T, size_t SIMD_WIDTH>
struct Zip16
{
  static SIMD_INLINE void _zip(Vec<T, SIMD_WIDTH> a, Vec<T, SIMD_WIDTH> b,
                               Vec<T, SIMD_WIDTH> &l, Vec<T, SIMD_WIDTH> &h)
  {
    zip16<NUM_ELEMS, T>(a, b, l, h);
  }
};

// ------------------------ swizzle matrix once ------------------------------

// primary template
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t SRC, size_t DST>
struct Swizzle2Once
{
  static constexpr auto SRC2 = SRC + N;
  static constexpr auto DST2 = DST + 1;

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    // printf("%s\n", "SwizzleOnce");
    // printf("  SRC=%d, SRC2=%d, DST=%d, DS2T=%d\n", SRC, SRC2, DST, DST2);
    Zip<1, T, SIMD_WIDTH>::_zip(v[SRC], v[SRC2], v2[DST], v2[DST2]);
    Swizzle2Once<Zip, T, SIMD_WIDTH, N, SRC + 1, DST + 2>::_swizzle(v, v2);
  }
};

// partial specialization to end the iteration
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t DST>
struct Swizzle2Once<Zip, T, SIMD_WIDTH, N, N, DST>
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH>[2 * N] /*v*/,
                                   Vec<T, SIMD_WIDTH>[2 * N] /*v2*/)
  {
    // for (size_t i = 0; i < 2 * N; i++) {
    //   print("%4d", v2[i]);
    //   puts("");
    // }
  }
};

// ------------------------ swizzle matrix multiple times --------------------

// primary template
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t REP, size_t FINAL_REPS,
          size_t ODD>
struct Swizzle2Multiple
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    // printf("%s\n", "SwizzleMultiple");
    // printf("  REP=%d, FINAL_REPS=%d\n", REP, FINAL_REPS);
    Swizzle2Once<Zip, T, SIMD_WIDTH, N, 0, 0>::_swizzle(v, v2);
    Swizzle2Once<Zip, T, SIMD_WIDTH, N, 0, 0>::_swizzle(v2, v);
    Swizzle2Multiple<Zip, T, SIMD_WIDTH, N, REP + 1, FINAL_REPS, ODD>::_swizzle(
      v, v2);
  }
};

// partial specialization to end the iteration without swizzle post-amble
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t FINAL_REPS, size_t ODD>
struct Swizzle2Multiple<Zip, T, SIMD_WIDTH, N, FINAL_REPS, FINAL_REPS, ODD>
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH>[2 * N],
                                   Vec<T, SIMD_WIDTH>[2 * N])
  {}
};

// partial specialization to end the iteration with swizzle post-amble
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t FINAL_REPS>
struct Swizzle2Multiple<Zip, T, SIMD_WIDTH, N, FINAL_REPS, FINAL_REPS, 1>
{
  static constexpr auto ROW_STOP = 2 * N;

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    // printf("%s\n", "SwizzlePostamble");
    Swizzle2Once<Zip, T, SIMD_WIDTH, N, 0, 0>::_swizzle(v, v2);
    CopyMatrix<T, SIMD_WIDTH, 0, ROW_STOP>::_copy(v2, v);
  }
};

// ------------------------ swizzle main meta template -----------------------

// generalized from Marat Dukhan's solution referred to at
// https://stackoverflow.com/a/15377386/3852630

// primary template
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N, size_t FINALBLKSIZE>
struct Swizzle2
{
  static constexpr auto ORIG_REPS  = floorlog2(FINALBLKSIZE) + 1;
  static constexpr auto FINAL_REPS = ORIG_REPS / 2;
  static constexpr auto ODD        = (ORIG_REPS & 0x01);

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N])
  {
    Vec<T, SIMD_WIDTH> v2[2 * N];
    // printf("%s\n", "Swizzle");
    // printf("  N=%d, FINALBLKSIZE=%d\n", N, FINALBLKSIZE);
    // printf("  ORIG_REPS=%d, FINAL_REPS=%d, ODD=%d\n", ORIG_REPS, FINAL_REPS,
    // ODD);
    Swizzle2Multiple<Zip, T, SIMD_WIDTH, N, 0, FINAL_REPS, ODD>::_swizzle(v,
                                                                          v2);
  }
};

// ===========================================================================
// swizzle2_c wrapper function
// ===========================================================================

// 15. Oct 22 (Jonas Keller): added swizzle2_c wrapper function

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2_c(Vec<T, SIMD_WIDTH> v[2 * N])
{
  Swizzle2<Zip, T, SIMD_WIDTH, N, Vec<T, SIMD_WIDTH>::elements>::_swizzle(v);
}

// ===========================================================================
// Unswizzle
// ===========================================================================

// 15. Oct 22 (Jonas Keller): added Unswizzle classes

// Note: Unlike the Swizzle2 classes, the Unswizzle classes do not have a
// template-template parameter for the Zip class.
// In the Swizzle2 classes, the Zip template parameter is used to choose
// between the zip and zip16 functions, which is needed by the Transpose_g
// classes. The Unswizzle classes are not used by Transpose_g, so the Zip
// template parameter is not needed.

template <typename T, size_t SIMD_WIDTH, size_t N, size_t SRC, size_t DST>
struct UnswizzleOnce
{
  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                     Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    unzip<1, T>(v[SRC], v[SRC + 1], v2[DST], v2[DST + N]);
    UnswizzleOnce<T, SIMD_WIDTH, N, SRC + 2, DST + 1>::_unswizzle(v, v2);
  }
};

// partial specialization to end the iteration
template <typename T, size_t SIMD_WIDTH, size_t N, size_t SRC>
struct UnswizzleOnce<T, SIMD_WIDTH, N, SRC, N>
{
  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH>[2 * N],
                                     Vec<T, SIMD_WIDTH>[2 * N])
  {}
};

template <typename T, size_t SIMD_WIDTH, size_t N, size_t REP,
          size_t FINAL_REPS, size_t ODD>
struct UnswizzleMultiple
{
  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                     Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    UnswizzleOnce<T, SIMD_WIDTH, N, 0, 0>::_unswizzle(v, v2);
    UnswizzleOnce<T, SIMD_WIDTH, N, 0, 0>::_unswizzle(v2, v);
    UnswizzleMultiple<T, SIMD_WIDTH, N, REP + 1, FINAL_REPS, ODD>::_unswizzle(
      v, v2);
  }
};

// partial specialization to end the iteration without unswizzle post-amble
template <typename T, size_t SIMD_WIDTH, size_t N, size_t FINAL_REPS,
          size_t ODD>
struct UnswizzleMultiple<T, SIMD_WIDTH, N, FINAL_REPS, FINAL_REPS, ODD>
{
  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH>[2 * N],
                                     Vec<T, SIMD_WIDTH>[2 * N])
  {}
};

// partial specialization to end the iteration with unswizzle post-amble
template <typename T, size_t SIMD_WIDTH, size_t N, size_t FINAL_REPS>
struct UnswizzleMultiple<T, SIMD_WIDTH, N, FINAL_REPS, FINAL_REPS, 1>
{
  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                     Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    UnswizzleOnce<T, SIMD_WIDTH, N, 0, 0>::_unswizzle(v, v2);
    CopyMatrix<T, SIMD_WIDTH, 0, 2 * N>::_copy(v2, v);
  }
};

// ------------------------ unswizzle main meta template ---------------------

template <typename T, size_t SIMD_WIDTH, size_t N>
struct Unswizzle
{
  static constexpr auto FINALBLKSIZE = Vec<T, SIMD_WIDTH>::elements;
  static constexpr auto ORIG_REPS    = floorlog2(FINALBLKSIZE) + 1;
  static constexpr auto FINAL_REPS   = ORIG_REPS / 2;
  static constexpr auto ODD          = (ORIG_REPS & 0x01);

  static SIMD_INLINE void _unswizzle(Vec<T, SIMD_WIDTH> v[2 * N])
  {
    Vec<T, SIMD_WIDTH> v2[2 * N];
    UnswizzleMultiple<T, SIMD_WIDTH, N, 0, FINAL_REPS, ODD>::_unswizzle(v, v2);
  }
};

// ===========================================================================
// unswizzle_b wrapper function
// ===========================================================================

// 15. Oct 22 (Jonas Keller): added unswizzle_b wrapper function

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unswizzle_b(Vec<T, SIMD_WIDTH> v[2 * N])
{
  Unswizzle<T, SIMD_WIDTH, N>::_unswizzle(v);
}

// ===========================================================================
// transpose_g Swizzle2<Zip>
// ===========================================================================

// contributed by Adam Marschall

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_g(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  const auto elems = Vec<T, SIMD_WIDTH>::elements;
  for (size_t i = 0; i < elems; i++) outRows[i] = inRows[i];
  Swizzle2<Zip, T, SIMD_WIDTH, elems / 2, elems / 2>::_swizzle(outRows);
}

// ===========================================================================
// transpose_h: Swizzle2<Zip16> + Swizzle post-process
// ===========================================================================

// contributed by Adam Marschall

// ------------------------ swizzle post-process 16 once ---------------------

// primary template
template <typename T, size_t SIMD_WIDTH, size_t N, size_t SRC, size_t DST,
          size_t LANE_ELEMS>
struct Swizzle2Postprocess16Once
{
  static constexpr auto SRC2 = SRC + 1;
  static constexpr auto DST2 = DST + N;

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    // printf("%s\n", "SwizzlePostprocess16Once");
    // printf("  SRC=%d, SRC2=%d, DST=%d, DS2T=%d\n", SRC, SRC2, DST, DST2);
    Zip16<LANE_ELEMS, T, SIMD_WIDTH>::_zip(v[SRC], v[SRC2], v2[DST], v2[DST2]);
    Swizzle2Postprocess16Once<T, SIMD_WIDTH, N, SRC + 2, DST + 1,
                              LANE_ELEMS>::_swizzle(v, v2);
  }
};

// partial specialization to end the iteration
template <typename T, size_t SIMD_WIDTH, size_t N, size_t SRC,
          size_t LANE_ELEMS>
struct Swizzle2Postprocess16Once<T, SIMD_WIDTH, N, SRC, N, LANE_ELEMS>
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH>[2 * N] /*v*/,
                                   Vec<T, SIMD_WIDTH>[2 * N] /*v2*/)
  {
    // for (size_t i = 0; i < 2 * N; i++) {
    //   print("%4d", v2[i]);
    //   puts("");
    // }
  }
};

// ------------------------ swizzle post-process 16 --------------------------

// primary template
template <typename T, size_t SIMD_WIDTH, size_t N, size_t LANE_ELEMS,
          size_t REP, size_t FINAL_REPS, size_t ODD>
struct Swizzle2Postprocess16
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    // printf("%s\n", "SwizzlePostprocess16");
    // printf("  REP=%d, FINAL_REPS=%d, LANE_ELEMS=%d\n", REP, FINAL_REPS,
    // LANE_ELEMS);
    Swizzle2Postprocess16Once<T, SIMD_WIDTH, N, 0, 0, LANE_ELEMS>::_swizzle(v,
                                                                            v2);
    Swizzle2Postprocess16Once<T, SIMD_WIDTH, N, 0, 0, LANE_ELEMS * 2>::_swizzle(
      v2, v);
    Swizzle2Postprocess16<T, SIMD_WIDTH, N, LANE_ELEMS * 4, REP + 1, FINAL_REPS,
                          ODD>::_swizzle(v, v2);
  }
};

// partial specialization to end the iteration without post-process post-amble
template <typename T, size_t SIMD_WIDTH, size_t N, size_t LANE_ELEMS,
          size_t FINAL_REPS, size_t ODD>
struct Swizzle2Postprocess16<T, SIMD_WIDTH, N, LANE_ELEMS, FINAL_REPS,
                             FINAL_REPS, ODD>
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH>[2 * N],
                                   Vec<T, SIMD_WIDTH>[2 * N])
  {}
};

// partial specialization to end the iteration with post-process post-amble
template <typename T, size_t SIMD_WIDTH, size_t N, size_t LANE_ELEMS,
          size_t FINAL_REPS>
struct Swizzle2Postprocess16<T, SIMD_WIDTH, N, LANE_ELEMS, FINAL_REPS,
                             FINAL_REPS, 1>
{
  static constexpr auto ROW_STOP = 2 * N;

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N],
                                   Vec<T, SIMD_WIDTH> v2[2 * N])
  {
    Swizzle2Postprocess16Once<T, SIMD_WIDTH, N, 0, 0, LANE_ELEMS>::_swizzle(v,
                                                                            v2);
    CopyMatrix<T, SIMD_WIDTH, 0, ROW_STOP>::_copy(v2, v);
  }
};

// ------------------------ swizzle post-process hub -------------------------

// primary template
template <template <size_t, typename, size_t> class Zip, typename T,
          size_t SIMD_WIDTH, size_t N>
struct Swizzle2Postprocess
{
  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH>[2 * N]) {}
};

// partial specialization to post-process Swizzle<Zip16>
template <typename T, size_t SIMD_WIDTH, size_t N>
struct Swizzle2Postprocess<Zip16, T, SIMD_WIDTH, N>
{
  static constexpr auto ORIG_REPS  = floorlog2(SIMD_WIDTH) - 4;
  static constexpr auto FINAL_REPS = ORIG_REPS / 2;
  static constexpr auto ODD        = (ORIG_REPS & 0x01);
  static constexpr auto LANE_ELEMS = 16 / sizeof(T);

  static SIMD_INLINE void _swizzle(Vec<T, SIMD_WIDTH> v[2 * N])
  {
    Vec<T, SIMD_WIDTH> v2[2 * N];
    Swizzle2Postprocess16<T, SIMD_WIDTH, N, LANE_ELEMS, 0, FINAL_REPS,
                          ODD>::_swizzle(v, v2);
  }
};

// ------------------------ transpose_h function call -------------------------

// contributed by Adam Marschall

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_h(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  const auto elems = Vec<T, SIMD_WIDTH>::elements;
  for (size_t i = 0; i < elems; i++) outRows[i] = inRows[i];
  Swizzle2<Zip16, T, SIMD_WIDTH, elems / 2, elems / 2>::_swizzle(outRows);
  Swizzle2Postprocess<Zip16, T, SIMD_WIDTH, elems / 2>::_swizzle(outRows);
}

// ===========================================================================
// transpose_i: register-count based transpose
// ===========================================================================

// contributed by Adam Marschall

// primary template: unpack repetition
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t PROCESS_ROW, size_t PROCESS_ROWS,
          size_t UNPACK_ELEMS, size_t UNPACK_REP, size_t UNPACK_REPS,
          size_t SUB_BASE, size_t SUB>
struct TransposeRcUnpackSingle
{
  static constexpr auto UNPACK_PART =
    (PROCESS_ROW >> (UNPACK_REPS - UNPACK_REP - 1)) & 0x01;
  static constexpr auto UNPACK_PART_NEXT =
    ((PROCESS_ROW + 1) >> (UNPACK_REPS - UNPACK_REP - 1)) & 0x01;
  static constexpr auto SRC1 = (PROCESS_ROW - SUB) * 2;
  static constexpr auto SRC2 = (PROCESS_ROW - SUB) * 2 + 1;

  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH> inRows[PROCESS_ROWS],
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    // printf("%2d <- Unpack<%d, %d, %s, %d>(%2d, %2d) SUB_BASE: %d, SUB: %d\n",
    //    PROCESS_ROW, UNPACK_PART, UNPACK_ELEMS,
    //    TypeInfo<T>::name(), SIMD_WIDTH, SRC1, SRC2, SUB_BASE, SUB);
    outRows[PROCESS_ROW] =
      Unpack<UNPACK_PART, UNPACK_ELEMS, T, SIMD_WIDTH>::_unpack(inRows[SRC1],
                                                                inRows[SRC2]);
    TransposeRcUnpackSingle<
      Unpack, T, SIMD_WIDTH, PROCESS_ROW + 1, PROCESS_ROWS, UNPACK_ELEMS,
      UNPACK_REP, UNPACK_REPS, SUB_BASE,
      SUB + (UNPACK_PART_NEXT == 1 && (PROCESS_ROW + 1) % SUB_BASE == 0 ?
               SUB_BASE :
               0)>::_transpose(inRows, outRows);
  }
};

// partial specialisation to end iteration PROCESS_REP
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t PROCESS_ROWS, size_t UNPACK_ELEMS,
          size_t UNPACK_REP, size_t UNPACK_REPS, size_t SUB_BASE, size_t SUB>
struct TransposeRcUnpackSingle<Unpack, T, SIMD_WIDTH, PROCESS_ROWS,
                               PROCESS_ROWS, UNPACK_ELEMS, UNPACK_REP,
                               UNPACK_REPS, SUB_BASE, SUB>
{
  static SIMD_INLINE void _transpose(
    const Vec<T, SIMD_WIDTH>[PROCESS_ROWS] /*inRows*/,
    Vec<T, SIMD_WIDTH>[PROCESS_ROWS] /*outRows*/)
  {
    // printf("%2d\n", PROCESS_ROWS);
    // for (size_t i = 0; i < PROCESS_ROWS; i++) {
    //   print("%5d", outRows[i]);
    //   puts("");
    // }
    // puts("");
  }
};

// ---------------------------------------------------------------------------

// primary template: unpack repetition
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t UNPACK_REP, size_t UNPACK_REPS,
          size_t PROCESS_ROWS, size_t UNPACK_ELEMS, size_t SUB_BASE,
          size_t UNPACK_ODD>
struct TransposeRcUnpackMultiple
{
  static SIMD_INLINE void _transpose(Vec<T, SIMD_WIDTH> inRows[PROCESS_ROWS],
                                     Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    // printf("\nTransposeRcUnpackMultiple %2d %s  %d/%d\n",
    //        SIMD_WIDTH, TypeInfo<T>::name(), UNPACK_REP+1, UNPACK_REPS);
    TransposeRcUnpackSingle<Unpack, T, SIMD_WIDTH, 0, PROCESS_ROWS,
                            UNPACK_ELEMS, UNPACK_REP, UNPACK_REPS, SUB_BASE,
                            0>::_transpose(inRows, outRows);
    TransposeRcUnpackMultiple<Unpack, T, SIMD_WIDTH, UNPACK_REP + 1,
                              UNPACK_REPS, PROCESS_ROWS, UNPACK_ELEMS * 2,
                              SUB_BASE / 2, UNPACK_ODD>::_transpose(outRows,
                                                                    inRows);
  }
};

// partial specialisation to end iteration UNPACK_REP
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t UNPACK_REPS, size_t PROCESS_ROWS,
          size_t UNPACK_ELEMS, size_t SUB_BASE, size_t UNPACK_ODD>
struct TransposeRcUnpackMultiple<Unpack, T, SIMD_WIDTH, UNPACK_REPS,
                                 UNPACK_REPS, PROCESS_ROWS, UNPACK_ELEMS,
                                 SUB_BASE, UNPACK_ODD>
{
  static SIMD_INLINE void _transpose(Vec<T, SIMD_WIDTH>[PROCESS_ROWS],
                                     Vec<T, SIMD_WIDTH>[PROCESS_ROWS])
  {}
};

// partial specialisation to end iteration UNPACK_REP
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t UNPACK_REPS, size_t PROCESS_ROWS,
          size_t UNPACK_ELEMS, size_t SUB_BASE>
struct TransposeRcUnpackMultiple<Unpack, T, SIMD_WIDTH, UNPACK_REPS,
                                 UNPACK_REPS, PROCESS_ROWS, UNPACK_ELEMS,
                                 SUB_BASE, 0>
{
  static SIMD_INLINE void _transpose(Vec<T, SIMD_WIDTH> inRows[PROCESS_ROWS],
                                     Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    // printf("\nTransposeRcUnpackMultiple %2d %s  %d/%d Copy Matrix\n",
    //        SIMD_WIDTH, TypeInfo<T>::name(), UNPACK_REPS, UNPACK_REPS);
    CopyMatrix<T, SIMD_WIDTH, 0, PROCESS_ROWS>::_copy(inRows, outRows);
  }
};

// ---------------------------------------------------------------------------

// primary template: store all registers lane-wise
template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_REPS, size_t PROCESS_ROWS, size_t UNPACK_REPS,
          size_t STORE_OFF, size_t VO, size_t LANE>
struct TransposeRcStoreLane
{
  static constexpr auto VEC_ELEMS_OUT = Vec<T, SIMD_WIDTH>::elems;

  static SIMD_INLINE void _store(
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    storeu(outArray + STORE_OFF, extractLane<LANE>(outRows[VO]));
    TransposeRcStoreLane<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS, PROCESS_ROWS,
                         UNPACK_REPS, STORE_OFF + PROCESS_ROWS * VEC_ELEMS_OUT,
                         VO, LANE + 1>::_store(outArray, outRows);
  }
};

// partial specialisation to end iteration LANE=PROCESS_REPS
template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_REPS, size_t PROCESS_ROWS, size_t UNPACK_REPS,
          size_t STORE_OFF, size_t VO>
struct TransposeRcStoreLane<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS,
                            PROCESS_ROWS, UNPACK_REPS, STORE_OFF, VO,
                            PROCESS_REPS>
{
  static SIMD_INLINE void _store(
    T[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[PROCESS_ROWS])
  {}
};

// ---------------------------------------------------------------------------

// primary template: store all registers lane-wise
template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_REPS, size_t PROCESS_ROWS, size_t UNPACK_REPS,
          size_t STORE_OFF, size_t VO>
struct TransposeRcStoreLanes
{
  static constexpr auto VEC_ELEMS_OUT = Vec<T, SIMD_WIDTH>::elems;

  static SIMD_INLINE void _store(
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    TransposeRcStoreLane<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS, PROCESS_ROWS,
                         UNPACK_REPS, STORE_OFF, VO, 0>::_store(outArray,
                                                                outRows);
    TransposeRcStoreLanes<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS,
                          PROCESS_ROWS, UNPACK_REPS, STORE_OFF + VEC_ELEMS_OUT,
                          VO + 1>::_store(outArray, outRows);
  }
};

// partial specialisation to end iteration VO=PROCESS_ROWS
template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_REPS, size_t PROCESS_ROWS, size_t UNPACK_REPS,
          size_t STORE_OFF>
struct TransposeRcStoreLanes<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS,
                             PROCESS_ROWS, UNPACK_REPS, STORE_OFF, PROCESS_ROWS>
{
  static SIMD_INLINE void _store(
    T[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH>[PROCESS_ROWS])
  {}
};

// ---------------------------------------------------------------------------

// primary template: store hub
// decides whether to store directly (Store16) or to store lane-wise
template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_REPS, size_t PROCESS_ROWS, size_t UNPACK_REPS>
struct TransposeRcStore
{
  static constexpr auto ELEMS_PER_LANE = 16 / sizeof(T);
  static constexpr auto STORE_OFF      = PROCESS_REP * ELEMS_PER_LANE;

  static SIMD_INLINE void _store(
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    TransposeRcStoreLanes<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS,
                          PROCESS_ROWS, UNPACK_REPS, STORE_OFF,
                          0>::_store(outArray, outRows);
  }
};

template <typename T, size_t SIMD_WIDTH, size_t PROCESS_REP,
          size_t PROCESS_ROWS, size_t UNPACK_REPS>
struct TransposeRcStore<T, SIMD_WIDTH, PROCESS_REP, 1, PROCESS_ROWS,
                        UNPACK_REPS>
{
  static SIMD_INLINE void _store(
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS])
  {
    // printf("\nStore16 PROCESS_ROWS=%d\n", PROCESS_ROWS);
    Store16<Store, T, SIMD_WIDTH, PROCESS_ROWS, 0, 16 / sizeof(T), SIMD_WIDTH,
            0, 0>::_store16(outArray, outRows);
  }
};

// ---------------------------------------------------------------------------

// primary template: main repetition
// loads, transposes, stores chunk of matrix
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t PROCESS_REP, size_t PROCESS_REPS,
          size_t PROCESS_ROWS, size_t UNPACK_REPS, size_t UNPACK_ODD>
struct TransposeRcRep
{
  static constexpr auto LOAD_OFF =
    PROCESS_REP * PROCESS_ROWS * SIMD_WIDTH / sizeof(T);
  static constexpr auto SUB_BASE = 1 << (floorlog2(PROCESS_ROWS) - 1);

  static SIMD_INLINE void _transpose(
    const T inArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("\nTransposeRcRep %2d %s  %d/%d\n",
    //         SIMD_WIDTH, TypeInfo<T>::name(), PROCESS_REP+1,
    //         PROCESS_REPS);
    Vec<T, SIMD_WIDTH> inRows[PROCESS_ROWS];
    Vec<T, SIMD_WIDTH> outRows[PROCESS_ROWS];
    load(inArray + LOAD_OFF, inRows, PROCESS_ROWS);
    // for (size_t i = 0; i < PROCESS_ROWS; i++) {
    //   print("%5d", inRows[i]);
    //   puts("");
    // }
    // puts("");
    TransposeRcUnpackMultiple<Unpack, T, SIMD_WIDTH, 0, UNPACK_REPS,
                              PROCESS_ROWS, 1, SUB_BASE,
                              UNPACK_ODD>::_transpose(inRows, outRows);
    // for (size_t i = 0; i < PROCESS_ROWS; i++) {
    //   print("%5d", outRows[i]);
    //   puts("");
    // }
    // puts("");
    TransposeRcStore<T, SIMD_WIDTH, PROCESS_REP, PROCESS_REPS, PROCESS_ROWS,
                     UNPACK_REPS>::_store(outArray, outRows);
    TransposeRcRep<Unpack, T, SIMD_WIDTH, PROCESS_REP + 1, PROCESS_REPS,
                   PROCESS_ROWS, UNPACK_REPS, UNPACK_ODD>::_transpose(inArray,
                                                                      outArray);
  }
};

// partial specialisation to end iteration PROCESS_REP
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH, size_t PROCESS_REPS, size_t PROCESS_ROWS,
          size_t UNPACK_REPS, size_t UNPACK_ODD>
struct TransposeRcRep<Unpack, T, SIMD_WIDTH, PROCESS_REPS, PROCESS_REPS,
                      PROCESS_ROWS, UNPACK_REPS, UNPACK_ODD>
{
  static SIMD_INLINE void _transpose(
    const T[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    T[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems])
  {}
};

// ---------------------------------------------------------------------------

// primary template: main entrance
template <template <size_t, size_t, typename, size_t> class Unpack, typename T,
          size_t SIMD_WIDTH>
struct TransposeRc
{
  static constexpr auto SIMD_REGS = NATIVE_SIMD_REG_COUNT / 2;
  static constexpr auto NUM_ROWS  = SIMD_WIDTH / sizeof(T);
  static constexpr auto PROCESS_REPS =
    NUM_ROWS > SIMD_REGS ? SIMD_WIDTH / 16 : 1;
  static constexpr auto PROCESS_ROWS = NUM_ROWS / PROCESS_REPS;
  static constexpr auto UNPACK_REPS =
    PROCESS_REPS == 1 ? floorlog2(PROCESS_ROWS) : floorlog2(16 / sizeof(T));
  static constexpr auto UNPACK_ODD = (UNPACK_REPS & 0x01);

  static SIMD_INLINE void _transpose(
    const T inArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems],
    T outArray[Vec<T, SIMD_WIDTH>::elems * Vec<T, SIMD_WIDTH>::elems])
  {
    // printf("TransposeRc Process Rows: %d \n", PROCESS_ROWS);
    TransposeRcRep<Unpack, T, SIMD_WIDTH, 0, PROCESS_REPS, PROCESS_ROWS,
                   UNPACK_REPS, UNPACK_ODD>::_transpose(inArray, outArray);
    // printf("%s","\n");
  }
};

// ---------------------------------------------------------------------------

// contributed by Adam Marschall

// function template: full transpose
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose_i(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  // 20. Sep 22 (Jonas Keller):
  // use simd_aligned_malloc for inArray and outArray
  // and free them at the end of the function
  // 30. Jul 23 (Jonas Keller):
  // put inArray and outArray on the stack instead of heap to avoid allocation
  // and possibly allow for better compiler optimisation
  const auto N = Vec<T, SIMD_WIDTH>::elements;
  T inArray[N * N] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  T outArray[N * N] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  store(inArray, inRows, N);
  TransposeRc<Unpack16, T, SIMD_WIDTH>::_transpose(inArray, outArray);
  load(outArray, outRows, N);
}

// ===========================================================================
// unswizzle_a (interleave)
// ===========================================================================

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unswizzle_a(Vec<T, SIMD_WIDTH> v[2 * N])
{
  const auto finalBlkSize = Vec<T, SIMD_WIDTH>::elements;
  Vec<T, SIMD_WIDTH> v2[2 * N];
  for (size_t blkSize = 1; blkSize <= finalBlkSize; blkSize *= 2) {
    // zip
    for (size_t dst = 0, src = 0; dst < N; dst++, src += 2)
      unzip<1>(v[src], v[src + 1], v2[dst], v2[dst + N]);
    // copy result back to v
    // TODO: unswizzle_a: check code produced by compiler for copying
    for (size_t i = 0; i < 2 * N; i++) v[i] = v2[i];
  }
}

} // namespace ext
} // namespace internal

/**
 * @ingroup group_swizzle
 * @brief Swizzle/de-interleave/convert from AoS to SoA multiple Vec's in-place.
 *
 * This function swizzles/de-interleaves/converts from AoS (Array of Structs) to
 * SoA (Struct of Arrays) multiple Vec's in-place.
 *
 * In contrast to swizzle(), this function takes double the number of Vec's as
 * input and might be faster.
 *
 * <h4>Example:</h4>
 * Example for a swizzle distance of 3 with 6 Vec's of 8 elements each:
 *
 * input stream (structures indicated by curly brackets):
 * @code
 * {0 1 2} {3 4 5} {6 7 8} {9 10 11} ... {45 46 47}
 * @endcode
 * input vectors:
 * @code
 * v[0] =  0  1  2  3  4  5  6  7
 * v[1] =  8  9 10 11 12 13 14 15
 * v[2] = 16 17 18 19 20 21 22 23
 * v[3] = 24 25 26 27 28 29 30 31
 * v[4] = 32 33 34 35 36 37 38 39
 * v[5] = 40 41 42 43 44 45 46 47
 * @endcode
 * output vectors:
 * @code
 * v[0] =  0  6 12 18 24 30 36 42
 * v[1] =  1  7 13 19 25 31 37 43
 * v[2] =  2  8 14 20 26 32 38 44
 * v[3] =  3  9 15 21 27 33 39 45
 * v[4] =  4 10 16 22 28 34 40 46
 * v[5] =  5 11 17 23 29 35 41 47
 * @endcode
 *
 * @tparam N swizzle distance, must be between 1 and 5
 * @param[in,out] v array of Vec's to swizzle
 *
 * @sa swizzle(): swizzles half the number of Vec's as this function
 * @sa unswizzle()
 */
template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2(Vec<T, SIMD_WIDTH> v[2 * N])
{
  // uncomment fastest version
  // internal::ext::swizzle2_a<N>(v);
  // internal::ext::swizzle2_b<N>(v);
  internal::ext::swizzle2_c<N>(v);
}

/**
 * @ingroup group_swizzle
 * @brief Unswizzle/interleave/convert from SoA to AoS multiple Vec's in-place.
 *
 * This function unswizzles/interleaves/converts from SoA (Struct of Arrays) to
 * AoS (Array of Structs) multiple Vec's in-place.
 *
 * <h4>Example:</h4>
 * Example for an unswizzle distance of 3 with 6 Vec's of 8 elements each:
 *
 * input vectors:
 * @code
 * v[0] =  0  6 12 18 24 30 36 42
 * v[1] =  1  7 13 19 25 31 37 43
 * v[2] =  2  8 14 20 26 32 38 44
 * v[3] =  3  9 15 21 27 33 39 45
 * v[4] =  4 10 16 22 28 34 40 46
 * v[5] =  5 11 17 23 29 35 41 47
 * @endcode
 * output vectors:
 * @code
 * v[0] =  0  1  2  3  4  5  6  7
 * v[1] =  8  9 10 11 12 13 14 15
 * v[2] = 16 17 18 19 20 21 22 23
 * v[3] = 24 25 26 27 28 29 30 31
 * v[4] = 32 33 34 35 36 37 38 39
 * v[5] = 40 41 42 43 44 45 46 47
 * @endcode
 *
 * @tparam N unswizzle distance
 * @param[in,out] v array of Vec's to unswizzle
 *
 * @sa swizzle(), swizzle2(): %swizzle functions
 */
template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unswizzle(Vec<T, SIMD_WIDTH> v[2 * N])
{
  // uncomment fastest version
  // internal::ext::unswizzle_a<N>(v);
  internal::ext::unswizzle_b<N>(v);
}

/**
 * @ingroup group_reordering
 * @brief Transposes a matrix held in an array of Vec's.
 *
 * The matrix must be given as an array of Vec's which are the rows of
 * the matrix. The number of rows must be equal to the number of elements
 * in a Vec (<tt>SIMD_WIDTH/sizeof(T)</tt>), i.e. the matrix must be
 * square.
 *
 * @param[in] inRows array of Vec's holding the matrix to be transposed
 * @param[out] outRows array of Vec's where the transposed matrix is stored
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose(
  const Vec<T, SIMD_WIDTH> inRows[Vec<T, SIMD_WIDTH>::elems],
  Vec<T, SIMD_WIDTH> outRows[Vec<T, SIMD_WIDTH>::elems])
{
  // uncomment fastest version

  // 06. Sep 23 (Jonas Keller):
  // added transpose1inplc, transpose2inplc, transpose1inplcLane and
  // transpose2inplcLane and switched to transpose1inplcLane

  // internal::ext::transpose_a(inRows, outRows);
  // internal::ext::transpose_b(inRows, outRows);
  // internal::ext::transpose_c(inRows, outRows);
  // internal::ext::transpose_d(inRows, outRows);
  // internal::ext::transpose_e(inRows, outRows);
  // internal::ext::transpose_f(inRows, outRows);
  // internal::ext::transpose_g(inRows, outRows);
  // internal::ext::transpose_h(inRows, outRows);
  // internal::ext::transpose_i(inRows, outRows);
  // internal::ext::transpose1inplc(inRows, outRows);
  // internal::ext::transpose2inplc(inRows, outRows);
  internal::ext::transpose1inplcLane(inRows, outRows);
  // internal::ext::transpose2inplcLane(inRows, outRows);
}

// ===========================================================================
// setones: set all bits to 1
// ===========================================================================

/**
 * @ingroup group_init
 * @brief Sets all bits of a Vec to 1.
 *
 * @return Vec with all bits set to 1
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setones()
{
  Vec<T, SIMD_WIDTH> zero = setzero<T, SIMD_WIDTH>();
  return cmpeq(zero, zero);
}

// ===========================================================================
// setmin / setmax: set all elements min./max. value of type without set1()
// setunity: set all elements to +1
// setnegunity: set all elements to -1
// ===========================================================================

/**
 * @ingroup group_init
 * @brief Sets all elements of a Vec to the minimum value of the element
 * type.
 *
 * @return Vec with all elements set to the minimum value of the element type
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setmin()
{
  return set1<T, SIMD_WIDTH>(std::numeric_limits<T>::lowest());
}

/**
 * @ingroup group_init
 * @brief Sets all elements of a Vec to the maximum value of the element
 * type.
 *
 * @return Vec with all elements set to the maximum value of the element type
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setmax()
{
  return set1<T, SIMD_WIDTH>(std::numeric_limits<T>::max());
}

/**
 * @ingroup group_init
 * @brief Sets all elements of a Vec to the value 1.
 *
 * @return Vec with all elements set to the value 1
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setunity()
{
  return set1<T, SIMD_WIDTH>(T(1));
}

/**
 * @ingroup group_init
 * @brief Sets all elements of a Vec to the value -1.
 *
 * Only available for signed integer and floating point types.
 *
 * @return Vec with all elements set to the value -1
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> setnegunity()
{
  static_assert(std::is_signed<T>::value || std::is_floating_point<T>::value,
                "setnegunity() only available for signed integer and floating "
                "point types");
  return set1<T, SIMD_WIDTH>(T(-1));
}

// ===========================================================================
// bitonic sort
// ===========================================================================

/**
 * @addtogroup group_simd_sort
 * @{
 */

// code contributed by Lukas Schiermeier and Moritz Breipohl, modified

namespace internal {
namespace ext {
// compare-and-swap
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
struct Cas;

// specialization for DESCENDING
template <typename T, size_t SIMD_WIDTH>
struct Cas<SortSlope::DESCENDING, T, SIMD_WIDTH>
{
  static void compareAndSwap(Vec<T, SIMD_WIDTH> &a, Vec<T, SIMD_WIDTH> &b)
  {
    Vec<T, SIMD_WIDTH> temp = min(a, b);
    a                       = max(a, b);
    b                       = temp;
  }
};

// specialization for ASCENDING
template <typename T, size_t SIMD_WIDTH>
struct Cas<SortSlope::ASCENDING, T, SIMD_WIDTH>
{
  static void compareAndSwap(Vec<T, SIMD_WIDTH> &a, Vec<T, SIMD_WIDTH> &b)
  {
    Vec<T, SIMD_WIDTH> temp = max(a, b);
    a                       = min(a, b);
    b                       = temp;
  }
};

// in-place sorting of multiple arbitrary vectors;
// transVecs have to be transposed vectors (same number of elements
// as in Vec), are still transposed afterwards
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE void bitonicSortTransposed(
  Vec<T, SIMD_WIDTH> transVecs[Vec<T, SIMD_WIDTH>::elems])
{
  constexpr auto numVecs = Vec<T, SIMD_WIDTH>::elements;
  /* Dependent Loops */
  for (size_t blkSize = 2; blkSize <= numVecs; blkSize *= 2) {
    /*
     * Bitonic Core
     * Independent Loops
     */
    for (size_t blkStart = 0; blkStart < numVecs; blkStart += blkSize) {
      size_t halfBlk      = blkSize / 2;
      size_t leftCounter  = blkStart;
      size_t rightCounter = blkStart + (blkSize - 1);
      /* Independent Loops */
      for (size_t i = 0; i < halfBlk; i++) {
        Cas<SLOPE, T, SIMD_WIDTH>::compareAndSwap(transVecs[leftCounter],
                                                  transVecs[rightCounter]);
        leftCounter++;
        rightCounter--;
      }
      /*
       * This loop is skipped for blkSize < 4
       * Builds the second half of the bitonic core.
       *
       * Dependent Loops
       */
      for (size_t step = blkSize / 4; step > 0; step /= 2) {
        /* Independent Loops */
        for (size_t jump = 0; jump < blkSize; jump += step * 2) {
          leftCounter  = blkStart + jump;
          rightCounter = blkStart + jump + step;
          /* Independent Loops */
          for (size_t k = 0; k < step; k++) {
            Cas<SLOPE, T, SIMD_WIDTH>::compareAndSwap(transVecs[leftCounter],
                                                      transVecs[rightCounter]);
            leftCounter++;
            rightCounter++;
          }
        }
      }
    }
  }
}

// post-fusion stage of bitonic sort, used to sort pairs of sorted vectors
// which were fused (one reversed) and then sorted such that the pair
// is sorted over the two vectors
// in-place sorting; transVecs have to be transposed vectors (same
// number of elements as in Vec), are still transposed
// afterwards
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE void bitonicSortReducedTransposed(
  Vec<T, SIMD_WIDTH> transVecs[Vec<T, SIMD_WIDTH>::elems])
{
  constexpr auto numVecs = Vec<T, SIMD_WIDTH>::elements;
  for (size_t step = numVecs / 2; step > 0; step /= 2) {
    /* Independent Loops */
    for (size_t jump = 0; jump < numVecs; jump += step * 2) {
      size_t leftCounter  = jump;
      size_t rightCounter = jump + step;
      /* Independent Loops */
      for (size_t k = 0; k < step; k++) {
        Cas<SLOPE, T, SIMD_WIDTH>::compareAndSwap(transVecs[leftCounter],
                                                  transVecs[rightCounter]);
        leftCounter++;
        rightCounter++;
      }
    }
  }
}
} // namespace ext
} // namespace internal

/**
 * @brief Sorts multiple Vec's independently using the bitonic sort algorithm.
 *
 * @tparam SLOPE direction to sort in (SortSlope::ASCENDING or
 * SortSlope::DESCENDING)
 * @param[in, out] vecs array of Vec's to sort
 */
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE void bitonicSort(
  Vec<T, SIMD_WIDTH> vecs[Vec<T, SIMD_WIDTH>::elems])
{
  Vec<T, SIMD_WIDTH> transVecs[Vec<T, SIMD_WIDTH>::elements];
  transpose(vecs, transVecs);
  internal::ext::bitonicSortTransposed<SLOPE>(transVecs);
  transpose(transVecs, vecs);
}

namespace internal {
namespace ext {
// second vector is reversed and fused with first vector
// we don't have to reverse b after the compare-swap since it is
// bitonic
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE void bitonicFusion(Vec<T, SIMD_WIDTH> &a,
                                      Vec<T, SIMD_WIDTH> &b)
{
  b = reverse(b);
  Cas<SLOPE, T, SIMD_WIDTH>::compareAndSwap(a, b);
}
} // namespace ext
} // namespace internal

// given sorted vectors as inputs, it fuses each consecutive pair
// such it is completely sorted over the pair

/**
 * @brief Fuses consecutive pairs of sorted Vec's such that the pair is sorted
 * over the two vectors.
 *
 * @tparam SLOPE direction to sort in (SortSlope::ASCENDING or
 * SortSlope::DESCENDING)
 * @param vecs array of Vec's to sort (Vec's must be sorted individually)
 */
template <SortSlope SLOPE, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE void bitonicSortSortedPairs(
  Vec<T, SIMD_WIDTH> vecs[Vec<T, SIMD_WIDTH>::elems])
{
  Vec<T, SIMD_WIDTH> transVecs[Vec<T, SIMD_WIDTH>::elements];
  // second vector of each pair is reversed and fused with first vector
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elements; i += 2)
    internal::ext::bitonicFusion<SLOPE>(vecs[i], vecs[i + 1]);
  transpose(vecs, transVecs);
  internal::ext::bitonicSortReducedTransposed<SLOPE>(transVecs);
  transpose(transVecs, vecs);
}

/** @} */

// ===========================================================================
// operators
// ===========================================================================

// C++ Coding Standards p.49 (item 27)

#define SIMDVEC_BINOPEQ(OP, FCT)                                               \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> OP(Vec<T, SIMD_WIDTH> &a,              \
                                           const Vec<T, SIMD_WIDTH> &b)        \
  {                                                                            \
    a = FCT(a, b);                                                             \
    return a;                                                                  \
  }

#define SIMDVEC_BINOP(OP, FCT)                                                 \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> OP(const Vec<T, SIMD_WIDTH> &a,        \
                                           const Vec<T, SIMD_WIDTH> &b)        \
  {                                                                            \
    return FCT(a, b);                                                          \
  }

#define SIMDVEC_UNOP(OP, FCT)                                                  \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> OP(const Vec<T, SIMD_WIDTH> &a)        \
  {                                                                            \
    return FCT(a);                                                             \
  }

// limitations:
// - mul, div only for Float
// - neg only for signed types

/**
 * @addtogroup group_operators
 * @{
 */

/// @brief Addition operator. Maps to adds(). @sa adds()
SIMDVEC_BINOP(operator+, adds)
/// @brief Subtraction operator. Maps to subs(). @sa subs()
SIMDVEC_BINOP(operator-, subs)
/// @brief Multiplication operator. Maps to mul(). @sa mul()
SIMDVEC_BINOP(operator*, mul)
/// @brief Division operator. Maps to div(). @sa div()
SIMDVEC_BINOP(operator/, div)
/// @brief Bitwise AND operator. Maps to bit_and(). @sa bit_and()
SIMDVEC_BINOP(operator&, bit_and)
/// @brief Bitwise OR operator. Maps to bit_or(). @sa bit_or()
SIMDVEC_BINOP(operator|, bit_or)
/// @brief Bitwise XOR operator. Maps to bit_xor(). @sa bit_xor()
SIMDVEC_BINOP(operator^, bit_xor)

/// @brief Addition assignment operator. Maps to adds(). @sa adds()
SIMDVEC_BINOPEQ(operator+=, adds)
/// @brief Subtraction assignment operator. Maps to subs(). @sa subs()
SIMDVEC_BINOPEQ(operator-=, subs)
/// @brief Multiplication assignment operator. Maps to mul(). @sa mul()
SIMDVEC_BINOPEQ(operator*=, mul)
/// @brief Division assignment operator. Maps to div(). @sa div()
SIMDVEC_BINOPEQ(operator/=, div)
/// @brief Bitwise AND assignment operator. Maps to bit_and(). @sa bit_and()
SIMDVEC_BINOPEQ(operator&=, bit_and)
/// @brief Bitwise OR assignment operator. Maps to bit_or(). @sa bit_or()
SIMDVEC_BINOPEQ(operator|=, bit_or)
/// @brief Bitwise XOR assignment operator. Maps to bit_xor(). @sa bit_xor()
SIMDVEC_BINOPEQ(operator^=, bit_xor)

/// @brief Greater than operator. Maps to cmpgt(). @sa cmpgt()
SIMDVEC_BINOP(operator>, cmpgt)
/// @brief Greater than or equal operator. Maps to cmpge(). @sa cmpge()
SIMDVEC_BINOP(operator>=, cmpge)
/// @brief Equal to operator. Maps to cmpeq(). @sa cmpeq()
SIMDVEC_BINOP(operator==, cmpeq)
/// @brief Not equal to operator. Maps to cmpneq(). @sa cmpneq()
SIMDVEC_BINOP(operator!=, cmpneq)
/// @brief Less than or equal operator. Maps to cmple(). @sa cmple()
SIMDVEC_BINOP(operator<=, cmple)
/// @brief Less than operator. Maps to cmplt(). @sa cmplt()
SIMDVEC_BINOP(operator<, cmplt)

/// @brief Negation operator. Maps to neg(). @sa neg()
SIMDVEC_UNOP(operator-, neg)
/// @brief Bitwise NOT operator. Maps to bit_not(). @sa bit_not()
SIMDVEC_UNOP(operator~, bit_not)

/** @} */
} // namespace simd

#endif

// Vecs: multiple Vec in template class
// ===========================================================================
//
// Vecs.H --
// multiple Vec in a template class plus some functions
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

#ifndef SIMD_VECS_H_
#define SIMD_VECS_H_

// exclude from doxygen
// TODO: include in doxygen documentation?

// exclude from doxygen (until endcond)
/// @cond

namespace simd {

// ===========================================================================
// Vecs: for functions operating on multiple Vec
// ===========================================================================

// class to compute number of input and output vectors of conversion functions
// (convert, packs, extend, f*)
template <typename Tout, typename Tin>
struct NumVecs
{
  static constexpr size_t in =
    (sizeof(Tout) < sizeof(Tin)) ? (sizeof(Tin) / sizeof(Tout)) : 1;
  static constexpr size_t out =
    (sizeof(Tout) > sizeof(Tin)) ? (sizeof(Tout) / sizeof(Tin)) : 1;
};

// multiple Vec

template <size_t NUM, typename T, size_t SIMD_WIDTH>
struct Vecs
{
  static constexpr size_t vectors  = NUM;
  static constexpr size_t elements = NUM * Vec<T, SIMD_WIDTH>::elements;
  static constexpr size_t bytes    = NUM * Vec<T, SIMD_WIDTH>::bytes;
  // shorter version:
  static constexpr size_t vecs  = vectors;
  static constexpr size_t elems = elements;

  Vec<T, SIMD_WIDTH> vec[NUM];
};

// wrapper functions working on Vecs

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void convert(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &inVecs,
  Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &outVecs)
{
  convert(inVecs.vec, outVecs.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void fdivmul(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsNum,
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsDenom, double fac,
  Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vecsOut)
{
  fdivmul(vecsNum.vec, vecsDenom.vec, fac, vecsOut.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void fmul(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsIn, double fac,
  Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vecsOut)
{
  fmul(vecsIn.vec, fac, vecsOut.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void faddmul(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsIn, double off,
  double fac, Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vecsOut)
{
  faddmul(vecsIn.vec, off, fac, vecsOut.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void fmuladd(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsIn, double fac,
  double off, Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vecsOut)
{
  fmuladd(vecsIn.vec, fac, off, vecsOut.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void fwaddmul(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsIn1,
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &vecsIn2, double w,
  double fac, Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vecsOut)
{
  fwaddmul(vecsIn1.vec, vecsIn2.vec, w, fac, vecsOut.vec);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void load(const T *const p, Vecs<NUM, T, SIMD_WIDTH> &inVecs)
{
  load(p, inVecs.vec, inVecs.vectors);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void loadu(const T *const p,
                              Vecs<NUM, T, SIMD_WIDTH> &inVecs)
{
  loadu(p, inVecs.vec, inVecs.vectors);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store(T *const p,
                              const Vecs<NUM, T, SIMD_WIDTH> &outVecs)
{
  store(p, outVecs.vec, outVecs.vectors);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu(T *const p,
                               const Vecs<NUM, T, SIMD_WIDTH> &outVecs)
{
  storeu(p, outVecs.vec, outVecs.vectors);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void store16(T *const p,
                                const Vecs<NUM, T, SIMD_WIDTH> &outVecs)
{
  store16(p, outVecs.vec);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void storeu16(T *const p,
                                 const Vecs<NUM, T, SIMD_WIDTH> &outVecs)
{
  storeu16(p, outVecs.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> packs(
  const Vecs<NumVecs<Tout, Tin>::in, Tin, SIMD_WIDTH> &a)
{
  return packs<Tout>(a.vec);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE void extend(
  const Vec<Tin, SIMD_WIDTH> &vIn,
  Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> &vOut)
{
  extend(vIn, vOut.vec);
}

// with Vecs as return value
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> extend(
  const Vec<Tin, SIMD_WIDTH> &vIn)
{
  Vecs<NumVecs<Tout, Tin>::out, Tout, SIMD_WIDTH> vOut;
  extend(vIn, vOut);
  return vOut;
}

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle(Vecs<N, T, SIMD_WIDTH> &v)
{
  swizzle<N>(v.vec);
}

// inRows passed by-value to allow in-place transpose
// 30. Sep 22 (rm): was called transpose1, moved back to transpose
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void transpose(
  const Vecs<(SIMD_WIDTH / sizeof(T)), T, SIMD_WIDTH> inRows,
  Vecs<(SIMD_WIDTH / sizeof(T)), T, SIMD_WIDTH> &outRows)
{
  transpose(inRows.vec, outRows.vec);
}

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void swizzle2(Vecs<2 * N, T, SIMD_WIDTH> &v)
{
  swizzle2(v.vec);
}

template <size_t N, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void unswizzle(Vecs<2 * N, T, SIMD_WIDTH> &v)
{
  unswizzle(v.vec);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadd(
  const Vecs<(SIMD_WIDTH / sizeof(T)), T, SIMD_WIDTH> &v)
{
  return hadd(v.vec);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> hadds(
  const Vecs<(SIMD_WIDTH / sizeof(T)), T, SIMD_WIDTH> &v)
{
  return hadds(v.vec);
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> add(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = add(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> adds(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = adds(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> sub(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = sub(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> subs(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = subs(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> min(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = min(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> max(
  const Vecs<NUM, T, SIMD_WIDTH> &a, const Vecs<NUM, T, SIMD_WIDTH> &b)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = max(a.vec[v], b.vec[v]);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> setzero()
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = setzero<T, SIMD_WIDTH>();
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void setzero(Vecs<NUM, T, SIMD_WIDTH> &res)
{
  for (size_t v = 0; v < NUM; v++) res.vec[v] = setzero<T, SIMD_WIDTH>();
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vecs<NUM, T, SIMD_WIDTH> set1(T a)
{
  Vecs<NUM, T, SIMD_WIDTH> res;
  for (size_t v = 0; v < NUM; v++) res.vec[v] = set1<T, SIMD_WIDTH>(a);
  return res;
}

template <size_t NUM, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void set1(Vecs<NUM, T, SIMD_WIDTH> &res, T a)
{
  for (size_t v = 0; v < NUM; v++) res.vec[v] = set1<T, SIMD_WIDTH>(a);
}

// TODO: add more Vecs functions if needed

} // namespace simd

/// @endcond

#endif

// templates/functions for masked operations
// ===========================================================================
//
// SIMDVecMask.H --
// mask classes and masked functions
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Jonas Keller, Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): introduced wrapper layer that wraps the internal
// architecture-specific implementations

// 02. Mar 23 (Jonas Keller): added doxygen documentation

#ifndef SIMD_VEC_MASK_H_
#define SIMD_VEC_MASK_H_

// ===========================================================================
//
// SIMDVecMaskImplEmu.H --
// emulated mask functions
// Author: Markus Vieth (Bielefeld University, mvieth@techfak.uni-bielefeld.de)
// Year of creation: 2019
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Markus Vieth, Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace

// 01. Feb 23 (Jonas Keller): implemented the emulated mask functions in a more
// efficient way, described below and also optimized other small things.
// 30. Nov 22 (Jonas Keller):
// NOTE:
// The float versions of the emulated mask functions in this file as well as in
// SIMDVecMaskImplIntel64.H are not as fast as they could be, as they are
// implemented such that they match the not emulated ones in flag and exception
// behavior as well. This is done by masking the inputs of the masked functions,
// which for example leads to the following code for masked addition:
/*
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH>
maskz_add(const Mask<Float, SIMD_WIDTH> &k,
          const Vec<Float, SIMD_WIDTH> &a,
          const Vec<Float, SIMD_WIDTH> &b)
{
  return add(mask_ifelse(k, a, setzero<Float, SIMD_WIDTH>()),
             mask_ifelse(k, b, setzero<Float, SIMD_WIDTH>()));
}
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH>
mask_add(const Vec<Float, SIMD_WIDTH> &src,
         const Mask<Float, SIMD_WIDTH> &k,
         const Vec<Float, SIMD_WIDTH> &a,
         const Vec<Float, SIMD_WIDTH> &b)
{
  return mask_ifelse(k, maskz_add(k, a, b), src);
}
*/
// which calls mask_ifelse 3 times for one call of mask_add, instead of
/*
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH>
maskz_add(const Mask<Float, SIMD_WIDTH> &k,
          const Vec<Float, SIMD_WIDTH> &a,
          const Vec<Float, SIMD_WIDTH> &b)
{
  return mask_ifelse(k, add(a, b), setzero<Float, SIMD_WIDTH>());
}
template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH>
mask_add(const Vec<Float, SIMD_WIDTH> &src,
         const Mask<Float, SIMD_WIDTH> &k,
         const Vec<Float, SIMD_WIDTH> &a,
         const Vec<Float, SIMD_WIDTH> &b)
{
  return mask_ifelse(k, add(a, b), k);
}
*/
// which calls mask_ifelse only once for one call of mask_add.
//
// The second version would however for example set the denormal flag if an
// input is denormalized, even if the corresponding mask bit is not set, which
// is different from the behavior of the not emulated mask functions.
//
// It may be worth considering to implement the emulated mask functions
// analogous to the second version to improve performance. This would
// change the flag/exception behavior of the emulated mask functions.
// However, the flag/exception behavior is probably not correct in the
// whole library anyway, and probably also different on ARM.
// Additionally, the T-SIMD does not provide an architecture independent
// way to use flags or exceptions, so emulating them does not make much
// sense anyway.

#include <cstddef>
#ifndef SIMD_VEC_MASK_IMPL_EMU_H_
#define SIMD_VEC_MASK_IMPL_EMU_H_

// ===========================================================================
//
// SIMDVecMaskImplIntel64.H --
// Mask class definitions and architecture specific functions
// for Intel 64 byte (512 bit)
// Author: Markus Vieth (Bielefeld University, mvieth@techfak.uni-bielefeld.de)
// Year of creation: 2019
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Markus Vieth, Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 22. Jan 23 (Jonas Keller): moved internal implementations into internal
// namespace

#ifndef SIMD_VEC_MASK_IMPL_INTEL_64_H_
#define SIMD_VEC_MASK_IMPL_INTEL_64_H_

#include <algorithm>
#include <cstdint>
#include <cstring>
#include <type_traits>

#if defined(SIMDVEC_INTEL_ENABLE) && defined(_SIMD_VEC_64_AVAIL_) &&           \
  !defined(SIMDVEC_SANDBOX)

namespace simd {
#define CLASS_MASK(TYPE, MASK_SIZE)                                            \
  template <>                                                                  \
  class Mask<TYPE, 64>                                                         \
  {                                                                            \
    __mmask##MASK_SIZE k;                                                      \
                                                                               \
  public:                                                                      \
    Mask()                                                                     \
    {                                                                          \
      k = 0;                                                                   \
    }                                                                          \
    SIMD_INLINE Mask(const __mmask##MASK_SIZE &x)                              \
    {                                                                          \
      k = x;                                                                   \
    }                                                                          \
    explicit SIMD_INLINE Mask(const Vec<TYPE, 64> &x)                          \
    {                                                                          \
      k = msb2int(x);                                                          \
    }                                                                          \
    Mask &operator=(const __mmask##MASK_SIZE &x)                               \
    {                                                                          \
      k = x;                                                                   \
      return *this;                                                            \
    }                                                                          \
    SIMD_INLINE operator __mmask##MASK_SIZE() const                            \
    {                                                                          \
      return k;                                                                \
    }                                                                          \
    explicit SIMD_INLINE operator Vec<TYPE, 64>() const                        \
    {                                                                          \
      return int2bits<TYPE, 64>(k);                                            \
    }                                                                          \
    SIMD_INLINE bool operator[](const uint8_t i) const                         \
    {                                                                          \
      return ((1lu << i) & k) != 0;                                            \
    }                                                                          \
    SIMD_INLINE bool operator==(const Mask<TYPE, 64> &x) const                 \
    {                                                                          \
      return k == x.k;                                                         \
    }                                                                          \
  };

#ifdef __AVX512BW__
CLASS_MASK(Byte, 64)
CLASS_MASK(SignedByte, 64)
CLASS_MASK(Word, 32)
CLASS_MASK(Short, 32)
#endif
CLASS_MASK(Int, 16)
CLASS_MASK(Float, 16)
CLASS_MASK(Long, 8)
CLASS_MASK(Double, 8)

namespace internal {
namespace mask {
#define MASK_SOP(OP, TYPE, SUF)                                                \
  static SIMD_INLINE Vec<TYPE, 64> mask_##OP(                                  \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a) \
  {                                                                            \
    return _mm512_mask_##OP##_##SUF(src, k, a);                                \
  }

#define MASKZ_SOP(OP, TYPE, SUF)                                               \
  static SIMD_INLINE Vec<TYPE, 64> maskz_##OP(const Mask<TYPE, 64> &k,         \
                                              const Vec<TYPE, 64> &a)          \
  {                                                                            \
    return _mm512_maskz_##OP##_##SUF(k, a);                                    \
  }

// For operations with one argument. OP is the name of the operation (e.g. add,
// sub, mul), TYPE is the typename (e.g. Word, Float), and SUF is the
// suffix of the intrinsic (e.g. epi8, epi16, ps).
#define GENERATE_SOP(OP, TYPE, SUF)                                            \
  MASK_SOP(OP, TYPE, SUF) MASKZ_SOP(OP, TYPE, SUF)

#define MASK_DOP(OP, TYPE, SUF)                                                \
  static SIMD_INLINE Vec<TYPE, 64> mask_##OP(                                  \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a, \
    const Vec<TYPE, 64> &b)                                                    \
  {                                                                            \
    return _mm512_mask_##OP##_##SUF(src, k, a, b);                             \
  }

#define MASKZ_DOP(OP, TYPE, SUF)                                               \
  static SIMD_INLINE Vec<TYPE, 64> maskz_##OP(                                 \
    const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a, const Vec<TYPE, 64> &b)   \
  {                                                                            \
    return _mm512_maskz_##OP##_##SUF(k, a, b);                                 \
  }

// For operations with two arguments. OP is the name of the operation (e.g. add,
// sub, mul), TYPE is the typename (e.g. Word, Float), and SUF is the
// suffix of the intrinsic (e.g. epi8, epi16, ps).
#define GENERATE_DOP(OP, TYPE, SUF)                                            \
  MASK_DOP(OP, TYPE, SUF) MASKZ_DOP(OP, TYPE, SUF)

// ---------------------------------------------------------------------------
// mask_ifelse v
// ---------------------------------------------------------------------------

// 29. Mar 23 (Jonas Keller): added explicit cast to __m512(i) register to avoid
// compiler errors (can't convert simd::Vec to __v64qi, etc...)

#define MASK_IFELSE(TYPE, SUF, REG)                                            \
  static SIMD_INLINE Vec<TYPE, 64> mask_ifelse(const Mask<TYPE, 64> &cond,     \
                                               const Vec<TYPE, 64> &a,         \
                                               const Vec<TYPE, 64> &b)         \
  {                                                                            \
    return (REG) _mm512_mask_blend_##SUF(cond, (REG) b, (REG) a);              \
  }

#ifdef __AVX512BW__
MASK_IFELSE(Byte, epi8, __m512i)
MASK_IFELSE(SignedByte, epi8, __m512i)
MASK_IFELSE(Word, epi16, __m512i)
MASK_IFELSE(Short, epi16, __m512i)
#endif
MASK_IFELSE(Int, epi32, __m512i)
MASK_IFELSE(Float, ps, __m512)
MASK_IFELSE(Long, epi64, __m512i)
MASK_IFELSE(Double, pd, __m512d)

// ---------------------------------------------------------------------------
// mask_ifelsezero (mask_ifelsezero(cond, a) is the same as mask_ifelse(cond, a,
// setzero()), but may have faster implementations)
// ---------------------------------------------------------------------------

#define MASK_IFELSEZERO(TYPE)                                                  \
  static SIMD_INLINE Vec<TYPE, 64> mask_ifelsezero(                            \
    const Mask<TYPE, 64> &cond, const Vec<TYPE, 64> &trueVal)                  \
  {                                                                            \
    return mask_ifelse(cond, trueVal, ::simd::setzero<TYPE, 64>());            \
  }

#ifdef __AVX512BW__
MASK_IFELSEZERO(Byte)
MASK_IFELSEZERO(SignedByte)
MASK_IFELSEZERO(Word)
MASK_IFELSEZERO(Short)
#endif
MASK_IFELSEZERO(Int)
MASK_IFELSEZERO(Float)
MASK_IFELSEZERO(Long)
MASK_IFELSEZERO(Double)

// ---------------------------------------------------------------------------
// reinterpret_mask v
// ---------------------------------------------------------------------------

// 06. Feb 23 (Jonas Keller): added reinterpret_mask

template <typename Tout, typename Tin>
static SIMD_INLINE Mask<Tout, 64> reinterpret_mask(const Mask<Tin, 64> &k)
{
  static_assert(sizeof(Tout) == sizeof(Tin), "");
  return Mask<Tout, 64>(k.k);
}

// ---------------------------------------------------------------------------
// masked convert (without changes in the number of of elements) v
// ---------------------------------------------------------------------------

// conversion with saturation; we wanted to have a fast solution that
// doesn't trigger the overflow which results in a negative two's
// complement result ("invalid int32": 0x80000000); therefore we clamp
// the positive values at the maximal positive float which is
// convertible to int32 without overflow (0x7fffffbf = 2147483520);
// negative values cannot overflow (they are clamped to invalid int
// which is the most negative int32)
SIMD_INLINE Vec<Int, 64> maskz_cvts(const Mask<Float, 64> &k,
                                    const Vec<Float, 64> &a)
{
  __m512 clip = _mm512_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT32);
  return _mm512_maskz_cvtps_epi32(k, _mm512_maskz_min_ps(k, clip, a));
}

SIMD_INLINE Vec<Int, 64> mask_cvts(const Vec<Int, 64> &src,
                                   const Mask<Float, 64> &k,
                                   const Vec<Float, 64> &a)
{
  __m512 clip = _mm512_set1_ps(MAX_POS_FLOAT_CONVERTIBLE_TO_INT32);
  return _mm512_mask_cvtps_epi32(src, k, _mm512_maskz_min_ps(k, clip, a));
}

// saturation is not necessary in this case
SIMD_INLINE Vec<Float, 64> maskz_cvts(const Mask<Int, 64> &k,
                                      const Vec<Int, 64> &a)
{
  return _mm512_maskz_cvtepi32_ps(k, a);
}

// saturation is not necessary in this case
SIMD_INLINE Vec<Float, 64> mask_cvts(const Vec<Float, 64> &src,
                                     const Mask<Int, 64> &k,
                                     const Vec<Int, 64> &a)
{
  return _mm512_mask_cvtepi32_ps(src, k, a);
}

// ---------------------------------------------------------------------------
// mask_set1 v
// ---------------------------------------------------------------------------

#define GENERATE_SET1(TYPE, SUF)                                               \
  static SIMD_INLINE Vec<TYPE, 64> mask_set1(                                  \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const TYPE a)           \
  {                                                                            \
    return _mm512_mask_set1_##SUF(src, k, a);                                  \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 64> maskz_set1(const Mask<TYPE, 64> &k,         \
                                              const TYPE a)                    \
  {                                                                            \
    return _mm512_maskz_set1_##SUF(k, a);                                      \
  }

#ifdef __AVX512BW__
GENERATE_SET1(Byte, epi8)
GENERATE_SET1(SignedByte, epi8)
GENERATE_SET1(Word, epi16)
GENERATE_SET1(Short, epi16)
#endif
GENERATE_SET1(Int, epi32)
GENERATE_SET1(Long, epi64)
// Workaround for Float, because there is no mask_set1_ps
static SIMD_INLINE Vec<Float, 64> mask_set1(const Vec<Float, 64> &src,
                                            const Mask<Float, 64> &k,
                                            const Float a)
{
  return _mm512_castsi512_ps(
    _mm512_mask_set1_epi32(_mm512_castps_si512(src), k, bit_cast<Int>(a)));
}
static SIMD_INLINE Vec<Float, 64> maskz_set1(const Mask<Float, 64> &k,
                                             const Float a)
{
  return _mm512_castsi512_ps(_mm512_maskz_set1_epi32(k, bit_cast<Int>(a)));
}
// Workaround for Double, because there is no mask_set1_pd
static SIMD_INLINE Vec<Double, 64> mask_set1(const Vec<Double, 64> &src,
                                             const Mask<Double, 64> &k,
                                             const Double a)
{
  return _mm512_castsi512_pd(
    _mm512_mask_set1_epi64(_mm512_castpd_si512(src), k, bit_cast<Long>(a)));
}
static SIMD_INLINE Vec<Double, 64> maskz_set1(const Mask<Double, 64> &k,
                                              const Double a)
{
  return _mm512_castsi512_pd(_mm512_maskz_set1_epi64(k, bit_cast<Long>(a)));
}

// ---------------------------------------------------------------------------
// mask_load v
// ---------------------------------------------------------------------------

#define GENERATE_LOAD(NAME, TYPE, SUF)                                         \
  static SIMD_INLINE Vec<TYPE, 64> mask_load(                                  \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const TYPE *const p)    \
  {                                                                            \
    /* AVX load and store instructions need alignment to 64 byte*/             \
    /* (lower 6 bit need to be zero) */                                        \
    SIMD_CHECK_ALIGNMENT(p, 64);                                               \
    return _mm512_mask_##NAME##_##SUF(src, k, p);                              \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 64> maskz_load(const Mask<TYPE, 64> &k,         \
                                              const TYPE *const p)             \
  {                                                                            \
    /* AVX load and store instructions need alignment to 64 byte*/             \
    /* (lower 6 bit need to be zero) */                                        \
    SIMD_CHECK_ALIGNMENT(p, 64);                                               \
    return _mm512_maskz_##NAME##_##SUF(k, p);                                  \
  }

#ifdef __AVX512BW__
// there is no aligned load for 8 and 16 bit types, so we use loadu
GENERATE_LOAD(loadu, Byte, epi8)
GENERATE_LOAD(loadu, SignedByte, epi8)
GENERATE_LOAD(loadu, Word, epi16)
GENERATE_LOAD(loadu, Short, epi16)
#endif

GENERATE_LOAD(load, Int, epi32)
GENERATE_LOAD(load, Float, ps)
GENERATE_LOAD(load, Long, epi64)
GENERATE_LOAD(load, Double, pd)

// ---------------------------------------------------------------------------
// mask_loadu v
// ---------------------------------------------------------------------------

#define GENERATE_LOADU(TYPE, SUF)                                              \
  static SIMD_INLINE Vec<TYPE, 64> mask_loadu(                                 \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const TYPE *const p)    \
  {                                                                            \
    return _mm512_mask_loadu_##SUF(src, k, p);                                 \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 64> maskz_loadu(const Mask<TYPE, 64> &k,        \
                                               const TYPE *const p)            \
  {                                                                            \
    return _mm512_maskz_loadu_##SUF(k, p);                                     \
  }

#ifdef __AVX512BW__
GENERATE_LOADU(Byte, epi8)
GENERATE_LOADU(SignedByte, epi8)
GENERATE_LOADU(Word, epi16)
GENERATE_LOADU(Short, epi16)
#endif

GENERATE_LOADU(Int, epi32)
GENERATE_LOADU(Float, ps)
GENERATE_LOADU(Long, epi64)
GENERATE_LOADU(Double, pd)

// ---------------------------------------------------------------------------
// mask_store v
// ---------------------------------------------------------------------------

// There are no *_maskz_store_* intrinsics, only *_mask_store_* intrinsics

#define MASK_STORE(NAME, TYPE, SUF)                                            \
  static SIMD_INLINE void mask_store(TYPE *const p, const Mask<TYPE, 64> &k,   \
                                     const Vec<TYPE, 64> &a)                   \
  {                                                                            \
    /* AVX load and store instructions need alignment to 64 byte*/             \
    /* (lower 6 bit need to be zero) */                                        \
    SIMD_CHECK_ALIGNMENT(p, 64);                                               \
    return _mm512_mask_##NAME##_##SUF(p, k, a);                                \
  }

#ifdef __AVX512BW__
// there is no aligned store for 8 and 16 bit types, so we use storeu
MASK_STORE(storeu, Byte, epi8)
MASK_STORE(storeu, SignedByte, epi8)
MASK_STORE(storeu, Word, epi16)
MASK_STORE(storeu, Short, epi16)
#endif

MASK_STORE(store, Int, epi32)
MASK_STORE(store, Float, ps)
MASK_STORE(store, Long, epi64)
MASK_STORE(store, Double, pd)

// ---------------------------------------------------------------------------
// mask_storeu v
// ---------------------------------------------------------------------------

// There are no *_maskz_storeu_* intrinsics, only *_mask_storeu_* intrinsics

#define MASK_STOREU(TYPE, SUF)                                                 \
  static SIMD_INLINE void mask_storeu(TYPE *const p, const Mask<TYPE, 64> &k,  \
                                      const Vec<TYPE, 64> &a)                  \
  {                                                                            \
    return _mm512_mask_storeu_##SUF(p, k, a);                                  \
  }
#ifdef __AVX512BW__
MASK_STOREU(Byte, epi8)
MASK_STOREU(SignedByte, epi8)
MASK_STOREU(Word, epi16)
MASK_STOREU(Short, epi16)
#endif
MASK_STOREU(Int, epi32)
MASK_STOREU(Float, ps)
MASK_STOREU(Long, epi64)
MASK_STOREU(Double, pd)

// ---------------------------------------------------------------------------
// mask_add v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(add, Byte, epi8)
GENERATE_DOP(add, SignedByte, epi8)
GENERATE_DOP(add, Word, epi16)
GENERATE_DOP(add, Short, epi16)
#endif
GENERATE_DOP(add, Int, epi32)
GENERATE_DOP(add, Float, ps)
GENERATE_DOP(add, Long, epi64)
GENERATE_DOP(add, Double, pd)

// ---------------------------------------------------------------------------
// mask_adds v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(adds, Byte, epu8)
GENERATE_DOP(adds, SignedByte, epi8)
GENERATE_DOP(adds, Word, epu16)
GENERATE_DOP(adds, Short, epi16)
#endif

// 09. Mar 23 (Jonas Keller): removed non saturating version of adds for Int and
// Float, use the emulated versions in SIMDVecMaskImplEmu.H instead

// ---------------------------------------------------------------------------
// mask_sub v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(sub, Byte, epi8)
GENERATE_DOP(sub, SignedByte, epi8)
GENERATE_DOP(sub, Word, epi16)
GENERATE_DOP(sub, Short, epi16)
#endif
GENERATE_DOP(sub, Int, epi32)
GENERATE_DOP(sub, Float, ps)
GENERATE_DOP(sub, Long, epi64)
GENERATE_DOP(sub, Double, pd)

// ---------------------------------------------------------------------------
// mask_subs v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(subs, Byte, epu8)
GENERATE_DOP(subs, SignedByte, epi8)
GENERATE_DOP(subs, Word, epu16)
GENERATE_DOP(subs, Short, epi16)
#endif

// 09. Mar 23 (Jonas Keller): removed non saturating version of subs for Int and
// Float, use the emulated versions in SIMDVecMaskImplEmu.H instead

// ---------------------------------------------------------------------------
// mask_mul v
// ---------------------------------------------------------------------------

GENERATE_DOP(mul, Float, ps)
GENERATE_DOP(mul, Double, pd)

// ---------------------------------------------------------------------------
// mask_div v
// ---------------------------------------------------------------------------

GENERATE_DOP(div, Float, ps)
GENERATE_DOP(div, Double, pd)

// ---------------------------------------------------------------------------
// masked ceil, floor, round, truncate v
// ---------------------------------------------------------------------------

// 10. Apr 23 (Jonas Keller): added versions for integer types

// versions for integer types do nothing:

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_ceil(const Vec<T, 64> &src,
                                        const Mask<T, 64> &k,
                                        const Vec<T, 64> &a)
{
  return mask_ifelse(k, a, src);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_ceil(const Mask<T, 64> &k,
                                         const Vec<T, 64> &a)
{
  return mask_ifelsezero(k, a);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_floor(const Vec<T, 64> &src,
                                         const Mask<T, 64> &k,
                                         const Vec<T, 64> &a)
{
  return mask_ifelse(k, a, src);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_floor(const Mask<T, 64> &k,
                                          const Vec<T, 64> &a)
{
  return mask_ifelsezero(k, a);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_round(const Vec<T, 64> &src,
                                         const Mask<T, 64> &k,
                                         const Vec<T, 64> &a)
{
  return mask_ifelse(k, a, src);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_round(const Mask<T, 64> &k,
                                          const Vec<T, 64> &a)
{
  return mask_ifelsezero(k, a);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_truncate(const Vec<T, 64> &src,
                                            const Mask<T, 64> &k,
                                            const Vec<T, 64> &a)
{
  return mask_ifelse(k, a, src);
}

template <typename T, SIMD_ENABLE_IF(std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_truncate(const Mask<T, 64> &k,
                                             const Vec<T, 64> &a)
{
  return mask_ifelsezero(k, a);
}

// Float versions:

static SIMD_INLINE Vec<Float, 64> mask_ceil(const Vec<Float, 64> &src,
                                            const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return _mm512_mask_roundscale_ps(src, k, a,
                                   _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> maskz_ceil(const Mask<Float, 64> &k,
                                             const Vec<Float, 64> &a)
{
  return _mm512_maskz_roundscale_ps(k, a,
                                    _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> mask_floor(const Vec<Float, 64> &src,
                                             const Mask<Float, 64> &k,
                                             const Vec<Float, 64> &a)
{
  return _mm512_mask_roundscale_ps(src, k, a,
                                   _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> maskz_floor(const Mask<Float, 64> &k,
                                              const Vec<Float, 64> &a)
{
  return _mm512_maskz_roundscale_ps(k, a,
                                    _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> mask_round(const Vec<Float, 64> &src,
                                             const Mask<Float, 64> &k,
                                             const Vec<Float, 64> &a)
{
  return _mm512_mask_roundscale_ps(
    src, k, a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> maskz_round(const Mask<Float, 64> &k,
                                              const Vec<Float, 64> &a)
{
  return _mm512_maskz_roundscale_ps(
    k, a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> mask_truncate(const Vec<Float, 64> &src,
                                                const Mask<Float, 64> &k,
                                                const Vec<Float, 64> &a)
{
  return _mm512_mask_roundscale_ps(src, k, a,
                                   _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Float, 64> maskz_truncate(const Mask<Float, 64> &k,
                                                 const Vec<Float, 64> &a)
{
  return _mm512_maskz_roundscale_ps(k, a,
                                    _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// Double versions:

static SIMD_INLINE Vec<Double, 64> mask_ceil(const Vec<Double, 64> &src,
                                             const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a)
{
  return _mm512_mask_roundscale_pd(src, k, a,
                                   _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> maskz_ceil(const Mask<Double, 64> &k,
                                              const Vec<Double, 64> &a)
{
  return _mm512_maskz_roundscale_pd(k, a,
                                    _MM_FROUND_TO_POS_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> mask_floor(const Vec<Double, 64> &src,
                                              const Mask<Double, 64> &k,
                                              const Vec<Double, 64> &a)
{
  return _mm512_mask_roundscale_pd(src, k, a,
                                   _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> maskz_floor(const Mask<Double, 64> &k,
                                               const Vec<Double, 64> &a)
{
  return _mm512_maskz_roundscale_pd(k, a,
                                    _MM_FROUND_TO_NEG_INF | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> mask_round(const Vec<Double, 64> &src,
                                              const Mask<Double, 64> &k,
                                              const Vec<Double, 64> &a)
{
  return _mm512_mask_roundscale_pd(
    src, k, a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> maskz_round(const Mask<Double, 64> &k,
                                               const Vec<Double, 64> &a)
{
  return _mm512_maskz_roundscale_pd(
    k, a, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> mask_truncate(const Vec<Double, 64> &src,
                                                 const Mask<Double, 64> &k,
                                                 const Vec<Double, 64> &a)
{
  return _mm512_mask_roundscale_pd(src, k, a,
                                   _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

static SIMD_INLINE Vec<Double, 64> maskz_truncate(const Mask<Double, 64> &k,
                                                  const Vec<Double, 64> &a)
{
  return _mm512_maskz_roundscale_pd(k, a,
                                    _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// ---------------------------------------------------------------------------
// masked elementary mathematical functions v
// ---------------------------------------------------------------------------

// masked estimate of a reciprocal
// NOTE: this has better precision than SSE and AVX versions!
static SIMD_INLINE Vec<Float, 64> mask_rcp(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a)
{
  return _mm512_mask_rcp14_ps(src, k, a);
}

static SIMD_INLINE Vec<Float, 64> maskz_rcp(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return _mm512_maskz_rcp14_ps(k, a);
}

static SIMD_INLINE Vec<Double, 64> mask_rcp(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a)
{
  return _mm512_mask_rcp14_pd(src, k, a);
}

static SIMD_INLINE Vec<Double, 64> maskz_rcp(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a)
{
  return _mm512_maskz_rcp14_pd(k, a);
}

// masked estimate of reverse square root
// NOTE: this has better precision than SSE and AVX versions!
static SIMD_INLINE Vec<Float, 64> mask_rsqrt(const Vec<Float, 64> &src,
                                             const Mask<Float, 64> &k,
                                             const Vec<Float, 64> &a)
{
  return _mm512_mask_rsqrt14_ps(src, k, a);
}

static SIMD_INLINE Vec<Float, 64> maskz_rsqrt(const Mask<Float, 64> &k,
                                              const Vec<Float, 64> &a)
{
  return _mm512_maskz_rsqrt14_ps(k, a);
}

static SIMD_INLINE Vec<Double, 64> mask_rsqrt(const Vec<Double, 64> &src,
                                              const Mask<Double, 64> &k,
                                              const Vec<Double, 64> &a)
{
  return _mm512_mask_rsqrt14_pd(src, k, a);
}

static SIMD_INLINE Vec<Double, 64> maskz_rsqrt(const Mask<Double, 64> &k,
                                               const Vec<Double, 64> &a)
{
  return _mm512_maskz_rsqrt14_pd(k, a);
}

// masked square root
GENERATE_SOP(sqrt, Float, ps)
GENERATE_SOP(sqrt, Double, pd)

// ---------------------------------------------------------------------------
// masked_abs v
// ---------------------------------------------------------------------------

// 25. Mar 25 (Jonas Keller): added masked abs for unsigned integers

// unsigned integers: do nothing
template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_abs(const Vec<T, 64> &src,
                                       const Mask<T, 64> &k,
                                       const Vec<T, 64> &a)
{
  return mask_ifelse(k, a, src);
}

template <typename T, SIMD_ENABLE_IF(std::is_unsigned<T>::value
                                       &&std::is_integral<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_abs(const Mask<T, 64> &k,
                                        const Vec<T, 64> &a)
{
  return mask_ifelsezero(k, a);
}

#ifdef __AVX512BW__
GENERATE_SOP(abs, SignedByte, epi8)
GENERATE_SOP(abs, Short, epi16)
#endif
GENERATE_SOP(abs, Int, epi32)
GENERATE_SOP(abs, Long, epi64)

MASK_SOP(abs, Float, ps)

// There is no _mm512_maskz_abs_ps
static SIMD_INLINE Vec<Float, 64> maskz_abs(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return _mm512_mask_abs_ps(::simd::setzero<Float, 64>(), k, a);
}

MASK_SOP(abs, Double, pd)

// There is no _mm512_maskz_abs_pd
static SIMD_INLINE Vec<Double, 64> maskz_abs(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a)
{
  return _mm512_mask_abs_pd(::simd::setzero<Double, 64>(), k, a);
}

// ---------------------------------------------------------------------------
// mask_and v
// ---------------------------------------------------------------------------

// there is no _mm512_mask_and_epi8 or _mm512_mask_and_epi16
GENERATE_DOP(and, Int, epi32)
GENERATE_DOP(and, Long, epi64)
#ifdef __AVX512DQ__
GENERATE_DOP(and, Float, ps)
GENERATE_DOP(and, Double, pd)
#else
// Workaround with the epi32/64-versions and casts
static SIMD_INLINE Vec<Float, 64> mask_and(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(_mm512_mask_and_epi32(_mm512_castps_si512(src), k,
                                                   _mm512_castps_si512(a),
                                                   _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Float, 64> maskz_and(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a,
                                            const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(
    _mm512_maskz_and_epi32(k, _mm512_castps_si512(a), _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> mask_and(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(_mm512_mask_and_epi64(_mm512_castpd_si512(src), k,
                                                   _mm512_castpd_si512(a),
                                                   _mm512_castpd_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> maskz_and(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a,
                                             const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_and_epi64(k, _mm512_castpd_si512(a), _mm512_castpd_si512(b)));
}
#endif

// ---------------------------------------------------------------------------
// mask_or v
// ---------------------------------------------------------------------------

// there is no _mm512_mask_or_epi8 or _mm512_mask_or_epi16
GENERATE_DOP(or, Int, epi32)
GENERATE_DOP(or, Long, epi64)
#ifdef __AVX512DQ__
GENERATE_DOP(or, Float, ps)
GENERATE_DOP(or, Double, pd)
#else
// Workaround with the epi32/64-versions and casts
static SIMD_INLINE Vec<Float, 64> mask_or(const Vec<Float, 64> &src,
                                          const Mask<Float, 64> &k,
                                          const Vec<Float, 64> &a,
                                          const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(_mm512_mask_or_epi32(_mm512_castps_si512(src), k,
                                                  _mm512_castps_si512(a),
                                                  _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Float, 64> maskz_or(const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(
    _mm512_maskz_or_epi32(k, _mm512_castps_si512(a), _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> mask_or(const Vec<Double, 64> &src,
                                           const Mask<Double, 64> &k,
                                           const Vec<Double, 64> &a,
                                           const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(_mm512_mask_or_epi64(_mm512_castpd_si512(src), k,
                                                  _mm512_castpd_si512(a),
                                                  _mm512_castpd_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> maskz_or(const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_or_epi64(k, _mm512_castpd_si512(a), _mm512_castpd_si512(b)));
}
#endif

// ---------------------------------------------------------------------------
// mask_andnot v
// ---------------------------------------------------------------------------

// there is no _mm512_mask_andnot_epi8 or _mm512_mask_andnot_epi16
GENERATE_DOP(andnot, Int, epi32)
GENERATE_DOP(andnot, Long, epi64)
#ifdef __AVX512DQ__
GENERATE_DOP(andnot, Float, ps)
GENERATE_DOP(andnot, Double, pd)
#else
// Workaround with the epi32/64-versions and casts
static SIMD_INLINE Vec<Float, 64> mask_andnot(const Vec<Float, 64> &src,
                                              const Mask<Float, 64> &k,
                                              const Vec<Float, 64> &a,
                                              const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(_mm512_mask_andnot_epi32(_mm512_castps_si512(src),
                                                      k, _mm512_castps_si512(a),
                                                      _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Float, 64> maskz_andnot(const Mask<Float, 64> &k,
                                               const Vec<Float, 64> &a,
                                               const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(_mm512_maskz_andnot_epi32(
    k, _mm512_castps_si512(a), _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> mask_andnot(const Vec<Double, 64> &src,
                                               const Mask<Double, 64> &k,
                                               const Vec<Double, 64> &a,
                                               const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(_mm512_mask_andnot_epi64(_mm512_castpd_si512(src),
                                                      k, _mm512_castpd_si512(a),
                                                      _mm512_castpd_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> maskz_andnot(const Mask<Double, 64> &k,
                                                const Vec<Double, 64> &a,
                                                const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(_mm512_maskz_andnot_epi64(
    k, _mm512_castpd_si512(a), _mm512_castpd_si512(b)));
}
#endif

// ---------------------------------------------------------------------------
// mask_xor v
// ---------------------------------------------------------------------------

// there is no _mm512_mask_xor_epi8 or _mm512_mask_xor_epi16
GENERATE_DOP(xor, Int, epi32)
GENERATE_DOP(xor, Long, epi64)
#ifdef __AVX512DQ__
GENERATE_DOP(xor, Float, ps)
GENERATE_DOP(xor, Double, pd)
#else
// Workaround with the epi32/64-versions and casts
static SIMD_INLINE Vec<Float, 64> mask_xor(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(_mm512_mask_xor_epi32(_mm512_castps_si512(src), k,
                                                   _mm512_castps_si512(a),
                                                   _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Float, 64> maskz_xor(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a,
                                            const Vec<Float, 64> &b)
{
  return _mm512_castsi512_ps(
    _mm512_maskz_xor_epi32(k, _mm512_castps_si512(a), _mm512_castps_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> mask_xor(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(_mm512_mask_xor_epi64(_mm512_castpd_si512(src), k,
                                                   _mm512_castpd_si512(a),
                                                   _mm512_castpd_si512(b)));
}

static SIMD_INLINE Vec<Double, 64> maskz_xor(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a,
                                             const Vec<Double, 64> &b)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_xor_epi64(k, _mm512_castpd_si512(a), _mm512_castpd_si512(b)));
}
#endif

// ---------------------------------------------------------------------------
// mask_not v
// ---------------------------------------------------------------------------

// 08. Apr 23 (Jonas Keller): added mask_not and maskz_not

// There is no masked "not"-intrinsic, so use the masked xor with all ones

// there are no masked xor intrinsics for epi8 and epi16

// Int
static SIMD_INLINE Vec<Int, 64> mask_not(const Vec<Int, 64> &src,
                                         const Mask<Int, 64> &k,
                                         const Vec<Int, 64> &a)
{
  return _mm512_mask_xor_epi32(src, k, a, _mm512_set1_epi32(-1));
}
static SIMD_INLINE Vec<Int, 64> maskz_not(const Mask<Int, 64> &k,
                                          const Vec<Int, 64> &a)
{
  return _mm512_maskz_xor_epi32(k, a, _mm512_set1_epi32(-1));
}

// Long
static SIMD_INLINE Vec<Long, 64> mask_not(const Vec<Long, 64> &src,
                                          const Mask<Long, 64> &k,
                                          const Vec<Long, 64> &a)
{
  return _mm512_mask_xor_epi64(src, k, a, _mm512_set1_epi64(-1));
}
static SIMD_INLINE Vec<Long, 64> maskz_not(const Mask<Long, 64> &k,
                                           const Vec<Long, 64> &a)
{
  return _mm512_maskz_xor_epi64(k, a, _mm512_set1_epi64(-1));
}
#ifdef __AVX512DQ__
// Float
static SIMD_INLINE Vec<Float, 64> mask_not(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a)
{
  return _mm512_mask_xor_ps(src, k, a,
                            _mm512_castsi512_ps(_mm512_set1_epi32(-1)));
}
static SIMD_INLINE Vec<Float, 64> maskz_not(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return _mm512_maskz_xor_ps(k, a, _mm512_castsi512_ps(_mm512_set1_epi32(-1)));
}

// Double
static SIMD_INLINE Vec<Double, 64> mask_not(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a)
{
  return _mm512_mask_xor_pd(src, k, a,
                            _mm512_castsi512_pd(_mm512_set1_epi64(-1)));
}
static SIMD_INLINE Vec<Double, 64> maskz_not(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a)
{
  return _mm512_maskz_xor_pd(k, a, _mm512_castsi512_pd(_mm512_set1_epi64(-1)));
}
#else
// Workaround with the epi32/64-versions and casts
static SIMD_INLINE Vec<Float, 64> mask_not(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a)
{
  return _mm512_castsi512_ps(_mm512_mask_xor_epi32(_mm512_castps_si512(src), k,
                                                   _mm512_castps_si512(a),
                                                   _mm512_set1_epi32(-1)));
}

static SIMD_INLINE Vec<Float, 64> maskz_not(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return _mm512_castsi512_ps(
    _mm512_maskz_xor_epi32(k, _mm512_castps_si512(a), _mm512_set1_epi32(-1)));
}

static SIMD_INLINE Vec<Double, 64> mask_not(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a)
{
  return _mm512_castsi512_pd(_mm512_mask_xor_epi64(_mm512_castpd_si512(src), k,
                                                   _mm512_castpd_si512(a),
                                                   _mm512_set1_epi64(-1)));
}

static SIMD_INLINE Vec<Double, 64> maskz_not(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a)
{
  return _mm512_castsi512_pd(
    _mm512_maskz_xor_epi64(k, _mm512_castpd_si512(a), _mm512_set1_epi64(-1)));
}
#endif

// ---------------------------------------------------------------------------
// mask_neg (negate = two's complement or unary minus), only signed types v
// ---------------------------------------------------------------------------

#define GENERATE_NEG(TYPE, SUF)                                                \
  static SIMD_INLINE Vec<TYPE, 64> mask_neg(                                   \
    const Vec<TYPE, 64> &src, const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a) \
  {                                                                            \
    return _mm512_mask_sub_##SUF(src, k, setzero<TYPE, 64>(), a);              \
  }                                                                            \
  static SIMD_INLINE Vec<TYPE, 64> maskz_neg(const Mask<TYPE, 64> &k,          \
                                             const Vec<TYPE, 64> &a)           \
  {                                                                            \
    return _mm512_maskz_sub_##SUF(k, setzero<TYPE, 64>(), a);                  \
  }

#ifdef __AVX512BW__
GENERATE_NEG(SignedByte, epi8)
GENERATE_NEG(Short, epi16)
#endif
GENERATE_NEG(Int, epi32)
GENERATE_NEG(Float, ps)
GENERATE_NEG(Long, epi64)
GENERATE_NEG(Double, pd)

// ---------------------------------------------------------------------------
// mask_min v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(min, Byte, epu8)
GENERATE_DOP(min, SignedByte, epi8)
GENERATE_DOP(min, Word, epu16)
GENERATE_DOP(min, Short, epi16)
#endif
GENERATE_DOP(min, Int, epi32)
GENERATE_DOP(min, Float, ps)
GENERATE_DOP(min, Long, epi64)
GENERATE_DOP(min, Double, pd)

// ---------------------------------------------------------------------------
// mask_max v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_DOP(max, Byte, epu8)
GENERATE_DOP(max, SignedByte, epi8)
GENERATE_DOP(max, Word, epu16)
GENERATE_DOP(max, Short, epi16)
#endif
GENERATE_DOP(max, Int, epi32)
GENERATE_DOP(max, Float, ps)
GENERATE_DOP(max, Long, epi64)
GENERATE_DOP(max, Double, pd)

// ---------------------------------------------------------------------------
// masked srai (16,32,64 only) v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> mask_srai(const Vec<Word, 64> &src,
                                           const Mask<Word, 64> &k,
                                           const Vec<Word, 64> &a)
{
  return _mm512_mask_srai_epi16(src, k, a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> maskz_srai(const Mask<Word, 64> &k,
                                            const Vec<Word, 64> &a)
{
  return _mm512_maskz_srai_epi16(k, a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> mask_srai(const Vec<Short, 64> &src,
                                            const Mask<Short, 64> &k,
                                            const Vec<Short, 64> &a)
{
  return _mm512_mask_srai_epi16(src, k, a, vec::min(COUNT, 15ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> maskz_srai(const Mask<Short, 64> &k,
                                             const Vec<Short, 64> &a)
{
  return _mm512_maskz_srai_epi16(k, a, vec::min(COUNT, 15ul));
}

#endif

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> mask_srai(const Vec<Int, 64> &src,
                                          const Mask<Int, 64> &k,
                                          const Vec<Int, 64> &a)
{
  return _mm512_mask_srai_epi32(src, k, a, vec::min(COUNT, 31ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> maskz_srai(const Mask<Int, 64> &k,
                                           const Vec<Int, 64> &a)
{
  return _mm512_maskz_srai_epi32(k, a, vec::min(COUNT, 31ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> mask_srai(const Vec<Long, 64> &src,
                                           const Mask<Long, 64> &k,
                                           const Vec<Long, 64> &a)
{
  return _mm512_mask_srai_epi64(src, k, a, vec::min(COUNT, 63ul));
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> maskz_srai(const Mask<Long, 64> &k,
                                            const Vec<Long, 64> &a)
{
  return _mm512_maskz_srai_epi64(k, a, vec::min(COUNT, 63ul));
}

// ---------------------------------------------------------------------------
// masked srli v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> mask_srli(const Vec<Word, 64> &src,
                                           const Mask<Word, 64> &k,
                                           const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_mask_srli_epi16(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi16(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> maskz_srli(const Mask<Word, 64> &k,
                                            const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_maskz_srli_epi16(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> mask_srli(const Vec<Short, 64> &src,
                                            const Mask<Short, 64> &k,
                                            const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_mask_srli_epi16(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi16(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> maskz_srli(const Mask<Short, 64> &k,
                                             const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_maskz_srli_epi16(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

#endif

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> mask_srli(const Vec<Int, 64> &src,
                                          const Mask<Int, 64> &k,
                                          const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_mask_srli_epi32(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi32(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> maskz_srli(const Mask<Int, 64> &k,
                                           const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_maskz_srli_epi32(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> mask_srli(const Vec<Long, 64> &src,
                                           const Mask<Long, 64> &k,
                                           const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_mask_srli_epi64(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi64(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> maskz_srli(const Mask<Long, 64> &k,
                                            const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_maskz_srli_epi64(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

// ---------------------------------------------------------------------------
// masked slli v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> mask_slli(const Vec<Word, 64> &src,
                                           const Mask<Word, 64> &k,
                                           const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_mask_slli_epi16(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi16(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Word, 64> maskz_slli(const Mask<Word, 64> &k,
                                            const Vec<Word, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_maskz_slli_epi16(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> mask_slli(const Vec<Short, 64> &src,
                                            const Mask<Short, 64> &k,
                                            const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_mask_slli_epi16(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi16(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Short, 64> maskz_slli(const Mask<Short, 64> &k,
                                             const Vec<Short, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 16) {
    return _mm512_maskz_slli_epi16(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

#endif

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> mask_slli(const Vec<Int, 64> &src,
                                          const Mask<Int, 64> &k,
                                          const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_mask_slli_epi32(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi32(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Int, 64> maskz_slli(const Mask<Int, 64> &k,
                                           const Vec<Int, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 32) {
    return _mm512_maskz_slli_epi32(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> mask_slli(const Vec<Long, 64> &src,
                                           const Mask<Long, 64> &k,
                                           const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_mask_slli_epi64(src, k, a, COUNT);
  } else {
    return _mm512_mask_blend_epi64(k, src, _mm512_setzero_si512());
  }
}

template <size_t COUNT>
static SIMD_INLINE Vec<Long, 64> maskz_slli(const Mask<Long, 64> &k,
                                            const Vec<Long, 64> &a)
{
  SIMD_IF_CONSTEXPR (COUNT < 64) {
    return _mm512_maskz_slli_epi64(k, a, COUNT);
  } else {
    return _mm512_setzero_si512();
  }
}

// 05. Aug 22 (Jonas Keller):
// Improved implementation of masked hadd, hadds, hsub and hsubs,
// implementation uses masked add/adds/sub/subs directly now instead of
// wrapping hadd, hadds, hsub and hsubs with a mask_ifelse(zero).
// Byte and SignedByte are now supported as well.

// ---------------------------------------------------------------------------
// masked hadd v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> mask_hadd(const Vec<T, 64> &src,
                                        const Mask<T, 64> &k,
                                        const Vec<T, 64> &a,
                                        const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return mask_add(src, k, x, y);
}

template <typename T>
static SIMD_INLINE Vec<T, 64> maskz_hadd(const Mask<T, 64> &k,
                                         const Vec<T, 64> &a,
                                         const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return maskz_add(k, x, y);
}

// ---------------------------------------------------------------------------
// masked hadds v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> mask_hadds(const Vec<T, 64> &src,
                                         const Mask<T, 64> &k,
                                         const Vec<T, 64> &a,
                                         const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return mask_adds(src, k, x, y);
}

template <typename T>
static SIMD_INLINE Vec<T, 64> maskz_hadds(const Mask<T, 64> &k,
                                          const Vec<T, 64> &a,
                                          const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return maskz_adds(k, x, y);
}

// ---------------------------------------------------------------------------
// masked hsub v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> mask_hsub(const Vec<T, 64> &src,
                                        const Mask<T, 64> &k,
                                        const Vec<T, 64> &a,
                                        const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return mask_sub(src, k, x, y);
}

template <typename T>
static SIMD_INLINE Vec<T, 64> maskz_hsub(const Mask<T, 64> &k,
                                         const Vec<T, 64> &a,
                                         const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return maskz_sub(k, x, y);
}

// ---------------------------------------------------------------------------
// masked hsubs v
// ---------------------------------------------------------------------------

template <typename T>
static SIMD_INLINE Vec<T, 64> mask_hsubs(const Vec<T, 64> &src,
                                         const Mask<T, 64> &k,
                                         const Vec<T, 64> &a,
                                         const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return mask_subs(src, k, x, y);
}

template <typename T>
static SIMD_INLINE Vec<T, 64> maskz_hsubs(const Mask<T, 64> &k,
                                          const Vec<T, 64> &a,
                                          const Vec<T, 64> &b)
{
  Vec<T, 64> x, y;
  unzip<1>(a, b, x, y);
  return maskz_subs(k, x, y);
}

// 16. Oct 22 (Jonas Keller): added overloaded versions of mask_cmp* functions
// that only take two vector parameters and no mask parameter

#define GENERATE_CMP(OP, TYPE, SUF)                                            \
  static SIMD_INLINE Mask<TYPE, 64> mask_##OP(                                 \
    const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a, const Vec<TYPE, 64> &b)   \
  {                                                                            \
    return _mm512_mask_##OP##_##SUF##_mask(k, a, b);                           \
  }                                                                            \
  static SIMD_INLINE Mask<TYPE, 64> mask_##OP(const Vec<TYPE, 64> &a,          \
                                              const Vec<TYPE, 64> &b)          \
  {                                                                            \
    return _mm512_##OP##_##SUF##_mask(a, b);                                   \
  }

#define GENERATE_CMP_WITH_GENERALIZED_FCT(OP, TYPE, SUF, IMM8)                 \
  static SIMD_INLINE Mask<TYPE, 64> mask_##OP(                                 \
    const Mask<TYPE, 64> &k, const Vec<TYPE, 64> &a, const Vec<TYPE, 64> &b)   \
  {                                                                            \
    return _mm512_mask_cmp_##SUF##_mask(k, a, b, IMM8);                        \
  }                                                                            \
  static SIMD_INLINE Mask<TYPE, 64> mask_##OP(const Vec<TYPE, 64> &a,          \
                                              const Vec<TYPE, 64> &b)          \
  {                                                                            \
    return _mm512_cmp_##SUF##_mask(a, b, IMM8);                                \
  }

// ---------------------------------------------------------------------------
// masked compare < v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmplt, Byte, epu8)
GENERATE_CMP(cmplt, SignedByte, epi8)
GENERATE_CMP(cmplt, Word, epu16)
GENERATE_CMP(cmplt, Short, epi16)
#endif
GENERATE_CMP(cmplt, Int, epi32)
GENERATE_CMP(cmplt, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmplt, Float, ps, _CMP_LT_OS)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmplt, Double, pd, _CMP_LT_OS)

// ---------------------------------------------------------------------------
// masked compare <= v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmple, Byte, epu8)
GENERATE_CMP(cmple, SignedByte, epi8)
GENERATE_CMP(cmple, Word, epu16)
GENERATE_CMP(cmple, Short, epi16)
#endif
GENERATE_CMP(cmple, Int, epi32)
GENERATE_CMP(cmple, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmple, Float, ps, _CMP_LE_OS)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmple, Double, pd, _CMP_LE_OS)

// ---------------------------------------------------------------------------
// masked compare == v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmpeq, Byte, epu8)
GENERATE_CMP(cmpeq, SignedByte, epi8)
GENERATE_CMP(cmpeq, Word, epu16)
GENERATE_CMP(cmpeq, Short, epi16)
#endif
GENERATE_CMP(cmpeq, Int, epi32)
GENERATE_CMP(cmpeq, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmpeq, Float, ps, _CMP_EQ_OQ)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmpeq, Double, pd, _CMP_EQ_OQ)

// ---------------------------------------------------------------------------
// masked compare > v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmpgt, Byte, epu8)
GENERATE_CMP(cmpgt, SignedByte, epi8)
GENERATE_CMP(cmpgt, Word, epu16)
GENERATE_CMP(cmpgt, Short, epi16)
#endif
GENERATE_CMP(cmpgt, Int, epi32)
GENERATE_CMP(cmpgt, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmpgt, Float, ps, _CMP_GT_OS)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmpgt, Double, pd, _CMP_GT_OS)

// ---------------------------------------------------------------------------
// masked compare >= v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmpge, Byte, epu8)
GENERATE_CMP(cmpge, SignedByte, epi8)
GENERATE_CMP(cmpge, Word, epu16)
GENERATE_CMP(cmpge, Short, epi16)
#endif
GENERATE_CMP(cmpge, Int, epi32)
GENERATE_CMP(cmpge, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmpge, Float, ps, _CMP_GE_OS)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmpge, Double, pd, _CMP_GE_OS)

// ---------------------------------------------------------------------------
// masked compare != v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
GENERATE_CMP(cmpneq, Byte, epu8)
GENERATE_CMP(cmpneq, SignedByte, epi8)
GENERATE_CMP(cmpneq, Word, epu16)
GENERATE_CMP(cmpneq, Short, epi16)
#endif
GENERATE_CMP(cmpneq, Int, epi32)
GENERATE_CMP(cmpneq, Long, epi64)

GENERATE_CMP_WITH_GENERALIZED_FCT(cmpneq, Float, ps, _CMP_NEQ_OQ)
GENERATE_CMP_WITH_GENERALIZED_FCT(cmpneq, Double, pd, _CMP_NEQ_OQ)

// ---------------------------------------------------------------------------
// masked avg: average with rounding down v
// ---------------------------------------------------------------------------

#ifdef __AVX512BW__
static SIMD_INLINE Vec<Byte, 64> mask_avg(const Vec<Byte, 64> &src,
                                          const Mask<Byte, 64> &k,
                                          const Vec<Byte, 64> &a,
                                          const Vec<Byte, 64> &b)
{
  return _mm512_mask_avg_epu8(src, k, a, b);
}

static SIMD_INLINE Vec<Byte, 64> maskz_avg(const Mask<Byte, 64> &k,
                                           const Vec<Byte, 64> &a,
                                           const Vec<Byte, 64> &b)
{
  return _mm512_maskz_avg_epu8(k, a, b);
}

static SIMD_INLINE Vec<Word, 64> mask_avg(const Vec<Word, 64> &src,
                                          const Mask<Word, 64> &k,
                                          const Vec<Word, 64> &a,
                                          const Vec<Word, 64> &b)
{
  return _mm512_mask_avg_epu16(src, k, a, b);
}

static SIMD_INLINE Vec<Word, 64> maskz_avg(const Mask<Word, 64> &k,
                                           const Vec<Word, 64> &a,
                                           const Vec<Word, 64> &b)
{
  return _mm512_maskz_avg_epu16(k, a, b);
}
#endif

// Paul R at
// http://stackoverflow.com/questions/12152640/signed-16-bit-sse-average

template <typename T,
          SIMD_ENABLE_IF(std::is_integral<T>::value &&std::is_signed<T>::value)>
static SIMD_INLINE Vec<T, 64> mask_avg(const Vec<T, 64> &src,
                                       const Mask<T, 64> &k,
                                       const Vec<T, 64> &a, const Vec<T, 64> &b)
{
  const auto one = ::simd::set1<T, 64>(1);
  const auto lsb = bit_and(bit_or(a, b), one);
  const auto as  = srai<1>(a);
  const auto bs  = srai<1>(b);
  return mask_add(src, k, lsb, add(as, bs));
}

template <typename T,
          SIMD_ENABLE_IF(std::is_integral<T>::value &&std::is_signed<T>::value)>
static SIMD_INLINE Vec<T, 64> maskz_avg(const Mask<T, 64> &k,
                                        const Vec<T, 64> &a,
                                        const Vec<T, 64> &b)
{
  const auto one = ::simd::set1<T, 64>(1);
  const auto lsb = bit_and(bit_or(a, b), one);
  const auto as  = srai<1>(a);
  const auto bs  = srai<1>(b);
  return maskz_add(k, lsb, add(as, bs));
}

// NOTE: Float version doesn't round!
static SIMD_INLINE Vec<Float, 64> mask_avg(const Vec<Float, 64> &src,
                                           const Mask<Float, 64> &k,
                                           const Vec<Float, 64> &a,
                                           const Vec<Float, 64> &b)
{
  return _mm512_mask_mul_ps(src, k, _mm512_maskz_add_ps(k, a, b),
                            _mm512_set1_ps(0.5f));
}

// NOTE: Float version doesn't round!
static SIMD_INLINE Vec<Float, 64> maskz_avg(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a,
                                            const Vec<Float, 64> &b)
{
  return _mm512_maskz_mul_ps(k, _mm512_maskz_add_ps(k, a, b),
                             _mm512_set1_ps(0.5f));
}

// NOTE: Double version doesn't round!
static SIMD_INLINE Vec<Double, 64> mask_avg(const Vec<Double, 64> &src,
                                            const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a,
                                            const Vec<Double, 64> &b)
{
  return _mm512_mask_mul_pd(src, k, _mm512_maskz_add_pd(k, a, b),
                            _mm512_set1_pd(0.5));
}

// NOTE: Double version doesn't round!
static SIMD_INLINE Vec<Double, 64> maskz_avg(const Mask<Double, 64> &k,
                                             const Vec<Double, 64> &a,
                                             const Vec<Double, 64> &b)
{
  return _mm512_maskz_mul_pd(k, _mm512_maskz_add_pd(k, a, b),
                             _mm512_set1_pd(0.5));
}

// ---------------------------------------------------------------------------
// masked test_all_zeros v
// ---------------------------------------------------------------------------

#define TEST_ALL_ZEROS(TYPE, SUF)                                              \
  static SIMD_INLINE bool mask_test_all_zeros(const Mask<TYPE, 64> &k,         \
                                              const Vec<TYPE, 64> &a)          \
  {                                                                            \
    return (_mm512_mask_test_epi##SUF##_mask(k, a, a) == 0);                   \
  }

#ifdef __AVX512BW__
TEST_ALL_ZEROS(Byte, 8)
TEST_ALL_ZEROS(SignedByte, 8)
TEST_ALL_ZEROS(Word, 16)
TEST_ALL_ZEROS(Short, 16)
#endif
TEST_ALL_ZEROS(Int, 32)
TEST_ALL_ZEROS(Long, 64)

static SIMD_INLINE bool mask_test_all_zeros(const Mask<Float, 64> &k,
                                            const Vec<Float, 64> &a)
{
  return (_mm512_mask_test_epi32_mask(k, _mm512_castps_si512(a),
                                      _mm512_castps_si512(a)) == 0);
}

static SIMD_INLINE bool mask_test_all_zeros(const Mask<Double, 64> &k,
                                            const Vec<Double, 64> &a)
{
  return (_mm512_mask_test_epi64_mask(k, _mm512_castpd_si512(a),
                                      _mm512_castpd_si512(a)) == 0);
}

// ---------------------------------------------------------------------------
// masked test_all_ones v
// ---------------------------------------------------------------------------

// already defined in SIMDVecMaskImplEmu.H

// ---------------------------------------------------------------------------
// mask_all_ones v
// ---------------------------------------------------------------------------

#define MASK_ALL_ONES(TYPE, MASK)                                              \
  static SIMD_INLINE Mask<TYPE, 64> mask_all_ones(OutputType<TYPE>,            \
                                                  Integer<64>)                 \
  {                                                                            \
    return MASK;                                                               \
  }

#ifdef __AVX512BW__
MASK_ALL_ONES(Byte, 0xFFFFFFFFFFFFFFFF)
MASK_ALL_ONES(SignedByte, 0xFFFFFFFFFFFFFFFF)
MASK_ALL_ONES(Word, 0xFFFFFFFF)
MASK_ALL_ONES(Short, 0xFFFFFFFF)
#endif
MASK_ALL_ONES(Int, 0xFFFF)
MASK_ALL_ONES(Float, 0xFFFF)
MASK_ALL_ONES(Long, 0xFF)
MASK_ALL_ONES(Double, 0xFF)

/*
Short explanation:
Intrinsics (e.g. _kand_mask16, _kor_mask32) are only available for gcc versions
>= 7. The intrinsics for __mmask32 and __mmask64 are only available under
AVX512BW Intrinsics with a different name and only for __mmask16 (e.g.
_mm512_kand) are available for gcc versions >= 6 If AVX512BW is not available,
the Byte/SignedByte/Word/Short masks are vectors, then the vector functions
are used The last resort (because it is probably slower in most cases) is to
emulate the functions with normal operators (e.g. "+" for kadd, "<<" for
kshiftl, "&" for kand)
*/

#if __GNUC__ >= 7 // TODO other compilers (not really a problem, then the
                  // intrinsics will just not be used)
#define GENERATE_DMASKOP(NAME, TYPE, NUM)                                      \
  static SIMD_INLINE Mask<TYPE, 64> k##NAME(const Mask<TYPE, 64> &a,           \
                                            const Mask<TYPE, 64> &b)           \
  {                                                                            \
    return _k##NAME##_mask##NUM(a, b);                                         \
  }

#define KNOT(TYPE, NUM)                                                        \
  static SIMD_INLINE Mask<TYPE, 64> knot(const Mask<TYPE, 64> &a)              \
  {                                                                            \
    return _knot_mask##NUM(a);                                                 \
  }

// shift with template parameter
#define KSHIFT(R_OR_L, TYPE, NUM)                                              \
  template <size_t COUNT>                                                      \
  static SIMD_INLINE Mask<TYPE, 64> kshift##R_OR_L##i(const Mask<TYPE, 64> &a) \
  {                                                                            \
    return _kshift##R_OR_L##i_mask##NUM(a, COUNT);                             \
  }
#ifdef __AVX512BW__
GENERATE_DMASKOP(and, Byte, 64)
GENERATE_DMASKOP(and, SignedByte, 64)
GENERATE_DMASKOP(and, Word, 32)
GENERATE_DMASKOP(and, Short, 32)

GENERATE_DMASKOP(andn, Byte, 64)
GENERATE_DMASKOP(andn, SignedByte, 64)
GENERATE_DMASKOP(andn, Word, 32)
GENERATE_DMASKOP(andn, Short, 32)

GENERATE_DMASKOP(or, Byte, 64)
GENERATE_DMASKOP(or, SignedByte, 64)
GENERATE_DMASKOP(or, Word, 32)
GENERATE_DMASKOP(or, Short, 32)

GENERATE_DMASKOP(xor, Byte, 64)
GENERATE_DMASKOP(xor, SignedByte, 64)
GENERATE_DMASKOP(xor, Word, 32)
GENERATE_DMASKOP(xor, Short, 32)

GENERATE_DMASKOP(xnor, Byte, 64)
GENERATE_DMASKOP(xnor, SignedByte, 64)
GENERATE_DMASKOP(xnor, Word, 32)
GENERATE_DMASKOP(xnor, Short, 32)

GENERATE_DMASKOP(add, Byte, 64)
GENERATE_DMASKOP(add, SignedByte, 64)
GENERATE_DMASKOP(add, Word, 32)
GENERATE_DMASKOP(add, Short, 32)

KNOT(Byte, 64)
KNOT(SignedByte, 64)
KNOT(Word, 32)
KNOT(Short, 32)

KSHIFT(r, Byte, 64)
KSHIFT(r, SignedByte, 64)
KSHIFT(r, Word, 32)
KSHIFT(r, Short, 32)
KSHIFT(l, Byte, 64)
KSHIFT(l, SignedByte, 64)
KSHIFT(l, Word, 32)
KSHIFT(l, Short, 32)
// else-case is further down
#endif // ifdef __AVX512BW__

GENERATE_DMASKOP(and, Int, 16)
GENERATE_DMASKOP(and, Float, 16)
GENERATE_DMASKOP(and, Long, 8)
GENERATE_DMASKOP(and, Double, 8)

GENERATE_DMASKOP(andn, Int, 16)
GENERATE_DMASKOP(andn, Float, 16)
GENERATE_DMASKOP(andn, Long, 8)
GENERATE_DMASKOP(andn, Double, 8)

GENERATE_DMASKOP(or, Int, 16)
GENERATE_DMASKOP(or, Float, 16)
GENERATE_DMASKOP(or, Long, 8)
GENERATE_DMASKOP(or, Double, 8)

GENERATE_DMASKOP(xor, Int, 16)
GENERATE_DMASKOP(xor, Float, 16)
GENERATE_DMASKOP(xor, Long, 8)
GENERATE_DMASKOP(xor, Double, 8)

GENERATE_DMASKOP(xnor, Int, 16)
GENERATE_DMASKOP(xnor, Float, 16)
GENERATE_DMASKOP(xnor, Long, 8)
GENERATE_DMASKOP(xnor, Double, 8)

#ifdef __AVX512DQ__ // _kadd_mask16 and _kadd_mask8 are only available unter
                    // AVX512DQ
GENERATE_DMASKOP(add, Int, 16)
GENERATE_DMASKOP(add, Float, 16)
GENERATE_DMASKOP(add, Long, 8)
GENERATE_DMASKOP(add, Double, 8)
#endif

KNOT(Int, 16)
KNOT(Float, 16)
KNOT(Long, 8)
KNOT(Double, 8)

KSHIFT(r, Int, 16)
KSHIFT(r, Float, 16)
KSHIFT(r, Long, 8)
KSHIFT(r, Double, 8)

KSHIFT(l, Int, 16)
KSHIFT(l, Float, 16)
KSHIFT(l, Long, 8)
KSHIFT(l, Double, 8)
#else
//(__GNUC__ >= 7) is false
#if __GNUC__ >= 6
// At least the intrinsics for 16- and 8-masks (Int, Float, Long and Double) are
// defined.
#define GENERATE_DMASKOP(NAME, TYPE, NUM)                                      \
  static SIMD_INLINE Mask<TYPE, 64> k##NAME(const Mask<TYPE, 64> &a,           \
                                            const Mask<TYPE, 64> &b)           \
  {                                                                            \
    return _mm512_k##NAME(a, b);                                               \
  }

#define KNOT(TYPE, NUM)                                                        \
  static SIMD_INLINE Mask<TYPE, 64> knot(const Mask<TYPE, 64> &a)              \
  {                                                                            \
    return _mm512_knot(a);                                                     \
  }
GENERATE_DMASKOP(and, Int, 16)
GENERATE_DMASKOP(and, Float, 16)
GENERATE_DMASKOP(and, Long, 8)
GENERATE_DMASKOP(and, Double, 8)

GENERATE_DMASKOP(andn, Int, 16)
GENERATE_DMASKOP(andn, Float, 16)
GENERATE_DMASKOP(andn, Long, 8)
GENERATE_DMASKOP(andn, Double, 8)

GENERATE_DMASKOP(or, Int, 16)
GENERATE_DMASKOP(or, Float, 16)
GENERATE_DMASKOP(or, Long, 8)
GENERATE_DMASKOP(or, Double, 8)

GENERATE_DMASKOP(xor, Int, 16)
GENERATE_DMASKOP(xor, Float, 16)
GENERATE_DMASKOP(xor, Long, 8)
GENERATE_DMASKOP(xor, Double, 8)

GENERATE_DMASKOP(xnor, Int, 16)
GENERATE_DMASKOP(xnor, Float, 16)
GENERATE_DMASKOP(xnor, Long, 8)
GENERATE_DMASKOP(xnor, Double, 8)

KNOT(Int, 16)
KNOT(Float, 16)
KNOT(Long, 8)
KNOT(Double, 8)
#endif

template <typename T>
static SIMD_INLINE Mask<T, 64> kand(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return (a & b);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kandn(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return (~a) & b;
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kor(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return (a | b);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kxor(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return (a ^ b);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kxnor(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return ~(a ^ b);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kadd(const Mask<T, 64> &a, const Mask<T, 64> &b)
{
  return (a + b);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> knot(const Mask<T, 64> &a)
{
  return ~a;
}

template <size_t COUNT, typename T>
static SIMD_INLINE Mask<T, 64> kshiftri(const Mask<T, 64> &a)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if COUNT is larger than 63, since then the >> operator is
  // undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  // since COUNT is a constant, the compiler should optimize away the
  // if-statement
  if (COUNT >= 64) { return 0; }
// we checked that COUNT is not too large above, disable warning
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wshift-count-overflow"
  return ((uint64_t) a) >> ((uint64_t) COUNT);
#pragma GCC diagnostic pop
}

template <size_t COUNT, typename T>
static SIMD_INLINE Mask<T, 64> kshiftli(const Mask<T, 64> &a)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if COUNT is larger than 63, since then the << operator is
  // undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  // since COUNT is a constant, the compiler should optimize away the
  // if-statement
  if (COUNT >= 64) { return 0; }
// we checked that COUNT is not too large above, disable warning
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wshift-count-overflow"
  return ((uint64_t) a) << ((uint64_t) COUNT);
#pragma GCC diagnostic pop
}
#endif // if __GNUC__ >= 7

// shift with flexible parameter (not template), probably slower than
// template-version
/*//TODO faster implementation with switch-case possible?
#define SHIFT_CASE(OP, NUM) case : OP<NUM>(a); break;

#define EMULATE_KSHIFT(R_OR_L, OP, TYPE) \
static SIMD_INLINE Mask<TYPE, 64> \
kshift ## R_OR_L ## i (const Mask<TYPE, 64> &a, \
    uint64_t count) \
{ \
  return (a OP count); \
  switch(count) { \
    SHIFT_CASE(OP2, 0) \
    SHIFT_CASE(OP2, 1) \
  } \
}*/

template <typename T>
static SIMD_INLINE Mask<T, 64> kshiftli(const Mask<T, 64> &a, uint64_t count)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if count is larger than 63, since then the << operator is
  // undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  if (count >= 64) { return Mask<T, 64>(0); }
  return Mask<T, 64>(((uint64_t) a) << count);
}

template <typename T>
static SIMD_INLINE Mask<T, 64> kshiftri(const Mask<T, 64> &a, uint64_t count)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if count is larger than 63, since then the >> operator is
  // undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  if (count >= 64) { return Mask<T, 64>(0); }
  return Mask<T, 64>(((uint64_t) a) >> count);
}

// 07. Aug 23 (Jonas Keller): added mask_test_all_zeros/ones.

template <typename T>
static SIMD_INLINE bool mask_test_all_zeros(const Mask<T, 64> &a)
{
  return a == 0;
}

template <typename T>
static SIMD_INLINE bool mask_test_all_ones(const Mask<T, 64> &a)
{
  return a == mask_all_ones(OutputType<T>(), Integer<64>());
}

// 07. Aug 23 (Jonas Keller): added kcmpeq

template <typename T>
static SIMD_INLINE Mask<T, 64> kcmpeq(const Mask<T, 64> &a,
                                      const Mask<T, 64> &b)
{
  return a == b;
}
} // namespace mask
} // namespace internal
} // namespace simd

#endif

#endif // SIMD_VEC_MASK_IMPL_INTEL_64_H_

#include <cstdint>

#ifndef SIMDVEC_SANDBOX

namespace simd {
// 05. Feb 23 (Jonas Keller): introduced generic emulated Mask class using the
// Vec class

// exclude from doxygen (until endcond)
/// @cond

template <typename T, size_t SIMD_WIDTH>
class Mask
{
  Vec<T, SIMD_WIDTH> mask;

public:
  Mask() = default;
  explicit SIMD_INLINE Mask(const Vec<T, SIMD_WIDTH> &x) : mask(x) {}
  SIMD_INLINE Mask(const uint64_t x) : mask(int2bits<T, SIMD_WIDTH>(x)) {}
  explicit SIMD_INLINE operator Vec<T, SIMD_WIDTH>() const { return mask; }
  SIMD_INLINE operator uint64_t() const { return msb2int<T, SIMD_WIDTH>(mask); }
  SIMD_INLINE bool operator[](const size_t i) const
  {
    if (i >= SIMD_WIDTH) { return false; }
    uint8_t mask_array[SIMD_WIDTH] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
    store((T *) mask_array, mask);
    // check first byte of i-th element (the bytes of a single element should
    // be the same)
    return mask_array[i * sizeof(T)] != 0;
  }
  SIMD_INLINE bool operator==(const Mask<T, SIMD_WIDTH> &other) const
  {
    return test_all_zeros(
      bit_xor(reinterpret<Int>(mask), reinterpret<Int>(other.mask)));
  }
  // define operators new and delete to ensure proper alignment, since
  // the default new and delete are not guaranteed to do so before C++17
  void *operator new(size_t size)
  {
    return simd_aligned_malloc(SIMD_WIDTH, size);
  }
  void operator delete(void *p) { simd_aligned_free(p); }
  void *operator new[](size_t size)
  {
    return simd_aligned_malloc(SIMD_WIDTH, size);
  }
  void operator delete[](void *p) { simd_aligned_free(p); }
};
/// @endcond

namespace internal {
namespace mask {
#define EMULATE_SOP_NAME(OP, OP_NAME)                                          \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_##OP_NAME(                       \
    const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &a)                 \
  {                                                                            \
    return mask::mask_ifelsezero(k, OP(a));                                    \
  }                                                                            \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_##OP_NAME(                        \
    const Vec<T, SIMD_WIDTH> &src, const Mask<T, SIMD_WIDTH> &k,               \
    const Vec<T, SIMD_WIDTH> &a)                                               \
  {                                                                            \
    return mask::mask_ifelse(k, OP(a), src);                                   \
  }

#define EMULATE_SOP(OP) EMULATE_SOP_NAME(OP, OP)

#define EMULATE_DOP_NAME(OP, OP_NAME)                                          \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_##OP_NAME(                       \
    const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &a,                 \
    const Vec<T, SIMD_WIDTH> &b)                                               \
  {                                                                            \
    return mask::mask_ifelsezero(k, OP(a, b));                                 \
  }                                                                            \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_##OP_NAME(                        \
    const Vec<T, SIMD_WIDTH> &src, const Mask<T, SIMD_WIDTH> &k,               \
    const Vec<T, SIMD_WIDTH> &a, const Vec<T, SIMD_WIDTH> &b)                  \
  {                                                                            \
    return mask::mask_ifelse(k, OP(a, b), src);                                \
  }

#define EMULATE_DOP(OP) EMULATE_DOP_NAME(OP, OP)

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelse(
  const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &trueVal,
  const Vec<T, SIMD_WIDTH> &falseVal)
{
  return ifelse((Vec<T, SIMD_WIDTH>) k, trueVal, falseVal);
}

// 04. Aug 22 (Jonas Keller): added mask_ifelsezero
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelsezero(
  const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &trueVal)
{
  return bit_and((Vec<T, SIMD_WIDTH>) k, trueVal);
}

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<Tout, SIMD_WIDTH> reinterpret_mask(
  const Mask<Tin, SIMD_WIDTH> &k)
{
  static_assert(sizeof(Tout) == sizeof(Tin), "");
  return Mask<Tout, SIMD_WIDTH>(reinterpret<Tout>((Vec<Tin, SIMD_WIDTH>) k));
}

// The types of the masks are kind of arbitrary
template <size_t SIMD_WIDTH>
SIMD_INLINE Vec<Int, SIMD_WIDTH> maskz_cvts(const Mask<Float, SIMD_WIDTH> &k,
                                            const Vec<Float, SIMD_WIDTH> &a)
{
  return mask::mask_ifelsezero(mask::reinterpret_mask<Int>(k),
                               ::simd::cvts<Int>(a));
}

template <size_t SIMD_WIDTH>
SIMD_INLINE Vec<Int, SIMD_WIDTH> mask_cvts(const Vec<Int, SIMD_WIDTH> &src,
                                           const Mask<Float, SIMD_WIDTH> &k,
                                           const Vec<Float, SIMD_WIDTH> &a)
{
  return mask::mask_ifelse(mask::reinterpret_mask<Int>(k), ::simd::cvts<Int>(a),
                           src);
}

template <size_t SIMD_WIDTH>
SIMD_INLINE Vec<Float, SIMD_WIDTH> maskz_cvts(const Mask<Int, SIMD_WIDTH> &k,
                                              const Vec<Int, SIMD_WIDTH> &a)
{
  return mask::mask_ifelsezero(mask::reinterpret_mask<Float>(k),
                               ::simd::cvts<Float>(a));
}

template <size_t SIMD_WIDTH>
SIMD_INLINE Vec<Float, SIMD_WIDTH> mask_cvts(const Vec<Float, SIMD_WIDTH> &src,
                                             const Mask<Int, SIMD_WIDTH> &k,
                                             const Vec<Int, SIMD_WIDTH> &a)
{
  return mask::mask_ifelse(mask::reinterpret_mask<Float>(k),
                           ::simd::cvts<Float>(a), src);
}

// =======================================================================
// emulated load/store
// =======================================================================

// 04. Feb 23 (Jonas Keller): improved implementation of masked load/store
// functions

template <size_t SIMD_WIDTH, typename T>
static SIMD_INLINE bool is_within_same_page(const T *const p)
{
  const uintptr_t PAGE_SIZE = 4096; // smallest page size I found
  const uintptr_t begin_page =
    reinterpret_cast<uintptr_t>(p) & ~(PAGE_SIZE - 1);
  // 29. Aug 23 (Jonas Keller): fixed wrong calculation of end_page
  const uintptr_t end_page =
    // reinterpret_cast<uintptr_t>(p + Vec<T, SIMD_WIDTH>::elems - 1) &
    // ~(PAGE_SIZE - 1);
    (reinterpret_cast<uintptr_t>(p) + SIMD_WIDTH - 1) & ~(PAGE_SIZE - 1);
  return begin_page == end_page;
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_load(const Mask<T, SIMD_WIDTH> &k,
                                                 const T *const p)
{
  // if k is all zeros nothing should be loaded
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) {
    return setzero<T, SIMD_WIDTH>();
  }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector and mask
  // it. Otherwise, we load the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    return mask::mask_ifelsezero(k, load<SIMD_WIDTH>(p));
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can load the whole vector
    return load<SIMD_WIDTH>(p);
  }
  T result[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    result[i] = k[i] ? p[i] : 0;
  }
  return load<SIMD_WIDTH>(result);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_load(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const T *const p)
{
  // if k is all zeros nothing should be loaded
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) { return src; }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector and mask
  // it. Otherwise, we load the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    return mask::mask_ifelse(k, load<SIMD_WIDTH>(p), src);
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can load the whole vector
    return load<SIMD_WIDTH>(p);
  }
  T result[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  T src_arr[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  store(src_arr, src);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    result[i] = k[i] ? p[i] : src_arr[i];
  }
  return load<SIMD_WIDTH>(result);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_loadu(const Mask<T, SIMD_WIDTH> &k,
                                                  const T *const p)
{
  // if k is all zeros nothing should be loaded
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) {
    return setzero<T, SIMD_WIDTH>();
  }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector and mask
  // it. Otherwise, we load the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    return mask::mask_ifelsezero(k, loadu<SIMD_WIDTH>(p));
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can load the whole vector
    return loadu<SIMD_WIDTH>(p);
  }
  T result[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    result[i] = k[i] ? p[i] : 0;
  }
  return load<SIMD_WIDTH>(result);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_loadu(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const T *const p)
{
  // if k is all zeros nothing should be loaded
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) { return src; }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector and mask
  // it. Otherwise, we load the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    return mask::mask_ifelse(k, loadu<SIMD_WIDTH>(p), src);
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can load the whole vector
    return loadu<SIMD_WIDTH>(p);
  }
  T result[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  T src_arr[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  store(src_arr, src);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    result[i] = k[i] ? p[i] : src_arr[i];
  }
  return load<SIMD_WIDTH>(result);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_store(T *const p, const Mask<T, SIMD_WIDTH> &k,
                                   const Vec<T, SIMD_WIDTH> &a)
{
  // if k is all zeros nothing should be stored
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) { return; }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector, mask it
  // and store it back. Otherwise, we store the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    store(p, mask::mask_ifelse(k, a, load<SIMD_WIDTH>(p)));
    return;
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can store the whole vector
    store(p, a);
    return;
  }
  T a_arr[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  store(a_arr, a);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    if (k[i]) { p[i] = a_arr[i]; }
  }
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_storeu(T *const p, const Mask<T, SIMD_WIDTH> &k,
                                    const Vec<T, SIMD_WIDTH> &a)
{
  // if k is all zeros nothing should be stored
  if (test_all_zeros((Vec<T, SIMD_WIDTH>) k)) { return; }
  // If p till p+Vec<T, SIMD_WIDTH>::elems-1 is within the same page,
  // there is no risk of a page fault, so we load the whole vector, mask it
  // and store it back. Otherwise, we store the vector element-wise.
  if (is_within_same_page<SIMD_WIDTH>(p)) {
    storeu(p, mask::mask_ifelse(k, a, loadu<SIMD_WIDTH>(p)));
    return;
  }
  if (test_all_ones((Vec<T, SIMD_WIDTH>) k)) {
    // if k is all ones, we can store the whole vector
    storeu(p, a);
    return;
  }
  T a_arr[Vec<T, SIMD_WIDTH>::elems] SIMD_ATTR_ALIGNED(SIMD_WIDTH);
  store(a_arr, a);
  for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
    if (k[i]) { p[i] = a_arr[i]; }
  }
}

// maskz_store(u) does not exist/does not make sense

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_set1(const Mask<T, SIMD_WIDTH> &k,
                                                 const T a)
{
  return mask::mask_ifelsezero(k, ::simd::set1<T, SIMD_WIDTH>(a));
}
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_set1(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const T a)
{
  return mask::mask_ifelse(k, ::simd::set1<T, SIMD_WIDTH>(a), src);
}

EMULATE_DOP(add)
EMULATE_DOP(adds)
EMULATE_DOP(sub)
EMULATE_DOP(subs)

EMULATE_DOP(mul)
EMULATE_DOP(div)

// ---------------------------------------------------------------------------
// masked ceil, floor, round, truncate v
// ---------------------------------------------------------------------------

EMULATE_SOP(ceil)
EMULATE_SOP(floor)
EMULATE_SOP(round)
EMULATE_SOP(truncate)

// ---------------------------------------------------------------------------
// masked elementary mathematical functions v
// ---------------------------------------------------------------------------

EMULATE_SOP(rcp)
EMULATE_SOP(rsqrt)
EMULATE_SOP(sqrt)

EMULATE_SOP(abs)

EMULATE_DOP_NAME(bit_and, and)
EMULATE_DOP_NAME(bit_or, or)
EMULATE_DOP_NAME(bit_andnot, andnot)
EMULATE_DOP_NAME(bit_xor, xor)
EMULATE_SOP_NAME(bit_not, not )
EMULATE_SOP(neg)
EMULATE_DOP(min)
EMULATE_DOP(max)
EMULATE_SOP(div2r0)
EMULATE_SOP(div2rd)

#define EMULATE_SHIFT(OP)                                                      \
  template <size_t COUNT, typename T, size_t SIMD_WIDTH>                       \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_##OP(                            \
    const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &a)                 \
  {                                                                            \
    return mask::mask_ifelsezero(k, OP<COUNT>(a));                             \
  }                                                                            \
  template <size_t COUNT, typename T, size_t SIMD_WIDTH>                       \
  static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_##OP(                             \
    const Vec<T, SIMD_WIDTH> &src, const Mask<T, SIMD_WIDTH> &k,               \
    const Vec<T, SIMD_WIDTH> &a)                                               \
  {                                                                            \
    return mask::mask_ifelse(k, OP<COUNT>(a), src);                            \
  }
EMULATE_SHIFT(srai)
EMULATE_SHIFT(srli)
EMULATE_SHIFT(slli)

EMULATE_DOP(hadd)
EMULATE_DOP(hadds)
EMULATE_DOP(hsub)
EMULATE_DOP(hsubs)

// TODO mask parameters?

// 16. Oct 22 (Jonas Keller): added overloaded versions of mask_cmp* functions
// that only take two vector parameters and no mask parameter
#define EMULATE_CMP(OP)                                                        \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_##OP(                            \
    const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &a,                 \
    const Vec<T, SIMD_WIDTH> &b)                                               \
  {                                                                            \
    return Mask<T, SIMD_WIDTH>(mask::mask_ifelsezero(k, OP(a, b)));            \
  }                                                                            \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_##OP(                            \
    const Vec<T, SIMD_WIDTH> &a, const Vec<T, SIMD_WIDTH> &b)                  \
  {                                                                            \
    return Mask<T, SIMD_WIDTH>(OP(a, b));                                      \
  }

EMULATE_CMP(cmplt)
EMULATE_CMP(cmple)
EMULATE_CMP(cmpeq)
EMULATE_CMP(cmpgt)
EMULATE_CMP(cmpge)
EMULATE_CMP(cmpneq)

EMULATE_DOP(avg)

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_zeros(const Mask<T, SIMD_WIDTH> &k,
                                            const Vec<T, SIMD_WIDTH> &a)
{
  return test_all_zeros(mask::mask_ifelsezero(k, a));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_ones(const Mask<T, SIMD_WIDTH> &k,
                                           const Vec<T, SIMD_WIDTH> &a)
{
  return mask::mask_test_all_zeros(
    k, bit_not(a)); // test_all_ones(mask_ifelse<T, SIMD_WIDTH>(k, a, ()
                    // set1<Byte, SIMD_WIDTH>(0xFF)));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_all_ones(OutputType<T>,
                                                     Integer<SIMD_WIDTH>)
{
  return (Mask<T, SIMD_WIDTH>) set1<T, SIMD_WIDTH>(TypeInfo<T>::trueval());
}

#define EMULATE_DMASKOP(NAME)                                                  \
  template <typename T, size_t SIMD_WIDTH>                                     \
  static SIMD_INLINE Mask<T, SIMD_WIDTH> k##NAME(const Mask<T, SIMD_WIDTH> &a, \
                                                 const Mask<T, SIMD_WIDTH> &b) \
  {                                                                            \
    return (Mask<T, SIMD_WIDTH>) NAME##_((Vec<T, SIMD_WIDTH>) a,               \
                                         (Vec<T, SIMD_WIDTH>) b);              \
  }

EMULATE_DMASKOP(and)

// EMULATE_DMASKOP(andn)
//  function name should be "kandn" but the vector function is "bit_andnot"
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kandn(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  return (Mask<T, SIMD_WIDTH>) bit_andnot((Vec<T, SIMD_WIDTH>) a,
                                          (Vec<T, SIMD_WIDTH>) b);
}

EMULATE_DMASKOP(or)
EMULATE_DMASKOP(xor)

// EMULATE_DMASKOP(xnor)
//  there is not xnor-function for vectors, so we have to do: bit_not(bit_xor(a,
//  b))
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kxnor(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  return (Mask<T, SIMD_WIDTH>) bit_not(
    bit_xor((Vec<T, SIMD_WIDTH>) a, (Vec<T, SIMD_WIDTH>) b));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kadd(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  Mask<T, SIMD_WIDTH> ret;
  ret = (((uintmax_t) a) + ((uintmax_t) b));
  return ret;
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> knot(const Mask<T, SIMD_WIDTH> &a)
{
  return (Mask<T, SIMD_WIDTH>) bit_not((Vec<T, SIMD_WIDTH>) a);
}

// shift with flexible parameter (not template), probably slower than
// template-version
// TODO faster implementation with switch-case possible?
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a,
                                                uintmax_t count)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if count is larger than sizeof(uintmax_t)*8 - 1, since then
  // the >> operator is undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  if (count >= sizeof(uintmax_t) * 8) { return Mask<T, SIMD_WIDTH>(0); }
  return (Mask<T, SIMD_WIDTH>) (((uintmax_t) a) >> count);
}
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a,
                                                uintmax_t count)
{
  // 04. Aug 22 (Jonas Keller):
  // return zero if count is larger than sizeof(uintmax_t)*8 - 1, since then
  // the << operator is undefined, but kshift should return zero
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=kshift
  if (count >= sizeof(uintmax_t) * 8) { return Mask<T, SIMD_WIDTH>(0); }
  return (Mask<T, SIMD_WIDTH>) (((uintmax_t) a) << count);
}

// shift with template parameter
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a)
{
  return (Mask<T, SIMD_WIDTH>) srle<COUNT>((Vec<T, SIMD_WIDTH>) a);
}
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a)
{
  return (Mask<T, SIMD_WIDTH>) slle<COUNT>((Vec<T, SIMD_WIDTH>) a);
}

// 30. Jan 23 (Jonas Keller): removed setTrueLeft/Right and replaced them with
// mask_set_true/false_low/high.

template <bool UP, typename T, size_t SIMD_WIDTH>
struct MaskSetBuffer
{
  T buffer[Vec<T, SIMD_WIDTH>::elems * 2];
  MaskSetBuffer()
  {
    for (size_t i = 0; i < Vec<T, SIMD_WIDTH>::elems; i++) {
      buffer[i] = UP ? 0 : TypeInfo<T>::trueval();
    }
    for (size_t i = Vec<T, SIMD_WIDTH>::elems;
         i < Vec<T, SIMD_WIDTH>::elems * 2; i++) {
      buffer[i] = UP ? TypeInfo<T>::trueval() : 0;
    }
  }
};

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_low(const size_t x,
                                                         OutputType<T>,
                                                         Integer<SIMD_WIDTH>)
{
  if (x >= Vec<T, SIMD_WIDTH>::elems) {
    return mask_all_ones(OutputType<T>(), Integer<SIMD_WIDTH>());
  }
  static MaskSetBuffer<false, T, SIMD_WIDTH> buffer;
  return Mask<T, SIMD_WIDTH>(
    loadu<SIMD_WIDTH>(buffer.buffer + Vec<T, SIMD_WIDTH>::elems - x));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_high(const size_t x,
                                                          OutputType<T>,
                                                          Integer<SIMD_WIDTH>)
{
  if (x >= Vec<T, SIMD_WIDTH>::elems) {
    return mask_all_ones(OutputType<T>(), Integer<SIMD_WIDTH>());
  }
  static MaskSetBuffer<true, T, SIMD_WIDTH> buffer;
  return Mask<T, SIMD_WIDTH>(loadu<SIMD_WIDTH>(buffer.buffer + x));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_low(const size_t x,
                                                          OutputType<T>,
                                                          Integer<SIMD_WIDTH>)
{
  if (x >= Vec<T, SIMD_WIDTH>::elems) { return Mask<T, SIMD_WIDTH>(0); }
  static MaskSetBuffer<true, T, SIMD_WIDTH> buffer;
  return Mask<T, SIMD_WIDTH>(
    loadu<SIMD_WIDTH>(buffer.buffer + Vec<T, SIMD_WIDTH>::elems - x));
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_high(const size_t x,
                                                           OutputType<T>,
                                                           Integer<SIMD_WIDTH>)
{
  if (x >= Vec<T, SIMD_WIDTH>::elems) { return Mask<T, SIMD_WIDTH>(0); }
  static MaskSetBuffer<false, T, SIMD_WIDTH> buffer;
  return Mask<T, SIMD_WIDTH>(loadu<SIMD_WIDTH>(buffer.buffer + x));
}

// 07. Aug 23 (Jonas Keller): added ktest_all_zeros/ones.

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_zeros(const Mask<T, SIMD_WIDTH> &a)
{
  return test_all_zeros((Vec<T, SIMD_WIDTH>) a);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_ones(const Mask<T, SIMD_WIDTH> &a)
{
  return test_all_ones((Vec<T, SIMD_WIDTH>) a);
}

// 07. Aug 23 (Jonas Keller): added kcmpeq

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool kcmpeq(const Mask<T, SIMD_WIDTH> &a,
                               const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::ktest_all_zeros(internal::mask::kxor(a, b));
}

} // namespace mask
} // namespace internal
} // namespace simd

#endif // SIMDVEC_SANDBOX

#endif // SIMD_VEC_MASK_IMPL_EMU_H_

// ===========================================================================
//
// SIMDVecMaskImplSandbox.H --
// test template functions for Mask and associated function templates
// these functions just print out template parameters and some arguments
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 04. Aug 22 (Jonas Keller): added masked functions

// 22. Jan 23 (Jonas Keller): moved sandbox implementations into internal
// namespace

#ifndef SIMD_VEC_MASK_IMPL_SANDBOX_H_
#define SIMD_VEC_MASK_IMPL_SANDBOX_H_

// TODO: should this only contain level 0 functions?

#include <cstddef>
#include <cstdint>
#include <cstdio>

#ifdef SIMDVEC_SANDBOX

namespace simd {
// ===========================================================================
// generic template for Mask
// ===========================================================================

template <typename T, size_t SIMD_WIDTH>
class Mask
{
public:
  Mask() {}
  Mask(const uint64_t &) {}
  operator uint64_t() const { return 0; }
};

namespace internal {
namespace mask {
// ===========================================================================
// masked functions
// ===========================================================================

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_all_ones(OutputType<T>,
                                                     Integer<SIMD_WIDTH>)
{
  printf("mask_all_ones<%s,%zu>()\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelse(
  const Mask<T, SIMD_WIDTH> &cond, const Vec<T, SIMD_WIDTH> &trueVal,
  const Vec<T, SIMD_WIDTH> &falseVal)
{
  (void) cond;
  (void) trueVal;
  (void) falseVal;
  printf("mask_ifelse<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// mask_ifelsezero
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelsezero(
  const Mask<T, SIMD_WIDTH> &cond, const Vec<T, SIMD_WIDTH> &trueVal)
{
  (void) cond;
  (void) trueVal;
  printf("mask_ifelsezero<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

// 06. Feb 23 (Jonas Keller): added reinterpret_mask

template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<Tout, SIMD_WIDTH> reinterpret_mask(
  const Mask<Tin, SIMD_WIDTH> &)
{
  printf("reinterpret_mask<%s,%s,%zu>(M)\n", TypeInfo<Tout>::name(),
         TypeInfo<Tin>::name(), SIMD_WIDTH);
  return Mask<Tout, SIMD_WIDTH>();
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Int, SIMD_WIDTH> mask_cvts(
  const Vec<Int, SIMD_WIDTH> &, const Mask<Float, SIMD_WIDTH> &,
  const Vec<Float, SIMD_WIDTH> &)
{
  printf("mask_cvts<%s,%s,%zu>(V,M,V)\n", TypeInfo<Int>::name(),
         TypeInfo<Float>::name(), SIMD_WIDTH);
  return Vec<Int, SIMD_WIDTH>();
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Int, SIMD_WIDTH> maskz_cvts(
  const Mask<Float, SIMD_WIDTH> &, const Vec<Float, SIMD_WIDTH> &)
{
  printf("maskz_cvts<%s,%s,%zu>(M,V)\n", TypeInfo<Int>::name(),
         TypeInfo<Float>::name(), SIMD_WIDTH);
  return Vec<Int, SIMD_WIDTH>();
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH> mask_cvts(
  const Vec<Float, SIMD_WIDTH> &, const Mask<Int, SIMD_WIDTH> &,
  const Vec<Int, SIMD_WIDTH> &)
{
  printf("mask_cvts<%s,%s,%zu>(V,M,V)\n", TypeInfo<Float>::name(),
         TypeInfo<Int>::name(), SIMD_WIDTH);
  return Vec<Float, SIMD_WIDTH>();
}

template <size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Float, SIMD_WIDTH> maskz_cvts(
  const Mask<Int, SIMD_WIDTH> &, const Vec<Int, SIMD_WIDTH> &)
{
  printf("maskz_cvts<%s,%s,%zu>(M,V)\n", TypeInfo<Float>::name(),
         TypeInfo<Int>::name(), SIMD_WIDTH);
  return Vec<Float, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_set1(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const T &)
{
  printf("mask_set1<%s,%zu>(V,M,T)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_set1(const Mask<T, SIMD_WIDTH> &,
                                                 const T &)
{
  printf("maskz_set1<%s,%zu>(M,T)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_load(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const T *const p)
{
  printf("mask_load<%s,%zu>(V,M,%p)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_load(const Mask<T, SIMD_WIDTH> &,
                                                 const T *const p)
{
  printf("maskz_load<%s,%zu>(M,%p)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_loadu(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const T *const p)
{
  printf("mask_loadu<%s,%zu>(V,M,%p)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_loadu(const Mask<T, SIMD_WIDTH> &,
                                                  const T *const p)
{
  printf("maskz_loadu<%s,%zu>(M,%p)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_store(T *const p, const Mask<T, SIMD_WIDTH> &,
                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_store<%s,%zu>(%p,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_storeu(T *const p, const Mask<T, SIMD_WIDTH> &,
                                    const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_storeu<%s,%zu>(%p,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         (void *) p);
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_add(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_add<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_add(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_add<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_adds(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_adds<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_adds(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_adds<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_sub(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_sub<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_sub(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_sub<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_subs(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_subs<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_subs(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_subs<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_mul(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_mul<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_mul(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_mul<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_div<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_div<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ceil(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_ceil<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_ceil(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_ceil<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_floor(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_floor<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_floor(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_floor<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_round(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_round<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_round(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_round<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_truncate(const Vec<T, SIMD_WIDTH> &,
                                                    const Mask<T, SIMD_WIDTH> &,
                                                    const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_truncate<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_truncate(
  const Mask<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_truncate<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_rcp(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_rcp<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_rcp(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_rcp<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_rsqrt(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_rsqrt<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_rsqrt(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_rsqrt<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_sqrt(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_sqrt<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_sqrt(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_sqrt<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_abs(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_abs<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_abs(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_abs<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_and(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_and<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_and(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_and<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_or(const Vec<T, SIMD_WIDTH> &,
                                              const Mask<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &,
                                              const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_or<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_or(const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_or<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_andnot(const Vec<T, SIMD_WIDTH> &,
                                                  const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_andnot<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_andnot(const Mask<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_andnot<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_xor(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_xor<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_xor(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_xor<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_not(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_not<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_not(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_not<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_neg(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_neg<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_neg(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_neg<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_min(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_min<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_min(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_min<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_max(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_max<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_max(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_max<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div2r0(const Vec<T, SIMD_WIDTH> &,
                                                  const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_div2r0<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div2r0(const Mask<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_div2r0<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div2rd(const Vec<T, SIMD_WIDTH> &,
                                                  const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_div2rd<%s,%zu>(V,M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div2rd(const Mask<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_div2rd<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_srai(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_srai<%zu,%s,%zu>(V,M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_srai(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_srai<%zu,%s,%zu>(M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_srli(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_srli<%zu,%s,%zu>(V,M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_srli(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_srli<%zu,%s,%zu>(M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_slli(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_slli<%zu,%s,%zu>(V,M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_slli(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_slli<%zu,%s,%zu>(M,V)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hadd(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_hadd<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hadd(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_hadd<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hadds(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_hadds<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hadds(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_hadds<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hsub(const Vec<T, SIMD_WIDTH> &,
                                                const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_hsub<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hsub(const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_hsub<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hsubs(const Vec<T, SIMD_WIDTH> &,
                                                 const Mask<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &,
                                                 const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_hsubs<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hsubs(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_hsubs<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmplt(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmplt<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmple(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmple<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpeq(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpeq<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpgt(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpgt<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpge(const Mask<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpge<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpneq(const Mask<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpneq<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

// 12. Jan 23 (Jonas Keller): added overloaded versions of mask_cmp*
// that only take two vector parameters and no mask parameter

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmplt(const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmplt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmple(const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmple<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpeq(const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpeq<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpgt(const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpgt<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpge(const Vec<T, SIMD_WIDTH> &,
                                                  const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpge<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpneq(const Vec<T, SIMD_WIDTH> &,
                                                   const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_cmpneq<%s,%zu>(V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_avg(const Vec<T, SIMD_WIDTH> &,
                                               const Mask<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &,
                                               const Vec<T, SIMD_WIDTH> &)
{
  printf("mask_avg<%s,%zu>(V,M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_avg(const Mask<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &,
                                                const Vec<T, SIMD_WIDTH> &)
{
  printf("maskz_avg<%s,%zu>(M,V,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Vec<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_zeros(const Mask<T, SIMD_WIDTH> &k,
                                            const Vec<T, SIMD_WIDTH> &a)
{
  (void) k;
  (void) a;
  printf("mask_test_all_zeros<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return 0;
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_ones(const Mask<T, SIMD_WIDTH> &k,
                                           const Vec<T, SIMD_WIDTH> &a)
{
  (void) k;
  (void) a;
  printf("mask_test_all_ones<%s,%zu>(M,V)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return 0;
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kadd(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kadd<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kand(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kand<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kandn(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kandn<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kor(const Mask<T, SIMD_WIDTH> &a,
                                           const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kor<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kxor(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kxor<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kxnor(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kxnor<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> knot(const Mask<T, SIMD_WIDTH> &a)
{
  (void) a;
  printf("knot<%s,%zu>(M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return Mask<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a)
{
  (void) a;
  printf("kshiftri<%zu, %s,%zu>(M,%zu)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH, COUNT);
  return Mask<T, SIMD_WIDTH>();
}

template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a)
{
  (void) a;
  printf("kshiftli<%zu, %s,%zu>(M,%zu)\n", COUNT, TypeInfo<T>::name(),
         SIMD_WIDTH, COUNT);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a,
                                                const uint64_t count)
{
  (void) a;
  printf("kshiftri<%s,%zu>(M,%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH, count);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a,
                                                const uint64_t count)
{
  (void) a;
  printf("kshiftli<%s,%zu>(M,%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH, count);
  return Mask<T, SIMD_WIDTH>();
}

// 12. Jan 23 (Jonas Keller): add setTrueRight and setTrueLeft
// 30. Jan 23 (Jonas Keller): removed setTrueLeft/Right and replaced them with
// mask_set_true/false_low/high.

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_low(const size_t x,
                                                         OutputType<T>,
                                                         Integer<SIMD_WIDTH>)
{
  printf("mask_set_true_low<%s,%zu>(%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         x);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_high(const size_t x,
                                                          OutputType<T>,
                                                          Integer<SIMD_WIDTH>)
{
  printf("mask_set_true_high<%s,%zu>(%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         x);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_low(const size_t x,
                                                          OutputType<T>,
                                                          Integer<SIMD_WIDTH>)
{
  printf("mask_set_false_low<%s,%zu>(%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         x);
  return Mask<T, SIMD_WIDTH>();
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_high(const size_t x,
                                                           OutputType<T>,
                                                           Integer<SIMD_WIDTH>)
{
  printf("mask_set_false_high<%s,%zu>(%zu)\n", TypeInfo<T>::name(), SIMD_WIDTH,
         x);
  return Mask<T, SIMD_WIDTH>();
}

// 07. Aug 23 (Jonas Keller): added ktest_all_zeros/ones.

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_zeros(const Mask<T, SIMD_WIDTH> &a)
{
  (void) a;
  printf("ktest_all_zeros<%s,%zu>(M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return false;
}

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_ones(const Mask<T, SIMD_WIDTH> &a)
{
  (void) a;
  printf("ktest_all_ones<%s,%zu>(M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return false;
}

// 07. Aug 23 (Jonas Keller): added kcmpeq

template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool kcmpeq(const Mask<T, SIMD_WIDTH> &a,
                               const Mask<T, SIMD_WIDTH> &b)
{
  (void) a;
  (void) b;
  printf("kcmpeq<%s,%zu>(M,M)\n", TypeInfo<T>::name(), SIMD_WIDTH);
  return false;
}
} // namespace mask
} // namespace internal
} // namespace simd

#endif // SIMDVEC_SANDBOX

#endif // SIMD_VEC_MASK_IMPL_SANDBOX_H_

#include <cstddef>
#include <cstdint>

namespace simd {

/**
 * @addtogroup group_mask
 * @{
 */

/**
 * @addtogroup group_other_mask
 * @{
 */

/**
 * @brief Masked version of set1(const T).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_set1(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const T a)
{
  return internal::mask::mask_set1(src, k, a);
}

/**
 * @brief Zero-masked version of set1(const T).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_set1(const Mask<T, SIMD_WIDTH> &k,
                                                 const T a)
{
  return internal::mask::maskz_set1(k, a);
}

/**
 * @brief Masked version of cvts(const Vec<Tin, SIMD_WIDTH> &).
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> mask_cvts(
  const Vec<Tout, SIMD_WIDTH> &src, const Mask<Tin, SIMD_WIDTH> &k,
  const Vec<Tin, SIMD_WIDTH> &a)
{
  return internal::mask::mask_cvts(src, k, a);
}

/**
 * @brief Zero-masked version of cvts(const Vec<Tin, SIMD_WIDTH> &).
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<Tout, SIMD_WIDTH> maskz_cvts(
  const Mask<Tin, SIMD_WIDTH> &k, const Vec<Tin, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_cvts(k, a);
}

/** @} */

/** @} */

/**
 * @addtogroup group_mask_functions
 * @{
 */

// 06. Feb 23 (Jonas Keller): added reinterpret_mask

/**
 * @brief Reinterprets a Mask of one type as a Mask of another type.
 *
 * The size of the element type of the Mask must be the same.
 *
 * @tparam Tout element type of the reinterpreted Mask
 * @tparam Tin element type of the Mask to reinterpret
 * @param a Mask to reinterpret
 * @return reinterpreted Mask
 */
template <typename Tout, typename Tin, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<Tout, SIMD_WIDTH> reinterpret_mask(
  const Mask<Tin, SIMD_WIDTH> &a)
{
  return internal::mask::reinterpret_mask<Tout, Tin, SIMD_WIDTH>(a);
}

/**
 * @brief Adds two Mask's together as if they were integers.
 *
 * @param a first Mask
 * @param b second Mask
 * @return sum of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kadd(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kadd(a, b);
}

/**
 * @brief Computes the bitwise AND of two Mask's.
 *
 * @param a first Mask
 * @param b second Mask
 * @return bitwise AND of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kand(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kand(a, b);
}

/**
 * @brief Computes bitwise ANDNOT of two Mask's.
 *
 * The result is computed as (not a) and b.
 *
 * @param a first Mask
 * @param b second Mask
 * @return bitwise ANDNOT of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kandn(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kandn(a, b);
}

/**
 * @brief Computes the bitwise OR of two Mask's.
 *
 * @param a first Mask
 * @param b second Mask
 * @return bitwise OR of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kor(const Mask<T, SIMD_WIDTH> &a,
                                           const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kor(a, b);
}

/**
 * @brief Computes the bitwise XOR of two Mask's.
 *
 * @param a first Mask
 * @param b second Mask
 * @return bitwise XOR of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kxor(const Mask<T, SIMD_WIDTH> &a,
                                            const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kxor(a, b);
}

/**
 * @brief Computes the bitwise XNOR of two Mask's.
 *
 * @param a first Mask
 * @param b second Mask
 * @return bitwise XNOR of the two Mask's
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kxnor(const Mask<T, SIMD_WIDTH> &a,
                                             const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kxnor(a, b);
}

/**
 * @brief Computes the bitwise NOT of a Mask.
 *
 * @param a Mask
 * @return bitwise NOT of the Mask
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> knot(const Mask<T, SIMD_WIDTH> &a)
{
  return internal::mask::knot(a);
}

/**
 * @brief Shifts the bits of a Mask to the right by a
 *        constant number of bits.
 *
 * @param a Mask
 * @tparam COUNT number of bits to shift
 * @return shifted Mask
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a)
{
  return internal::mask::kshiftri<COUNT>(a);
}

/**
 * @brief Shifts the bits of a Mask to the left by a
 *        constant number of bits.
 *
 * @param a Mask
 * @tparam COUNT number of bits to shift
 * @return shifted Mask
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a)
{
  return internal::mask::kshiftli<COUNT>(a);
}

/**
 * @brief Shifts the bits of a Mask to the right by a
 *        variable number of bits.
 *
 * @param a Mask
 * @param count number of bits to shift
 * @return shifted Mask
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftri(const Mask<T, SIMD_WIDTH> &a,
                                                const uint64_t count)
{
  return internal::mask::kshiftri(a, count);
}

/**
 * @brief Shifts the bits of a Mask to the left by a
 *        variable number of bits.
 *
 * @param a Mask
 * @param count number of bits to shift
 * @return shifted Mask
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> kshiftli(const Mask<T, SIMD_WIDTH> &a,
                                                const uint64_t count)
{
  return internal::mask::kshiftli(a, count);
}

/**
 * @brief Creates a Mask with all elements set to true.
 *
 * @return Mask with all elements set to true
 */
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_all_ones()
{
  return internal::mask::mask_all_ones(internal::OutputType<T>(),
                                       internal::Integer<SIMD_WIDTH>());
}

// 30. Jan 23 (Jonas Keller): removed setTrueLeft/Right and replaced them with
// mask_set_true/false_low/high.

/**
 * @brief Sets the lower @p x bits of a Mask to true.
 *
 * The remaining bits are set to false.
 *
 * @param x number of bits to set to true
 * @return Mask with the lower @p x bits set to true
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_low(const size_t x)
{
  return internal::mask::mask_set_true_low(x, internal::OutputType<T>(),
                                           internal::Integer<SIMD_WIDTH>());
}

/**
 * @brief Sets the upper @p x bits of a Mask to true.
 *
 * The remaining bits are set to false.
 *
 * @param x number of bits to set to true
 * @return Mask with the upper @p x bits set to true
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_true_high(const size_t x)
{
  return internal::mask::mask_set_true_high(x, internal::OutputType<T>(),
                                            internal::Integer<SIMD_WIDTH>());
}

/**
 * @brief Sets the lower @p x bits of a Mask to false.
 *
 * The remaining bits are set to true.
 *
 * @param x number of bits to set to false
 * @return Mask with the lower @p x bits set to false
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_low(const size_t x)
{
  return internal::mask::mask_set_false_low(x, internal::OutputType<T>(),
                                            internal::Integer<SIMD_WIDTH>());
}

/**
 * @brief Sets the upper @p x bits of a Mask to false.
 *
 * The remaining bits are set to true.
 *
 * @param x number of bits to set to false
 * @return Mask with the upper @p x bits set to false
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_set_false_high(const size_t x)
{
  return internal::mask::mask_set_false_high(x, internal::OutputType<T>(),
                                             internal::Integer<SIMD_WIDTH>());
}

// 07. Aug 23 (Jonas Keller): added ktest_all_zeros/ones.

/**
 * @brief Tests if all bits of a Mask are set to false.
 *
 * @param a Mask to test
 * @return true if all bits are set to false, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_zeros(const Mask<T, SIMD_WIDTH> &a)
{
  return internal::mask::ktest_all_zeros(a);
}

/**
 * @brief Tests if all bits of a Mask are set to true.
 *
 * @param a Mask to test
 * @return true if all bits are set to true, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool ktest_all_ones(const Mask<T, SIMD_WIDTH> &a)
{
  return internal::mask::ktest_all_ones(a);
}

/** @} */

/**
 * @addtogroup group_mask_memory
 * @{
 */

/**
 * @brief Masked versions of load(const T *const).
 *
 * Does not touch memory if the corresponding mask element is false, so may
 * be used to prevent page faults where a non-masked load would cause a
 * fault.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_load(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const T *const p)
{
  return internal::mask::mask_load(src, k, p);
}

/**
 * @brief Zero-masked version of load(const T *const).
 *
 * Does not touch memory if the corresponding mask element is false, so may
 * be used to prevent page faults where a non-masked load would cause a
 * fault.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_load(const Mask<T, SIMD_WIDTH> &k,
                                                 const T *const p)
{
  return internal::mask::maskz_load(k, p);
}

/**
 * @brief Masked version of loadu(const T *const).
 *
 * Does not touch memory if the corresponding mask element is false, so may
 * be used to prevent page faults where a non-masked load would cause a
 * fault.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_loadu(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const T *const p)
{
  return internal::mask::mask_loadu(src, k, p);
}

/**
 * @brief Zero-masked version of loadu(const T *const).
 *
 * Does not touch memory if the corresponding mask element is false, so may
 * be used to prevent page faults where a non-masked load would cause a
 * fault.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_loadu(const Mask<T, SIMD_WIDTH> &k,
                                                  const T *const p)
{
  return internal::mask::maskz_loadu(k, p);
}

/**
 * @brief Masked version of store(T *const, const Vec<T, SIMD_WIDTH> &).
 *
 * Only stores the elements for which the corresponding mask element is true.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_store(T *const p, const Mask<T, SIMD_WIDTH> &k,
                                   const Vec<T, SIMD_WIDTH> &a)
{
  internal::mask::mask_store(p, k, a);
}

/**
 * @brief Masked version of storeu(T *const, const Vec<T, SIMD_WIDTH> &).
 *
 * Only stores the elements for which the corresponding mask element is true.
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE void mask_storeu(T *const p, const Mask<T, SIMD_WIDTH> &k,
                                    const Vec<T, SIMD_WIDTH> &a)
{
  internal::mask::mask_storeu(p, k, a);
}

/** @} */

/**
 * @addtogroup group_mask_arithmetic
 * @{
 */

/**
 * @brief Masked version of
 * add(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_add(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_add(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * add(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_add(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_add(k, a, b);
}

/**
 * @brief Masked version of
 * adds(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_adds(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_adds(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * adds(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_adds(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_adds(k, a, b);
}

/**
 * @brief Masked version of
 * sub(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_sub(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_sub(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * sub(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_sub(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_sub(k, a, b);
}

/**
 * @brief Masked version of
 * subs(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_subs(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_subs(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * subs(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_subs(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_subs(k, a, b);
}

/**
 * @brief Masked version of
 * mul(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_mul(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_mul(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * mul(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_mul(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_mul(k, a, b);
}

/**
 * @brief Masked version of
 * div(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_div(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * div(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_div(k, a, b);
}

/**
 * @brief Masked version of
 * avg(const Vec<T, SIMD_WIDTH> &a, const Vec<T, SIMD_WIDTH> &b).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_avg(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_avg(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * avg(const Vec<T, SIMD_WIDTH> &a, const Vec<T, SIMD_WIDTH> &b).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_avg(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_avg(k, a, b);
}

/**
 * @brief Masked version of
 * div2r0(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div2r0(const Vec<T, SIMD_WIDTH> &src,
                                                  const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_div2r0(src, k, a);
}

/**
 * @brief Zero-masked version of
 * div2r0(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div2r0(const Mask<T, SIMD_WIDTH> &k,
                                                   const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_div2r0(k, a);
}

/**
 * @brief Masked version of
 * div2rd(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_div2rd(const Vec<T, SIMD_WIDTH> &src,
                                                  const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_div2rd(src, k, a);
}

/**
 * @brief Zero-masked version of
 * div2rd(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_div2rd(const Mask<T, SIMD_WIDTH> &k,
                                                   const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_div2rd(k, a);
}

/** @} */

/**
 * @addtogroup group_mask_horizontal
 * @{
 */

/**
 * @brief Masked version of
 * hadd(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hadd(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_hadd(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * hadd(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hadd(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_hadd(k, a, b);
}

/**
 * @brief Masked version of
 * hadds(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hadds(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_hadds(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * hadds(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hadds(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_hadds(k, a, b);
}

/**
 * @brief Masked version of
 * hsub(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hsub(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_hsub(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * hsub(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hsub(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_hsub(k, a, b);
}

/**
 * @brief Masked version of
 * hsubs(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_hsubs(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a,
                                                 const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_hsubs(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * hsubs(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_hsubs(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_hsubs(k, a, b);
}

/** @} */

/**
 * @addtogroup group_mask_math_functions
 * @{
 */

/**
 * @brief Masked version of
 * rcp(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_rcp(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_rcp(src, k, a);
}

/**
 * @brief Zero-masked version of
 * rcp(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_rcp(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_rcp(k, a);
}

/**
 * @brief Masked version of
 * rsqrt(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_rsqrt(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_rsqrt(src, k, a);
}

/**
 * @brief Zero-masked version of
 * rsqrt(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_rsqrt(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_rsqrt(k, a);
}

/**
 * @brief Masked version of
 * sqrt(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_sqrt(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_sqrt(src, k, a);
}

/**
 * @brief Zero-masked version of
 * sqrt(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_sqrt(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_sqrt(k, a);
}

/** @} */

/**
 * @addtogroup group_mask_math_operations
 * @{
 */

/**
 * @brief Masked version of
 * min(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_min(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_min(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * min(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_min(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_min(k, a, b);
}

/**
 * @brief Masked version of
 * max(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_max(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_max(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * max(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_max(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_max(k, a, b);
}

/**
 * @brief Masked version of
 * neg(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_neg(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_neg(src, k, a);
}

/**
 * @brief Zero-masked version of
 * neg(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_neg(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_neg(k, a);
}

/**
 * @brief Masked version of
 * abs(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_abs(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_abs(src, k, a);
}

/**
 * @brief Zero-masked version of
 * abs(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_abs(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_abs(k, a);
}

/**
 * @brief Masked version of
 * ceil(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ceil(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_ceil(src, k, a);
}

/**
 * @brief Zero-masked version of
 * ceil(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_ceil(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_ceil(k, a);
}

/**
 * @brief Masked version of
 * floor(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_floor(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_floor(src, k, a);
}

/**
 * @brief Zero-masked version of
 * floor(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_floor(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_floor(k, a);
}

/**
 * @brief Masked version of
 * round(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_round(const Vec<T, SIMD_WIDTH> &src,
                                                 const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_round(src, k, a);
}

/**
 * @brief Zero-masked version of
 * round(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_round(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_round(k, a);
}

/**
 * @brief Masked version of
 * truncate(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_truncate(
  const Vec<T, SIMD_WIDTH> &src, const Mask<T, SIMD_WIDTH> &k,
  const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_truncate(src, k, a);
}

/**
 * @brief Zero-masked version of
 * truncate(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_truncate(
  const Mask<T, SIMD_WIDTH> &k, const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_truncate(k, a);
}

/** @} */

/**
 * @addtogroup group_mask_logic
 * @{
 */

/**
 * @brief Masked version of
 * bit_and(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_and(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_and(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * bit_and(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_and(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_and(k, a, b);
}

/**
 * @brief Masked version of
 * bit_or(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_or(const Vec<T, SIMD_WIDTH> &src,
                                              const Mask<T, SIMD_WIDTH> &k,
                                              const Vec<T, SIMD_WIDTH> &a,
                                              const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_or(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * bit_or(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_or(const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_or(k, a, b);
}

/**
 * @brief Masked version of
 * bit_andnot(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_andnot(const Vec<T, SIMD_WIDTH> &src,
                                                  const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_andnot(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * bit_andnot(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_andnot(const Mask<T, SIMD_WIDTH> &k,
                                                   const Vec<T, SIMD_WIDTH> &a,
                                                   const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_andnot(k, a, b);
}

/**
 * @brief Masked version of
 * bit_xor(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_xor(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a,
                                               const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_xor(src, k, a, b);
}

/**
 * @brief Zero-masked version of
 * bit_xor(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_xor(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a,
                                                const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::maskz_xor(k, a, b);
}

/**
 * @brief Masked version of
 * bit_not(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_not(const Vec<T, SIMD_WIDTH> &src,
                                               const Mask<T, SIMD_WIDTH> &k,
                                               const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_not(src, k, a);
}

/**
 * @brief Zero-masked version of
 * bit_not(const Vec<T, SIMD_WIDTH> &).
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_not(const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_not(k, a);
}

/** @} */

/**
 * @addtogroup group_mask_shift
 * @{
 */

/**
 * @brief Masked version of
 * srai(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_srai(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_srai<COUNT>(src, k, a);
}

/**
 * @brief Zero-masked version of
 * srai(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_srai(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_srai<COUNT>(k, a);
}

/**
 * @brief Masked version of
 * srli(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_srli(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_srli<COUNT>(src, k, a);
}

/**
 * @brief Zero-masked version of
 * srli(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_srli(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_srli<COUNT>(k, a);
}

/**
 * @brief Masked version of
 * slli(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_slli(const Vec<T, SIMD_WIDTH> &src,
                                                const Mask<T, SIMD_WIDTH> &k,
                                                const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_slli<COUNT>(src, k, a);
}

/**
 * @brief Zero-masked version of
 * slli(const Vec<T, SIMD_WIDTH> &).
 */
template <size_t COUNT, typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> maskz_slli(const Mask<T, SIMD_WIDTH> &k,
                                                 const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::maskz_slli<COUNT>(k, a);
}

// TODO: add masked versions of sra, srl and sll

/** @} */

/**
 * @addtogroup group_mask_cmp
 * @{
 */

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for less-than ( @c < ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmplt(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmple(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmplt(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmplt(k, a, b);
}

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for less-than-or-equal ( @c <= ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmple(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmplt(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmple(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmple(k, a, b);
}

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for equality ( @c == ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmpeq(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpeq(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpeq(k, a, b);
}

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for greater-than ( @c > ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmpgt(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpge(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpgt(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpgt(k, a, b);
}

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for greater-than-or-equal ( @c >= ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmpge(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpgt(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpge(const Mask<T, SIMD_WIDTH> &k,
                                                  const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpge(k, a, b);
}

/**
 * @brief Masked comparison between corresponding elements of two Vec's
 * for inequality ( @c != ).
 *
 * Bits in the resulting Mask are zeroed out if the corresponding bit in
 * the given Mask is not set.
 *
 * @param k Mask to use for masking the results
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the masked results of the comparisons
 * @sa cmpneq(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpneq(const Vec<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpneq(const Mask<T, SIMD_WIDTH> &k,
                                                   const Vec<T, SIMD_WIDTH> &a,
                                                   const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpneq(k, a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for less-than
 * ( @c < ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmplt(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmplt(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmplt(const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmplt(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * less-than-or-equal ( @c <= ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmple(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmple(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmple(const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmple(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for equality
 * ( @c == ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmpeq(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpeq(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpeq(const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpeq(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for greater-than
 * ( @c > ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmpgt(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpgt(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpgt(const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpgt(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for
 * greater-than-or-equal ( @c >= ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmpge(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpge(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpge(const Vec<T, SIMD_WIDTH> &a,
                                                  const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpge(a, b);
}

/**
 * @brief Compares corresponding elements of two Vec's for inequality
 * ( @c != ).
 *
 * @param a first Vec
 * @param b second Vec
 * @return Mask with the results of the comparisons
 * @sa cmpneq(const Vec<T, SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 * @sa mask_cmpneq(const Mask<T, SIMD_WIDTH> &, const Vec<T,
 * SIMD_WIDTH> &, const Vec<T, SIMD_WIDTH> &)
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Mask<T, SIMD_WIDTH> mask_cmpneq(const Vec<T, SIMD_WIDTH> &a,
                                                   const Vec<T, SIMD_WIDTH> &b)
{
  return internal::mask::mask_cmpneq(a, b);
}

/**
 * @brief Tests if all elements of an Vec are zero, while
 * ignoring elements where the corresponding bit in an Mask is
 * zero.
 *
 * @param k Mask to use for the test
 * @param a Vec to test
 * @return true if all elements (except those ignored by the mask) are zero,
 * false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_zeros(const Mask<T, SIMD_WIDTH> &k,
                                            const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_test_all_zeros(k, a);
}

/**
 * @brief Tests if all bits of all elements of an Vec are one,
 * while ignoring elements where the corresponding bit in an Mask
 * is zero.
 *
 * @param k Mask to use for the test
 * @param a Vec to test
 * @return true if all bits of all elements (except those ignored by the mask)
 * are one, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool mask_test_all_ones(const Mask<T, SIMD_WIDTH> &k,
                                           const Vec<T, SIMD_WIDTH> &a)
{
  return internal::mask::mask_test_all_ones(k, a);
}

/**
 * @brief Selects elements from two Vec's based on a condition
 * Mask.
 *
 * @param cond condition mask
 * @param trueVal Vec to select from if the condition is true
 * @param falseVal Vec to select from if the condition is false
 * @return Vec containing the selected elements
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelse(
  const Mask<T, SIMD_WIDTH> &cond, const Vec<T, SIMD_WIDTH> &trueVal,
  const Vec<T, SIMD_WIDTH> &falseVal)
{
  return internal::mask::mask_ifelse(cond, trueVal, falseVal);
}

/**
 * @brief Selects elements from a Vec and zero based on a
 * condition Mask.
 *
 * @param cond condition mask
 * @param trueVal Vec to select from if the condition is true
 * @return Vec containing the selected elements
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> mask_ifelsezero(
  const Mask<T, SIMD_WIDTH> &cond, const Vec<T, SIMD_WIDTH> &trueVal)
{
  return internal::mask::mask_ifelsezero(cond, trueVal);
}

// 07. Aug 23 (Jonas Keller): added kcmpeq

/**
 * @brief Tests if all bits of two Mask's are equal.
 *
 * @param a first Mask
 * @param b second Mask
 * @return true if all bits of both Mask's are equal, false otherwise
 */
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE bool kcmpeq(const Mask<T, SIMD_WIDTH> &a,
                               const Mask<T, SIMD_WIDTH> &b)
{
  return internal::mask::kcmpeq(a, b);
}

/** @} */
} // namespace simd

#endif // SIMD_VEC_MASK_H_

// aligned memory allocation

// backward compatibility
// ===========================================================================
//
// SIMDBackwardCompat.H --
// aliases for backward compatibility
//
// This source code file is part of the following software:
//
//    - the low-level C++ template SIMD library
//    - the SIMD implementation of the MinWarping and the 2D-Warping methods
//      for local visual homing.
//
// The software is provided based on the accompanying license agreement
// in the file LICENSE or LICENSE.doc. The software is provided "as is"
// without any warranty by the licensor and without any liability of the
// licensor, and the software may not be distributed by the licensee; see
// the license agreement for details.
//
// (C) Jonas Keller, Ralf Möller
//     Computer Engineering
//     Faculty of Technology
//     Bielefeld University
//     www.ti.uni-bielefeld.de
//
// ===========================================================================

// 13. Feb 23 (Jonas Keller): added SIMDBackwardCompat.H for backward
// compatibility of renamed types and functions

// 09. Mar 23 (Jonas Keller): added doxygen documentation

/**
 * @file SIMDBackwardCompat.H
 * @brief Aliases for backward compatibility.
 */

#ifndef SIMD_BACKWARD_COMPAT_H_
#define SIMD_BACKWARD_COMPAT_H_

namespace simd {

/**
 * @addtogroup group_aliases
 * @{
 */

using SIMDByte       = Byte;       ///< Alias for Byte.
using SIMDSignedByte = SignedByte; ///< Alias for SignedByte.
using SIMDWord       = Word;       ///< Alias for Word.
using SIMDShort      = Short;      ///< Alias for Short.
using SIMDInt        = Int;        ///< Alias for Int.
using SIMDFloat      = Float;      ///< Alias for Float.

using SIMDBytePtr       = SIMDByte *;       ///< Pointer to SIMDByte.
using SIMDSignedBytePtr = SIMDSignedByte *; ///< Pointer to SIMDSignedByte.
using SIMDWordPtr       = SIMDWord *;       ///< Pointer to SIMDWord.
using SIMDShortPtr      = SIMDShort *;      ///< Pointer to SIMDShort.
using SIMDIntPtr        = SIMDInt *;        ///< Pointer to SIMDInt.
using SIMDFloatPtr      = SIMDFloat *;      ///< Pointer to SIMDFloat.

/// Alias for Vec. @deprecated Use Vec instead.
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
using SIMDVec = Vec<T, SIMD_WIDTH>;

/// Alias for Vecs. @deprecated Use Vecs instead.
template <size_t NUM, typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
using SIMDVecs = Vecs<NUM, T, SIMD_WIDTH>;

/// Alias for Mask. @deprecated Use Mask instead.
template <typename T, size_t SIMD_WIDTH_DEFAULT_NATIVE>
using SIMDMask = Mask<T, SIMD_WIDTH>;

/// Alias for TypeInfo. @deprecated Use TypeInfo instead.
template <typename T>
using SIMDTypeInfo = TypeInfo<T>;

/// Alias for Format. @deprecated Use Format instead.
template <typename T>
using SIMDFormat = Format<T>;

/// Alias for Decimal. @deprecated Use Decimal instead.
template <typename T>
using SIMDDecimal = Decimal<T>;

/// Alias for NumVecs. @deprecated Use NumVecs instead.
template <typename Tout, typename Tin>
using NumSIMDVecs = NumVecs<Tout, Tin>;

/// Alias for bit_and(). @deprecated Use bit_and() instead.
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> and_(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return bit_and(a, b);
}

/// Alias for bit_or(). @deprecated Use bit_or() instead.
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> or_(const Vec<T, SIMD_WIDTH> &a,
                                          const Vec<T, SIMD_WIDTH> &b)
{
  return bit_or(a, b);
}

/// Alias for bit_xor(). @deprecated Use bit_xor() instead.
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> xor_(const Vec<T, SIMD_WIDTH> &a,
                                           const Vec<T, SIMD_WIDTH> &b)
{
  return bit_xor(a, b);
}

/// Alias for bit_not(). @deprecated Use bit_not() instead.
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE Vec<T, SIMD_WIDTH> not_(const Vec<T, SIMD_WIDTH> &a)
{
  return bit_not(a);
}

/// Alias for extract<0>(). @deprecated Use extract<0>() instead.
template <typename T, size_t SIMD_WIDTH>
static SIMD_INLINE T elem0(const Vec<T, SIMD_WIDTH> &a)
{
  return extract<0>(a);
}

/// Alias for Vec<T, SIMD_WIDTH>::elems. @deprecated Use Vec::elems instead.
template <typename T, size_t SIMD_WIDTH>
static constexpr SIMD_INLINE size_t numSIMDVecElements()
{
  return Vec<T, SIMD_WIDTH>::elems;
}

/// Alias for numInVecs(). @deprecated Use numInVecs() instead.
template <typename Tout, typename Tin>
static constexpr SIMD_INLINE size_t numInputSIMDVecs()
{
  return numInVecs<Tout, Tin>();
}

/// Alias for numOutVecs(). @deprecated Use numOutVecs() instead.
template <typename Tout, typename Tin>
static constexpr SIMD_INLINE size_t numOutputSIMDVecs()
{
  return numOutVecs<Tout, Tin>();
}

/** @} */
} // namespace simd

// some IDEs don't see this if it comes first (compiles, but IDE has problems)
// shifted here:

#ifdef DOXYGEN
// doxygen doesn't understand namespace aliases
/**
 * @ingroup group_aliases
 * @brief Alias for @ref simd.
 * @deprecated Use @ref simd instead.
 */
namespace ns_simd {}
#else
namespace ns_simd = simd;
#endif

#endif

#endif
