\documentclass[a4paper, 11pt]{memoir}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\input{smart-thesis/style}
\input{smart-thesis/common-packages}
\input{smart-thesis/common-macros}

\usepackage{lipsum}
\usepackage[table]{xcolor}
\usepackage{minted}
\usepackage{float}

\usepackage{nicematrix}

% pgfplots preamble
\usetikzlibrary{arrows.meta}
\usetikzlibrary{backgrounds}
\usepgfplotslibrary{patchplots}
\usepgfplotslibrary{fillbetween}
\pgfplotsset{%
    layers/standard/.define layer set={%
        background,axis background,axis grid,axis ticks,axis lines,axis tick labels,pre main,main,axis descriptions,axis foreground%
    }{
        grid style={/pgfplots/on layer=axis grid},%
        tick style={/pgfplots/on layer=axis ticks},%
        axis line style={/pgfplots/on layer=axis lines},%
        label style={/pgfplots/on layer=axis descriptions},%
        legend style={/pgfplots/on layer=axis descriptions},%
        title style={/pgfplots/on layer=axis descriptions},%
        colorbar style={/pgfplots/on layer=axis descriptions},%
        ticklabel style={/pgfplots/on layer=axis tick labels},%
        axis background@ style={/pgfplots/on layer=axis background},%
        3d box foreground style={/pgfplots/on layer=axis foreground},%
    },
}

\addbibresource{main.bib}

\makeglossaries
\input{glossary}

\thesistype{Bachelor Thesis}
\discipline{Computer Science}
\title{Parallel Gaussian Ray Tracing on the CPU}
\author{Sebastian Dawid}
\institution{Bielefeld University,Faculty of Technology,Visual AI for Extended Reality Group}
\supervisors{Prof.\@~Dr.\@~Helge Rhodin,Prof.\@~Dr.-Ing.\@~Ralf M\"oller}

\newcommand*{\erf}{\text{erf}}

\makepagestyle{abs}

% Display the page number in the footer.
\makeevenfoot{abs}{\thepage}{}{}
\makeoddfoot{abs}{}{}{\thepage}

\begin{document}
    \frontmatter
    \smarttitle
    \newpage
    \tableofcontents*

    \clearpage
    \thispagestyle{abs}
    \abstractintoc
    \begin{abstract}
        \lipsum[1]
    \end{abstract}

    \mainmatter
    \chapter{Introduction}
    \begin{itemize}
        \item Gaussian image representations are widely used in CG/CV
        \item ray tracing is more accurate than e.g. splatting â†’  accurate intersections
        \item CPU ray tracing is slow
        \item Some machines have stronger CPUs than GPUs (e.g. integrated GPU only)
    \end{itemize}

    \chapter{Background}
    \label{ch:background}
    There are two Gaussian based image formation models relevant to the contents of this thesis. Here I will discuss both
    Gaussian ray tracing (\ref{sec:int_grt}), the focus of the optimizations described in this thesis, and Gaussian
    splatting (\ref{sec:splatting}). Additionally, I will describe some of the core differences of the methods (\ref{sec:ray_v_splat}).

    \section{Gaussian Ray Tracing}
    \label{sec:int_grt}
    In their 2015 Paper \citetitle{Rhodin:2015} \cite{Rhodin:2015} Rhodin \etal describe a volumetric image formation
    model based on a parametric density representation $D(\mathbf{x})$ defined by a sum of scaled isotropic guassians
    $\mathcal{G} = \{ G_q \}_q$. The density $D$ is then given as
    \begin{align}
        D(\mathbf{x}) &= \sum_{G_q \in \mathcal{G}} G_q(\mathbf{x})
        \label{eq:density}\\
        \text{with} \nonumber\\
        G_q(\mathbf{x}) &= c_q \cdot \exp{\left( - \frac{\Vert\mathbf{x} - \mu_q\Vert_2^2}{2\sigma_q^2} \right)}
        \label{eq:gaussian}
    \end{align}
    where $c_q$ describes the magnitude, $\mu_q$ the center and $\sigma_q$ the standard deviation of the Gaussian $G_q$.
    Additionally, an \gls{albedo} attribute $\mathbf{a}_q$ is defined for each Gaussian to denote its color. This leads
    to the scene representation $\gamma = \{ c_q, \mu_q, \sigma_q, \mathbf{a}_q \}$. The $\gamma$ is omitted from
    $G_q(\mathbf{x})$ for readability and since the parameters are given implicitly via $q$.

    To get the final color of a pixel they determine the amount of light that reaches the camera from each point
    along a ray. For this purpose they determine the \gls{transmittance} $T$ of a point at distance $s$ along a ray from
    a camera position $\mathbf{o}$ in direction $\mathbf{n}$ as
    \begin{equation}
        T(\mathbf{o}, \mathbf{n}, s, \gamma) = \exp{\left( - \int_0^s D(\mathbf{o} + t\mathbf{n}) dt \right)}.
        \label{eq:transmittance}
    \end{equation}

    For the Gaussian density representation the density along a ray $\mathbf{x} = \mathbf{o} + s\mathbf{n}$ through a
    sum of 3D Gaussians is a sum of 1D Gaussians, where the 1D Gaussians are given by inserting the ray into the
    3D Gaussians $G_q$ \eqref{eq:gaussian}. This results in the form
    $\bar{c} \exp{\left( - \frac{(x - \bar{\mu})^2}{2\bar{\sigma}^2} \right)}$ for the 1D scaled Gaussians, with
    $\bar{\mu} = (\mu - \mathbf{o})^T\mathbf{n}$, $\bar{\sigma} = \sigma$ and
    $\bar{c} = c \cdot \exp{\left( - \frac{(\mu - \mathbf{o})^T(\mu - \mathbf{o}) - \bar{\mu}^2}{2\bar{\sigma}^2} \right)}$.

    The \gls{transmittance} can be expressed analytically using the error function
    $\erf{(x)} = \frac{2}{\sqrt{\pi}}\int_0^s \exp{(-t^2)} dt$ and Gaussian form of the density, as
    \begin{equation}
        \begin{aligned}
            T(\mathbf{o}, \mathbf{n}, s, \gamma) &= \exp{\left( -\int_0^s
                \sum_q G_q(\mathbf{o} + t\mathbf{n} ) dt \right)}\\
            &= \exp{\left( \sum_q \frac{\bar{\sigma}_q \bar{c}_q}{\sqrt{\frac{2}{\pi}}}
            \left( \erf{\left( \frac{-\bar{\mu}_q}{\sqrt{2}\bar{\sigma}_q} \right)}
            - \erf{\left( \frac{s - \bar{\mu}_q}{\sqrt{2}\bar{\sigma}_q} \right)} \right) \right)}.\\
        \end{aligned}
        \label{eq:transmittance_analytical}
    \end{equation}

    Assuming the elements in the scene emit an equal amount of \gls{radiance}, a ray is shot through each pixel of a virtual
    \gls{pinhole_camera}. The \gls{radiance} can be computed as the product of \gls{transmittance} $T$ \eqref{eq:transmittance},
    density $D$ \eqref{eq:density}, \gls{albedo} $\mathbf{a}$ and the ambient \gls{radiance} $L_e$ integrated along a ray
    $\mathbf{x} = \mathbf{o} + s\mathbf{n}$. They assume the ambient \gls{radiance} is fixed as $L_e = 1$. As such they
    disregard it in
    \begin{equation}
        L(\mathbf{o}, \mathbf{n}, \gamma) = \int_0^\infty T(\mathbf{o}, \mathbf{n}, s, \gamma)
            \sum_q G_q(\mathbf{o} + s\mathbf{n})\mathbf{a}_q ds.
    \end{equation}
    
    This integral may be approximated with sufficient accuracy by sampling around the mean of each Gaussian $G_q$
    a compact interval $S_q = \{ \bar{\mu}_q + k\lambda_q | k \in K \subset \Z \}$. For their purposes it was sufficient
    to choose $\lambda_q \sim \bar{\sigma}_q$ as the step length, which yields
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_q \mathbf{a}_q \sum_{s \in S_q}
            \lambda_q T(\mathbf{o}, \mathbf{n}, s, \gamma)G_q(\mathbf{o} + s\mathbf{n}).
        \label{eq:radiance}
    \end{equation}

    Rhodin \etal describe that local sampling with $\lambda_q = \bar{\sigma}_q$ and
    $K = \{ -4, -3, \dots, 0 \}$ delivers a good enough approximation.

    \section{Gaussian Splatting}
    \label{sec:splatting}
    %@TODO: describe image formation model (splatting) as laid out in the following papers
    Gaussian splatting is an image formation model first described by \citeauthor{splatting} in \enquote{\citetitle{splatting}}
    (\citeyear{splatting}) \cite{splatting}. The method works, like classic mesh based rasterization methods, by projecting
    Gaussians, ordered by their depth, onto the image plane. Derivatives of the method are still used in computer vision
    and graphics contexts, since its fully differentiable and can be applied in real time contexts.

    One of these derived methods is called 3D Gaussian splatting and was introduced by \citeauthor{kerbl3Dgaussians} in
    their \citeyear{kerbl3Dgaussians} paper \citetitle{kerbl3Dgaussians}\cite{kerbl3Dgaussians}. \citeauthor{splatting}
    define their Gaussians as 4-tuples $(\mu, \Sigma, \mathbf{a}, \alpha)$, where $\mu \in \R^3$ is the mean of the
    Gaussian, $\Sigma \in \R^{3\times 3}$ its 3D covariance Matrix, $\mathbf{a} \in [0, 1]^3$ its albedo and
    $\alpha \in [0, 1]$ its opacity. The camera is defined as a tuple $(W, P)$, where $W$ is the viewing transformation
    and $P$ is the affine projective transformation.

    To project the 3D covariance matrix to a 2D covariance matrix they employ a method described by \citeauthor{volume_splatting}
    in \citetitle{volume_splatting}\cite{volume_splatting}. A covariance matrix
    \begin{equation}
        \Sigma' = JW\Sigma W^TJ^T
    \end{equation}
    in camera coordinates can be computed from the viewing transformation $W$ and the jacobian $J$ of $P$ at $\mu$.
    \citeauthor{kerbl3Dgaussians} cite that \citeauthor{volume_splatting} also show that discarding the third row and
    column yields a 2D covariance matrix $\Sigma_{2D}$.

    %@TODO: This is not part of the paper technically -> how do I cite this?
    %@TODO: Wording seems iffy, should rework this
    At this point the means of the Gaussians are in \gls{ndc} (NDC), to render them they are converted to the corresponding
    screen space coordinates. To ensure correct occlusion the Gaussians are sorted back to front by the $z$ coordinate of
    their means in \gls{ndc}. To generate the final image the Gaussians are blended, using alpha blending, onto the image
    in order. This means that
    \begin{equation}
        \mathbf{c}_{\text{new}} = \mathbf{c}_{\text{old}} \cdot (1 - \alpha) + \mathbf{a} \cdot \exp{\left(-(\mathbf{x} - \bar{\mu})^T\Sigma_{2D}^{-1}(\mathbf{x} - \bar{\mu})\right)} \cdot \alpha
    \end{equation}
    where $\mathbf{c}$ is the new/old pixel color and $\bar{\mu}$ the Gaussian's mean in screen space.

    \section{Ray Tracing vs. Splatting}
    \label{sec:ray_v_splat}
    Gaussian splatting and ray tracing are primarily differentiated by their handling of intersections between Gaussians
    and the presence of perspective effects.

    Due to the nature of the image formation model of Gaussian splatting it can not handle intersecting Gaussians correctly
    since they are always blended onto the image sequentially. Ray tracing solves this problem since the order of the
    Gaussians is irrelevant to the method.

    Since Gaussian splatting relies on an affine projection to project the Gaussians into camera coordinates it can not
    model perspective effects such as distortion at the edges of the field of view. The \gls{pinhole_camera} model that
    Gaussian ray tracing relies on ensures that these perspective effects are present in images generated using ray
    tracing.

    The advantage to Gaussian splatting is speed, since its computational cost is a lot lower than that of ray tracing.
    This allows Gaussian splatting to render scenes with thousands of Gaussians in real time ($>100$ frames per second
    in some of the demo scenes from \cite{kerbl3Dgaussians}). Ray tracing provides more accurate renders at the expense
    of real time performance.

    \chapter{Optimizations}
    \label{ch:optimizations}
    I propose two optimizations to make execution of the method proposed by \citeauthor{Rhodin:2015}
    in \cite{Rhodin:2015} on the CPU possible with greatly reduced runtimes.
    The first optimization is vectorization (\ref{sec:vectorization}) of the method
    parallelizing part of its calculations.
    The second optimization is tiling (\ref{sec:tiling}) which reduces the number of Gaussians
    that have to be processed per pixel.

    \section{Vectorization}
    \label{sec:vectorization}
    Vectorization, describes applying some function or operation usually applied to a scalar value
    to multiple values - a vector - in parallel.
    It is usually applied to reduce the number of operations necessary to finish some task that
    requires the application of some function on a large list of values.

    Take summing up a list $L$ of $N$ values as an example. Usually this requires $N - 1$ additions.
    Applying vectorization with a vector length of $W$ I can divide $L$ into $\lceil \frac{L}{W} \rceil$
    batches of $W$ values each, padding the last one with zeros in case $|L| \mod W \not\equiv 0$. Now I
    can get the same sum by first adding up the batches and then summing up the values in the resulting
    vector. This requires $\lceil \frac{N}{W} \rceil + W - 2 \leq N - 1$ additions.

    \subsection{Notation}
    \label{sec:notation}
    Since the method as it is laid out in \cite{Rhodin:2015} makes use of conventional vectors it is
    necessary to introduce some notation to work with vectorization and ensure it is
    cleanly separated from conventional vector spaces. Vectors used for parallelization
    will be referred to as p-vectors (parallel vectors) from here on. This notation is
    defined in Table~\ref{tab:notation}.
    \begin{table}[b]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|}
            \hline
            Notation & Definition \\
            \hline
            $x^W$ & p-vector containing $W$ elements\\
            $x^W_i$ & $i$-th element of p-vector $x$\\
            $\mathbf{x}^W$ & p-vector containing $W$ vectors $\mathbf{x} \in \R^n$\\
            $\langle \mathbf{x}^W, \mathbf{y}^W \rangle$ & elementwise inner product of the vectors in $\mathbf{x}$ and $\mathbf{y}$\\
            $[ x ]^W$ & $x$ broadcast to a p-vector of $W$ elements\\
            $\odot$ & elementwise multiplication\\
            $\frac{x^W}{y^W}$ & elementwise division\\
            $f^W$ & function that produces a p-vector containing $W$ elements\\\hline
        \end{tabular}
        \caption{Vectorization Notation}
        \label{tab:notation}
    \end{table}

    \subsection{Approaches}
    There are multiple ways to parallelize Gaussian ray tracing using vectorization. To generate the final image it is
    necessary to iterate over two quantities: (1) the Gaussians in the scene and (2) the pixels of the image. Therefore,
    these are the opportunities to parallelize. Parallelism over the pixels is possible by expanding the rendering equations
    given in \cite{Rhodin:2015} to operate on multiple rays at the same time. Parallelism over the Gaussians is possible
    in two ways:
    \begin{enumerate}
        \item The summands of the sum in the \gls{transmittance} equation~\eqref{eq:transmittance_analytical} do not
            depend on each other therefore it is possible to divide the summands into equal groups and process these in
            parallel and add these results up in the end to get the final result.
        \item The same principle can be applied to the outer sum in the \gls{radiance} equation~\eqref{eq:radiance}:
            the summands are not dependent on each other therefore they can be divided into equal groups that can
            be processed in parallel and added up in the end to get the final \gls{radiance}.
    \end{enumerate}
    The inner sum of the \gls{radiance} equation~\eqref{eq:radiance} is not interesting, in terms of parallelism, since
    the number of summands it has is fixed and therefore potential runtime improvements would not scale with the width
    of the p-vectors used, and it is fair to assume that most methods of vectorization will allow for p-vectors wider
    than five elements.

    To enable parallelization over the Gaussians it is necessary to split the set of Gaussians $\mathcal{G}$ into $W$
    disjoint subsets of equal magnitude. Since the magnitude of $\mathcal{G}$ is not guaranteed to be divisible by $W$
    a number of subsets equal to the remainder $|\mathcal{G}| \mod W$ will be augmented with an additional zero element.

    The most sensible division of $\mathcal{G}$ is to assign some order to its elements, such that $g_i, i=1,\dots,|\mathcal{G}|$
    describes the $i$-th element of $\mathcal{G}$. With this $\mathcal{G}$ can be divided into subsets $\mathcal{G}_j$ with
    \begin{equation}
        \mathcal{G}_j := \{ g_i \in \mathcal{G} \,|\, i \mod W \equiv j \} \text{ for } j=0,\dots,W-1.
    \end{equation}

    \paragraph{Parallel \gls{transmittance}:}
    \label{par:parallel_transmittance}
    Given this division I can define a \gls{transmittance} function
    \begin{equation}
        \begin{aligned}
            T^W(\mathbf{o}, \mathbf{n}, s, \gamma) &= \exp^W\left( \sum_{m = 0}^{\left\lceil \frac{|\mathcal{G}|}{W} \right\rceil - 1}
            \begin{pmatrix}
                \frac{\bar{\sigma}_{mW}\bar{c}_{mW}}{\sqrt{\frac{2}{\pi}}} \\ \vdots \\\frac{\bar{\sigma}_{(m+1)W-1}\bar{c}_{(m+1)W-1}}{\sqrt{\frac{2}{\pi}}} 
            \end{pmatrix} \right.\\&\left.\odot \begin{pmatrix}
                \erf{\left( \frac{- \bar{\mu}_{mW}}{\sqrt{2}\bar{\sigma}_{mW}} \right) - \erf{\left( \frac{s - \bar{\mu}_{mW}}{\sqrt{2}\bar{\sigma}_{mW}} \right)}} \\
                \vdots \\
               \erf{\left( \frac{- \bar{\mu}_{(m+1)W - 1}}{\sqrt{2}\bar{\sigma}_{(m+1)W - 1}} \right) - \erf{\left( \frac{s - \bar{\mu}_{(m+1)W - 1}}{\sqrt{2}\bar{\sigma}_{(m+1)W - 1}} \right)}} 
            \end{pmatrix}\right)
        \end{aligned}
        \label{eq:transmittance_parallel}
    \end{equation}
    that operates on the elements of the subsets $\mathcal{G}_j$ in parallel.

    Note that the separation of the Gaussians into the subsets $\mathcal{G}_j$ is done implicitly through the index $m$ of the sum.

    Now I replace the call to the \gls{transmittance} function in the \gls{radiance} equation~\eqref{eq:radiance} with the sum
    over the results of the parallel \gls{transmittance} equation~\eqref{eq:transmittance_parallel} yielding
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_{q} \mathbf{a}_q \sum_{s \in S_q} \lambda_qG_q(\mathbf{o}
        + s\mathbf{n})\sum_{i = 1}^W T^W_i(\mathbf{o}, \mathbf{n}, s, \gamma).
    \end{equation}

    Since the subsets $\mathcal{G}_j$ are disjoint by definition this version of the \gls{radiance} equation yields the
    same results as the original (Eq.~\eqref{eq:radiance})

    \paragraph{Parallel \gls{radiance}:}
    \label{par:parallel_radiance}
    The analytical solution to the \gls{transmittance} integral from Eq.~\eqref{eq:transmittance_analytical} can be broadcast
    to
    \begin{equation}
        \begin{aligned}
            T^W(\mathbf{o}^W, \mathbf{n}^W, s^W, \gamma) = \exp^W\Bigg(& \sum_q \frac{(\bar{\sigma}_q)^W
            (\bar{c}_q)^W}{\left[ \sqrt{\frac{2}{\pi}} \right]^W} \\
            \odot \Bigg(& \erf^W{\left( \frac{-(\bar{\mu}_q)^W}{[ \sqrt{2} ]^W \odot (\bar{\sigma}_q)^W} \right)}\\
            &- \erf^W{\left( \frac{s^W - (\bar{\mu}_q)^W}{[ \sqrt{2} ]^W \odot (\bar{\sigma}_q)^W} \right)} \Bigg) \Bigg)
        \end{aligned}
        \label{eq:transmittance_broadcast}
    \end{equation}
    operating on p-vectors of points on rays parameterized by origins $\mathbf{o}^W$, directions $\mathbf{n}^W$
    and distances $s^W$ with
    \begin{align*}
        \bar{\mu}^W &= \left\langle [ \mu ]^W - \mathbf{o}, \mathbf{n}^W \right\rangle, \bar{\sigma}^W = \left[ \sigma \right]^W\\
        \bar{c}^W &= [c]^W \odot \exp^W{\left( - \frac{\left\langle [\mu]^W - \mathbf{o}^W, [\mu]^W - \mathbf{o}^W \right\rangle
    - \left(\bar{\mu}^W\right)^2}{[2]^W \odot \left(\bar{\sigma}^W\right)^2} \right)}.
    \end{align*}

    This version of the \gls{transmittance} equation \eqref{eq:transmittance_analytical} allows me to rewrite the
    \gls{radiance} equation \eqref{eq:radiance} to operate on multiple Gaussians at the same time in the same way it
    was done in the parallel \gls{transmittance} equation \eqref{eq:transmittance_parallel}, yielding the parallel \gls{radiance}
    \begin{equation}
        \begin{aligned}
            \hat{L}^W(\mathbf{o}, \mathbf{n}, \gamma) &= \sum_{m = 0}^{\left\lceil \frac{|\mathcal{G}|}{W} \right\rceil - 1} \left( \begin{pmatrix}
                \mathbf{a}_{mW}\\ \vdots \\ \mathbf{a}_{(m+1)W - 1}
            \end{pmatrix} \right.\\
            &\odot \sum_{s^W \in S_m} \left( T^W([\mathbf{o}]^W, [\mathbf{n}]^W, s^w, \gamma)\right.\\
            &\odot \left.\left.\begin{pmatrix}
                G_{mW}(\mathbf{o} + s^W_1\mathbf{n})\\ \vdots\\ G_{(m+1)W - 1}(\mathbf{o} + s^W_W\mathbf{n})
            \end{pmatrix}\right)\right)
        \end{aligned}
        \label{eq:radiance_parallel_gaussians}
    \end{equation}
    with
    \[ S_m = \left\{\left. \begin{pmatrix}
        \bar{\mu}_{mW}\\ \vdots\\ \bar{\mu}_{(m+1)W - 1}
    \end{pmatrix} + [k]^W \odot \begin{pmatrix}
        \lambda_{mW}\\ \vdots\\ \lambda_{(m+1)W - 1}
    \end{pmatrix} \,\right|\, k \in K \subset \Z \right\} \]
    and $\bar{\mu}$, $\bar{\sigma}$ and $\bar{c}$ defined the same as in Section~\ref{sec:int_grt}.

    Finally, we can collect these results into the \gls{radiance}
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_{i = 1}^W \hat{L}^W_i(\mathbf{o}, \mathbf{n}, \gamma).
        \label{eq:radiance_parallel_final}
    \end{equation}

    \paragraph{Parallel Pixels:}
    \label{par:parallel_pixels}
    Given the broadcast version of the \gls{transmittance} equation \eqref{eq:transmittance_broadcast} from before I
    can broadcast the \gls{radiance} equation to
    \begin{equation}
        \begin{aligned}
            \hat{L}^W(\mathbf{o}^W, \mathbf{n}^W, \gamma) &= \sum_q [ \mathbf{a}_q ]^W \sum_{s \in S_q} \Big(
            [ \lambda_q ]^W \odot T^W(\mathbf{o}^W, \mathbf{n}^W, [ s ]^W, \gamma)\\
            &\odot G_q^W(\mathbf{o}^W + [ s ]^W \odot \mathbf{n}^W) \Big)
        \end{aligned}
        \label{eq:radiance_parallel_pixels}
    \end{equation}
    calculating the \gls{radiance} for multiple pixels at once.

    Note that both the \gls{transmittance} and \gls{radiance} equations still operate on the Gaussians one by one as the
    sums iterate over every Gaussian $G_q \in \mathcal{G}$ individually.

    \section{Tiling}
    \label{sec:tiling}
    In most scenes most pixels will be unaffected by most Gaussians, therefore it is not sensible to perform
    the per pixel calculations for all Gaussians instead of just the Gaussians that could affect the pixel.
    Reducing the number of Gaussians considered when dealing with any individual pixel is a good way to reduce
    time wasted performing unnecessary computations.

    \begin{figure}[t]
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tikzpicture}
                \draw[gray, thin] (0, 0) grid[xstep=4, ystep=2.25] (16, 9);

                \fill[purple] (12, 5.5) circle (0.1);
                \draw[purple, thick] (12, 5.5) circle (3);

                \fill[red] (2.7, 1.8) circle (0.1);
                \draw[red, thick] (2.7, 1.8) circle (1.7);
                
                \fill[green] (4, 3) circle (0.1);
                \draw[green, thick] (4, 3) circle (1.5);
                
                \fill[blue] (5, 2) circle (0.1);
                \draw[blue, thick] (5, 2) circle (1);
            \end{tikzpicture}
        }
        \caption{Image of four Gaussians split into 16 tiles.}
        \label{fig:tiling}
    \end{figure}

    One way of achieving this goal is to split the final image into $N \times N$ tiles (\eg Figure~\ref{fig:tiling})
    and determining which Gaussians are relevant to which tiles through spacial proximity. Since it is not clear where
    in the image any one Gaussian is going to end up before generating the image it is necessary to estimate location
    and size of the Gaussians before generating the final image through ray tracing.

    Tiling is technique that is not unique to ray tracing and has been used in various other rendering methods such as
    the rasterizer used by \citeauthor{kerbl3Dgaussians} in \cite{kerbl3Dgaussians}.

    To estimate size and position of the Gaussians in the final image the guassians are projected to the image plane
    using scaled orthographic projection. Let $V \in \R^{4 \times 4}$ be the view matrix of the camera. Then the
    projected guassian $\bar{g} = (\bar{\mu}, \bar{\sigma})$ to a Gaussian $g = (\mu, \sigma)$ is calculated as

    \begin{align}
        \mu_p &= V \cdot (\mu, 1)^T\\
        \bar{\mu} &= \frac{1}{\mu_{p3}} \cdot (\mu_{p1}, \mu_{p2})^T \label{eq:tiling_projection}\\
        \text{and}\nonumber\\
        \bar{\sigma} &= \frac{\sigma}{\mu_{p3}}.
        \label{eq:tiling_scale}
    \end{align}

    Note that this method of projection is a simplified version of the projection used in \cite{kerbl3Dgaussians},
    since I only consider isotropic Gaussians which removes the complexity of projecting the covariance matrix while
    retaining its properties. As in \cite{Rhodin:2015} Gaussians with $\bar{\sigma} < 10^{-5}$ are discarded, since
    they are unlikely to contribute to the final image significantly. Also note that after the projection the image
    coordinates are mapped to $[-1, 1] \times [-1, 1]$.

    Let $tw, th \in \R$ be the width and height of a tile and let $\mathbf{c} \in \R^2$ denote the center of the tile.
    The vector $\mathbf{p} = (|\mathbf{c}_1 - \bar{\mu}_1|, |\mathbf{c}_2 - \bar{\mu}_2|)^T$ describes the absolute
    distances along the $x$ and $y$ Axes between the center of the tile $\mathbf{c}$ and mean of the projected
    Gaussian $\bar{\mu}$. A Gaussian counts as relevant to a tile if
    \begin{equation}
        \mathbf{p}_1 < |\mathbf{c}_1| + \frac{tw}{2} + 3.3\cdot\bar{\sigma}
        \label{eq:tiling_cond_1}
    \end{equation}
    and 
    \begin{equation}
        \mathbf{p}_2 < |\mathbf{c}_2| + \frac{th}{2} + 3.3\cdot\bar{\sigma}
        \label{eq:tiling_cond_2}
    \end{equation}
    meaning that the Gaussian has to be in a $3.3 \cdot \bar{\sigma}$ range from the borders of the tile to be
    considered relevant. The $|\mathbf{c}_i|, i=1,2$ terms are added to account for stronger perspective distortion
    towards the edges of the image.

    The $3.3 \cdot \sigma$ can be derived from
    \begin{align}
        G(\mu + k \cdot \sigma) &= c \cdot \exp{\left(-\frac{|k \sigma|^2}{2\sigma^2}\right)}\\
        &= c \cdot \exp{\left(-\frac{k^2}{2}\right)}\\
        \overset{\ln}{\iff} \ln{\left( G(\mu + k\cdot\sigma) \right)} &= \ln{(c)} - \frac{k^2}{2}\\
        \iff k &= \sqrt{2\left(\ln{(c)} - \ln{\left( G(\mu + k\cdot\sigma) \right)}\right)}
    \end{align}
    yielding
    \begin{equation}
        k(c) = \sqrt{2(\ln{(c)} - \ln{(\varepsilon)})}
        \label{eq:sig_lim}
    \end{equation}
    with $\varepsilon = \frac{1}{255}$ the last density value that can result in a visible color and $c$ as the
    magnitude. Assuming the magnitude of the Gaussians to be in a reasonable range such as $[0, 10]$ using $k(1) =
    \sqrt{2(\ln{(1)} - \ln{(\frac{1}{255})})} = 3.3$ is a sufficient approximation since $k(10) = 3.9$. Using values
    smaller than $k(1) = 3.3$, such as $2$, can lead to visible tiling artifacts for magnitudes in the given range.

    \chapter{Approximations}
    \label{ch:approximations}
    %@TODO: Explain why it is necessary to approximate functions / implement own approximations
    Gaussian ray tracing relies on the exponential and error functions. Usually standard implementations of these
    functions are optimized for accuracy first and speed second and are not easily vectorized. In the case of ray
    tracing accuracy is only important to a certain degree since the visual artifacts caused by errors smaller than
    $10^{-3}$ are unlikely to be noticeable.

    Therefore, it is worthwhile to investigate which approximations give the best balance between accuracy, speed and
    potential to be vectorized.

    The implementation of the approximations will be discussed in Section~\ref{sec:impl_approx}.

    \section{Fast Exponential Function}
    \label{sec:fast_exp}
    The approximation of the exponential function described by \citeauthor{fast_exp} in \citetitle{fast_exp}
    \cite{fast_exp} is based on the fact that
    \begin{equation}
        \exp{(x)} = 2^{\frac{x}{\ln{(2)}}}
        \label{eq:exp_ident}
    \end{equation}
    is an identity of the exponential function. The rest of the approximation relies on the structure of floating point
    numbers under the IEEE-754 standard and will be discussed in Section~\ref{sec:impl_fast_exp}.
    
    \section{Polynomial Approximations}
    \label{sec:poly_approx}
    Many non-polynomial functions can be approximated locally using polynomial functions. There are various methods of
    finding these approximations among them cubic spline interpolation (\ref{sec:cubic_spline}) and the taylor series
    (\ref{sec:taylor}).

    \subsection{Cubic Spline Interpolation}
    \label{sec:cubic_spline}
    Given the nodes $(x_i, y_i)$ of a decomposition $a = x_0 < \cdots < x_n = b$ a function
    \begin{align}
        \begin{aligned}
            S_n \in S_n^{(k, r)}[a, b] = \{ &P \in C^r([a, b]),\\&P|_{[x_{i - 1}, x_i]} \in \mathcal{P}_k([x_{i - 1}, x_i]), i=1,\dots,n \}
        \end{aligned}
    \end{align}
    is called a spline and
    \begin{equation}
        h_i = x_i - x_{i-1}
    \end{equation}
    for $i=1,\dots,n$ the step widths. If $S_n \in S_n^{(3, 2)}([a, b])$ it is called a cubic spline. A natural cubic spline is defined by
    the additional condition $S_n''(a) = S_n''(b) = 0$.

    Let
    \begin{equation}
        S_n|_{[x_{i-1}, x_i]} = P_i \in \mathcal{P}_3
    \end{equation}
    and
    \begin{equation}
        P_i(x) = a_{0,i} + a_{1,i}(x - x_i) + a_{2,i}(x - x_i)^2 + a_{3,i}(x - x_i)^3,
    \end{equation}
    with $x\in[x_{i-1}, x_i]$ for $i=1,\dots,n$.

    I can determine the coefficients
    \begin{align}
        a_{0, i} &= y_i\\
        a_{1, i} &= \frac{y_i - y_{i+1}}{h_i} + a_{2,i}h_i - a_{3,i}h_i^2\\
        a_{2, i} &= \begin{cases}
            0 & i = n\\
            (A^{-1}b)_i & \text{else}
        \end{cases}\label{eq:cubic_spline_a2}\\
        a_{3, i} &= \frac{a_{2,i} - a_{2,i-1}}{3h_i}
    \end{align}
    for $i=1,\dots,n$.
    For the computation of \eqref{eq:cubic_spline_a2} I additionally need
    \begin{equation}
        b_i = 3\left(\frac{y_{i+1} - y_i}{h_{i+1}} - \frac{y_i - y_{i-1}}{h_i}\right)
    \end{equation}
    for $i=1,\dots,n-1$ and
    \begin{equation}
        A = \begin{bNiceMatrix}
            2(h_1 + h_2) & h_2          & \Cdots & 0\\
            h_2          & 2(h_2 + h_3) & h_3     & \Vdots\\
            \Vdots       &              &         & \\
                         &              &         & \\
                         & h_{n-2} & 2(h_{n-2} - h_{n-1}) & h_{n-1}\\
            0            & \Cdots       & h_{n-1}              & 2(h_{n-1} - h_n)\\
        \end{bNiceMatrix}
    \end{equation}

    This method can be applied to compute natural cubic splines for both the error and exponential functions.

    \subsection{Taylor Series}
    \label{sec:taylor}
    The Taylor series
    \begin{equation}
        \sum_{i = 0}^\infty \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n
        \label{eq:taylor}
    \end{equation}
    can be used to approximate the error and exponential functions. When evaluated at $x_0 = 0$ it can give a good
    approximation
    \begin{equation}
        \erf{(x)} = \frac{2}{\sqrt{\pi}} \sum_{n = 0}^\infty \frac{x}{2n + 1} \prod_{k = 1}^n \frac{-x^2}{k}
        \label{eq:taylor_erf}
    \end{equation}
    of the error function around $x = 0$. For the approximation to be usable on the
    relevant interval of $[-2, 2]$ it needs to include at least ten terms of the series as seen in Figure~\ref{fig:taylor_erf}.

    \begin{figure}[t]
        \centering
        \input{plots/taylor_erf}
        \caption{Taylor approximation of the error function with 4, 8, 10 and 16 terms.}
        \label{fig:taylor_erf}
    \end{figure}

    The exponential function can be approximated using its taylor expansion on its own too, but it would require too
    many terms to be reasonable.

    \subsection{Abramowitz and Stegun}
    \label{sec:abrasteg}
    \citeauthor{AbraSteg72} give a polynomial approximation of the error function in Section 7.1.27 of their
    \enquote{\citetitle{AbraSteg72}}\cite{AbraSteg72} as
    \begin{equation}
        \erf{(x)} = 1 - \frac{1}{(1 + ax + bx^2 + cx^3 + dx^4)^4}
        \label{eq:abrasteg_positive}
    \end{equation}
    with
    \begin{equation}
        \begin{aligned}
            a &= 0.278393,\,
            b = 0.230289\\
            c &= 0.000972,\,
            d = 0.078108
        \end{aligned}
        \label{eq:abrasteg_defs}
    \end{equation}
    for $x \in [0, \infty)$. To extend the approximation for $x \in \R$ I exploit the point symmetry of the error function
    with respect to $x = 0$. This yields
    \begin{equation}
        \erf{(x)} = \begin{cases}
            1 - \frac{1}{(1 + ax + bx^2 + cx^3 + dx^4)^4} & x \in [0, \infty)\\
            \erf{(-x)} & x \in (-\infty, 0)
        \end{cases}
        \label{eq:abrasteg}
    \end{equation}
    with $a$, $b$, $c$ and $d$ the same as before. How these factors were determined is unclear since they give the
    approximation without an explanation as to how they derived it.
    
    \chapter{Implementation}
    \label{ch:implementation}
    %@TODO: details TBD
    I implemented Gaussian ray tracing as described in Section~\ref{sec:int_grt} with the optimizations described in
    Chapter~\ref{ch:optimizations} in C++. For this I utilized the \gls{simd} instructions provided by modern x64 CPUs
    (\ref{sec:simd}) to implement vectorization (\ref{sec:vectorization}) along with a thread pool
    (\ref{sec:thread_pool}) to further increase the performance gained from the tiling (\ref{sec:tiling}).

    The Implementation is split into two parts: a library that implements the rendering functions, tiling and approximations
    (\ref{sec:library}) and a renderer that uses the library to generate images (\ref{sec:renderer}).

    Note that I will use aliased type names such as \mintinline{c++}{f32} instead of \mintinline{c++}{float} to keep the
    text consistent with the code base. For a full list of the type aliases used see Listing~\ref{lst:type_aliases}.

    The source code of the implementation is available at: \\
    \href{https://github.com/Sebastian-Dawid/simd-gaussian-ray-tracing}{https://github.com/Sebastian-Dawid/simd-gaussian-ray-tracing}
    \footnote{Or mirrored at \href{https://github.com/vaixr/_simd-gaussian-raytracing}{https://github.com/vaixr/\_simd-gaussian-raytracing}}

    \section{SIMD}
    \label{sec:simd}
    \gls{simd} (Single Instruction Multiple Data) is a type of parallelism available in most modern x64 and ARM CPUs. This
    form of parallelism is made available in the form of CPU instructions that operate on special vector registers that
    are characterized by their width. For example a x64 CPU might provide registers that are 256 bits wide instead of
    the usual register width of 64 bits on x64 CPUs.

    Which instructions and which register width a CPU supports depend on which extensions to the instruction set are
    available on that CPU.

    \gls{simd} instructions are usable from C/C++ code through intrinsics. Which intrinsics are available based on the
    available instruction set extensions can be referenced in the
    \href{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}{Intel Intrinsics
    Guide}\footnote{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}. Using instrinsics
    directly has the disadvantage that the code is tied to the extension that makes those intrinsics available.
    Therefore, I opted to use a wrapper library (\ref{sec:tsimd}) to avoid direct usage of intrinsics.

    \subsection{Vectorization to SIMD}
    \label{sec:vectorization_to_simd}
    \begin{itemize}
        \item most conversion are 1 to 1
        \item p-vectors of vectors are implemented as vectors of p-vectors
    \end{itemize}

    \subsection{T-SIMD}
    \label{sec:tsimd}
    A simple way to make the code compatible with different \gls{simd} extensions is the T-SIMD library described in
    \enquote{\citetitle{own_moeller_16_2}} \cite{own_moeller_16_2} by \citeauthor{own_moeller_16_2}.
    \begin{itemize}
        \item Vector class â†’  Template determines width in bytes
        \item Intrinsics are wrapped in template functions â†’  intrinsic used based on width template parameter
        \item Which widths are instantiated is based on the vector extensions detected by the compiler (\mintinline{sh}{gcc -march=native}) or set via flags (\mintinline{sh}{gcc -march=znver2} or \mintinline{sh}{gcc -mavx2})
        \item Only available widths are instantiated
        \item Missing instructions are emulated (e.g. extension not available)
    \end{itemize}

    \subsubsection{Sequential to T-SIMD}
    \label{sec:seq_to_tsimd}
    \begin{itemize}
        \item Most arithmetic conversions are one to one â†’  operator overloading in T-SIMD
        \item \mintinline{c++}{if () {} else {}} can be done via \mintinline{c++}{simd::ifelse} â†’  a bit more restrictive (no code blocks, lambdas with returns as a workaround maybe? wasn't relevant here)
        \item scalar values via \mintinline{c++}{simd::set1}
        \item reciprocal and square root are available as instructions â†’  available in T-SIMD as \mintinline{c++}{simd::rcp} and \mintinline{c++}{simd::sqrt}
        \item Sum over vector \mintinline{c++}{simd::hadds}
    \end{itemize}

    \section{Library}
    \label{sec:library}

    %@TODO: Description of the functionality of the library

    \subsection{Approximations}
    \label{sec:impl_approx}
    I implemented the approximations described in Chapter~\ref{ch:approximations} and provide wrappers for
    approximations from Intel's Short Vector Math Library (\ref{sec:svml}) and Agner Fogs Vector Class Library
    (\ref{sec:vcl}).

    For the sake of readability I will only embed the sequential implementations of the approximations in the text. The
    versions of the functions parallelized using T-SIMD can be found in Appendix~\ref{ch:listings}.

    \subsubsection{Fast Exponential Function}
    \label{sec:impl_fast_exp}
    As described in Section~\ref{sec:fast_exp} this approximation of the exponential function relies on the structure of
    floating point numbers defined by the IEEE-754 standard.

    The IEEE-754 standard defines a double precision floating point number to consist of a sign bit $s \in \{ 0, 1 \}$,
    a 52-bit mantissa $m \in [0, 1)$ and an 11-bit exponent $x$ and is calculated as
    \begin{equation}
        (-1)^s (1 + m) 2^{x - x_0}
        \label{eq:def_ieee754}
    \end{equation}
    with $x_0 = 1023$ as a bias term.

    Note that \citeauthor{fast_exp} uses two 32-bit integers $i$ and $j$ to represent a 64-bit float, but the
    approximation does not rely on this decomposition. As such I will describe it using a single 64-bit integer $i$ instead.

    Given the identity \eqref{eq:exp_ident} of the exponential function it is sufficient to find an approximation for
    $2^x$. \citeauthor{fast_exp} gives
    \begin{equation}
        2^{52}(x + 1023)
    \end{equation}
    as an exact approximation as long as $x$ is an integer. For non integer arguments $x$ he notes that the spillover into
    the most significant bits of the mantissa provides a linear interpolation between $\lfloor x \rfloor$ and $\lceil x
    \rceil$ under IEEE-754.

    The final approximation of $\exp{(x)}$ can now be computed as
    \begin{equation}
        \frac{2^{52}}{\ln{(2)}} x + 2^{52}(1023 - c)
    \end{equation}
    where $c$ is an adjustment term that provides some control over the approximation.

    Since I use single precision floating point numbers instead of double precision floats the version of the
    approximation I use
    \begin{equation}
        \frac{2^{23}}{\ln{(2)}}x + 2^{23}(127 - c)
    \end{equation}
    instead. The exponent has changed from $52$ to $23$ due to the difference in mantissa and exponent length in single and
    double precision floats. The $1023$ has been replaced with $127$ since it is the single precision bias.

    \citeauthor{fast_exp} derives that $c = 0.043677448$ minimizes the maximum relative error to range from $-2.98\%$ to
    $2.98\%$. The implementation of the method shown in Listing~\ref{lst:fast_exp_impl} is based on the implementation
    by Johan Rade \footnote{\href{https://gist.github.com/jrade/293a73f89dfef51da6522428c857802d}{https://gist.github.com/jrade/293a73f89dfef51da6522428c857802d}}
    that foregoes the usage of a \mintinline{c}{union} to realize the reinterpretation and uses \mintinline{c}{memcpy}
    instead. In my implementation I opted to use \mintinline{c++}{std::bit_cast} and discard the bounds checking
    introduced by Rade since my usage should never leave the given bounds.

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
constexpr f32 a = (1 << 23) / std::numbers::ln2_v<f32>;
constexpr f32 b = (1 << 23) * (127 - 0.043677448f);
f32 fast_exp(f32 x)
{
    x = a*x + b;
    u32 n = static_cast<u32>(x);
    return std::bit_cast<f32>(n);
}
        \end{minted}
        \caption{Fast Exponential Function Implementations}
        \label{lst:fast_exp_impl}
    \end{listing}
    
    \subsubsection{Cubic Spline Interpolation}
    \label{sec:impl_cubic_spline}
    An efficient implementation of a cubic spline first requires an efficient method of evaluating polynomials. A way to
    evaluate polynomials efficiently is Horner's method
    \begin{equation}
        p(x) = \left(\left(\cdots\left(a_nx + a_{n-1}\right)x + \cdots a_2\right)x + a_1\right)x + a_0
        \label{eq:horners_method}
    \end{equation}
    which avoids the need to evaluate multiple $x^n$ terms.
    %@TODO: should I mention wasted pipelining opportunities here?

    Applying the method for determining a cubic spline described in Section~\ref{sec:cubic_spline}, and writing the
    corresponding code, for decompositions consisting of many intervals, by hand is not feasible. Therefore I implemented 
    the algorithm for determining the coefficients (see Listing~\ref{lst:jl_cubic_spline}) along with two functions for
    generating sequential (Listing~\ref{lst:jl_code_gen}) and \gls{simd} (Listing~\ref{lst:jl_code_gen_simd}) code.
    
    The generated code switches over the polynomials $P_i$ based on the passed parameter. The corresponding polynomial
    $P_i$ is then evaluated using Horner's method \eqref{eq:horners_method}.

    For the error function I can additionally expoit its symmetry to halve the number of cases that need to be checked.

    \subsubsection{Taylor Series and Abramowitz and Stegun}
    \label{sec:impl_abrasteg_taylor}
    Polynomial approximations of the error function as described by \citeauthor{AbraSteg72} (see
    Section~\ref{sec:abrasteg}) and the taylor series (see Section~\ref{sec:taylor}) could be implemented trivially (see
    Listing~\ref{lst:trivial_abrasteg}), this however is an inefficient way of evaluating polynomials. As with the cubic
    spline interpolation above it is better to use Horner's method \eqref{eq:horners_method} to evaluate the polynomial
    as shown in Listing~\ref{lst:horner_abrasteg} and Listing~\ref{lst:horner_taylor}.

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
f32 abramowitz_stegun_erf(f32 x)
{
    const f32 sign = (x >= 0) - (x < 0);
    x *= sign;
    const f32 denom = 1.f + a*x + b*x*x
            + c*x*x*x + d*x*x*x*x;
    const f32 val = (1.f - 1.f/(denom*denom*denom*denom));
    return sign * val;
}
        \end{minted}
        \caption{Trivial implementation of approximation of the error function in Eq.~\refeq{eq:abrasteg}.}
        \label{lst:trivial_abrasteg}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
f32 abramowitz_stegun_erf(f32 x)
{
    const f32 sign = (x >= 0) - (x < 0);
    x *= sign;
    const f32 denom = (((d*x + c)*x + b)*x + a)*x + 1.f;
    const f32 denom2 = denom * denom;
    const f32 val = (1.f - 1.f/(denom2*denom2));
    return sign * val;
}
        \end{minted}
        \caption{Implementation of the approximation of the error function in Eq.~\refeq{eq:abrasteg} using Horner's method.}
        \label{lst:horner_abrasteg}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
constexpr f32 ts[] = { ... };
f32 taylor_erf(const f32 x)
{
    if (-2.f >= x) return -1.f;
    if (x >= 2.f) return 1.f;
    return sign * 2.f * std::numbers::inv_sqrtpi_v<f32>
            * ((((((((((ts[9])*x*x + ts[8])*x*x
            + ts[7])*x*x + ts[6])*x*x + ts[5])*x*x
            + ts[4])*x*x + ts[3])*x*x + ts[2])*x*x
            + ts[1])*x*x + ts[0])*x;
}
        \end{minted}
        \caption{Implementation of the approximation of the error function using ten terms of its taylor series \eqref{eq:taylor_erf} at $0$ (\mintinline{c++}{ts}).}
        \label{lst:horner_taylor}
    \end{listing}

    \subsubsection{Short Vector Math Library (SVML)}
    \label{sec:svml}
    Intel's SVML provides implementations for both the exponential and error functions. Since the library is closed
    source I have no way of knowing how these approximations were implemented. I export the symbols for the AVX2
    (\mintinline{c}{__svml_expf8}) and AVX512 (\mintinline{c}{__svml_expf16}) versions of the implementations as part of
    the \mintinline{c++}{approx} namespace along with a template wrapper function that is instantiated for the vector
    widths available (see Listing~\ref{lst:template_svml_exp}).

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
template<size_t SIMD_WIDTH>
inline simd::Vec<simd::Float, SIMD_WIDTH>
svml_exp(simd::Vec<simd::Float, SIMD_WIDTH> x);

#ifdef __AVX2__
template<>
inline simd::Vec<simd::Float, 32>
svml_exp(simd::Vec<simd::Float, 32> x);
{
    return __svml_expf8(x);
}
#endif

#ifdef __AVX512F__
template<>
inline simd::Vec<simd::Float, 64>
svml_exp(simd::Vec<simd::Float, 64> x);
{
    return __svml_expf16(x);
}
#endif
        \end{minted}
        \caption{Template wrapper for SVML's exponential function.}
        \label{lst:template_svml_exp}
    \end{listing}

    \subsubsection{Vector Class Library (VCL)}
    \label{sec:vcl}
    The approximation of the exponential function provided by VCL is based on the IEEE-754 single precision floating
    point format and the Taylor series \eqref{eq:taylor} of the exponential function
    \begin{equation}
        \exp{(x)} = \sum_{n = 0}^\infty \frac{x^n}{n!}.
    \end{equation}
    I provide wrapped versions of the approximation in
    the same way shown in Listing~\ref{lst:template_svml_exp} to deal with the conversion between the T-SIMD and VCL
    vector types (see Listing~\ref{lst:vcl_exp}).

    \subsection{Thread Pool}
    \label{sec:thread_pool}
    As part of the library I implemented a simple thread pool to provide multithreaded versions of the rendering
    functions (\ref{sec:rendering_functions}). The pool provides a queue into which tasks are submitted. When a new task
    is submitted a waiting thread of the pool, if available, is notified. All interactions with the queue are protected
    through a scoped \mintinline{c++}{std::unique_lock<std::mutex>}.

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
struct thread_pool_t
{
    std::contion_variable cond;
    std::mutex queue_mutex;
    std::queue<std::function<void()>> tasks;
    std::vector<std::thread> threads;
    bool stopped = false;

    thread_pool_t(size_t thread_count);
    void enqueue(std::function<void()> task);
    ~thread_pool_t();
};
        \end{minted}
        \caption{Interface of the thread pool}
        \label{lst:thread_pool_interface}
    \end{listing}

    The pool implements the interface shown in Listing~\ref{lst:thread_pool_interface}.
    The pool is initialized with \mintinline{c++}{thread_cont} threads. The threads are joined when the pool's
    destructor is called either through the end of the scope when used as a stack object or smart pointer or the
    \mintinline{c++}{delete} operator in case of a heap object.

    The threads execute the following steps: (1) claim the pool's mutex, (2) wait for the condition variable to be
    signaled if no tasks are available and the pool has not been stopped, (3) exit if the pool has been stopped and
    there are no more tasks in the queue, (4) dequeue a task from the queue, (5) release the mutex, (6) execute the task
    and return to (1).

%     \begin{listing}[H]
%         \begin{minted}[linenos, frame=lines]{c++}
% while (true)
% {
%     std::function<void()> task;
%     {
%         std::unique_lock<std::mutex> lock(queue_mutex);
%         cond.wait(lock, [this](){
%             return !tasks.empty() || stopped; });
%         if (stopped && tasks.empty()) return;
%         task = std::move(tasks.front());
%         tasks.pop();
%     }
%     task();
% }
%         \end{minted}
%         \caption{Code executed by threads in pool}
%         \label{lst:thread_code}
%     \end{listing}

    \subsection{Rendering Functions}
    \label{sec:rendering_functions}

    \subsubsection{Types}
    The implementation relies on various types representing the mathematical objects used for Gaussian ray tracing. They
    are listed in Listing~\ref{lst:types_vrt}. Since most of the functions implemented on the structures are not essential
    to the implementation I have omitted them here for the sake of brevity. The full interfaces are shown in Listing~\ref{lst:types_vrt_full}.
    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
struct vec4f_t { f32 x, y, z, w; };
struct simd_vec4f_t { simd::Vec<simd::Float> x, y, z, w; };
struct gaussian_t {
    vec4f_t albedo, mu;
    f32 sigma, magnitude;
};
struct simd_gaussian_t {
    simd_vec4f_t albedo, mu;
    simd::Vec<simd::Float> sigma, magnitude;
};
struct gaussian_vec_t {
    // all pointers aligned to SIMD_WIDTH
    struct { f32 *r, *g, *b, *a; } albedo;
    struct { f32 *x, *y, *z, *w; } mu;
    f32 *sigma;
    f32 *magnitude;
    u64 size = 0;
};
struct gaussians_t {
    std::vector<gaussian_t> gaussians;
    gaussian_vec_t *soa_gaussians;
};
struct tiles_t {
    const std::vector<gaussians_t> tiles;
    const f32 tw, th;
    const u64 w, h;
};
        \end{minted}
        \caption{Types used for the implementation of the rendering functions.}
        \label{lst:types_vrt}
    \end{listing}

    Since I rely on passing functions as template arguments to configure the rendering functions I have defined some
    additional function types in Listing~\ref{lst:func_types_vrt} to make them more readable and meaningful.

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
typedef f32(*f32_func_t)(f32);
typedef simd::Vec<simd::Float>(*simd_f32_func_t)
            (simd::Vec<simd::Float>);
typedef vec4f_t(*radiance_func_t)(const vec4f_t,
            const vec4f_t, const gaussians_t&);
typedef f32(*transmittance_func_t)(const vec4f_t,
            const vec4f_t, const f32, const gaussians_t&);
        \end{minted}
        \caption{Function types used for template parameters.}
        \label{lst:func_types_vrt}
    \end{listing}

    \subsubsection{Tiling}
    The tiling function has
    \begin{minted}{c++}
tiles_t tile_gaussians(const f32 tw, const f32 th,
        const std::vector<gaussian_t> &gaussians,
        const glm::mat4 &view)
    \end{minted}
    as its signature. Its implementation is fairly straight forward as the math from Section~\ref{sec:tiling} can be
    translated into an algorithm directly. Note that the function returns a \mintinline{c++}{tiles_t} object. The
    \mintinline{c++}{std::vector} of \mintinline{c++}{gaussian_t} objects is decomposed into
    multiple \mintinline{c++}{std::vector}s, one for each field of the \mintinline{c++}{gaussian_t} structure, during
    the projection \eqref{eq:tiling_projection} and scaling \eqref{eq:tiling_scale} phase of the algorithm. This is done
    to prevent having to write the projected versions of discarded Gaussians back into memory.

    After the Gaussians are in normalized screen space ($[-1, 1] \times [-1, 1]$), I iterate over the centers of all
    tiles and over all remaining Gaussians per center. In the innermost loop the viability conditions for the $x$-distance
    \eqref{eq:tiling_cond_1} and $y$-distance \eqref{eq:tiling_cond_2} are checked. If a Gaussian is viable it is
    added to the \mintinline{c++}{gaussians} field of the current tile.

    After all possibly viable Gaussians have been check for the current tile a \mintinline{c++}{gaussian_vec_t} object
    is generated for the \mintinline{c++}{soa_gaussians} field of the current tile.

    \subsubsection{\gls{transmittance}}

    \begin{minted}{c++}
template<f32_func_t Exp, f32_func_t Erf>
f32 transmittance(const vec4f_t o, const vec4f_t n,
        const f32 s, const gaussians_t& gaussians)
    \end{minted}
    
    \begin{minted}{c++}
template<simd_f32_func_t Exp, simd_f32_func_t Erf,
        f32_func_t Expf>
f32 simd_transmittance(const vec4f_t o, const vec4f_t n,
        const f32 s, const gaussians_t& gaussians)
    \end{minted}
    
    \begin{minted}{c++}
template<simd_f32_func_t Exp, simd_f32_func_t Erf>
simd::Vec<simd::Float> broadcast_transmittance(
        const simd_vec4f_t o, const simd_vec4f_t n,
        const simd::Vec<simd::Float> s,
        const gaussians_t& gaussians)
    \end{minted}

    \begin{itemize}
        \item implementation of Eq.~\refeq{eq:transmittance_analytical}
        \item erf and exp implementations passed as template parameters
        \item parallelism â†’  memory overhead through load instruction, horizontal add Eq.~\refeq{eq:transmittance_parallel}
        \item broadcast Eq.~\refeq{eq:transmittance_broadcast}
    \end{itemize}
    
    \subsubsection{\gls{radiance}}

    \begin{minted}{c++}
template<transmittance_func_t Tr>
vec4f_t radiance(const vec4f_t o, const vec4f_t n,
        const gaussians_t& gaussians)
    \end{minted}
    
    \begin{minted}{c++}
template<transmittance_func_t Tr>
vec4f_t simd_radiance(const vec4f_t o, const vec4f_t n,
        const gaussians_t& gaussians)
    \end{minted}
    
    \begin{minted}{c++}
template<simd_f32_func_t Exp, simd_f32_func_t Erf>
simd_vec4f_t broadcast_radiance(const simd_vec4f_t o,
        const simd_vec4f_t n, const gaussians_t& gaussians)
    \end{minted}

    \begin{itemize}
        \item implementation of Eq.~\refeq{eq:radiance}
        \item transmittance functions as template argument
        \item parallelism â†’  memory overhead via load Eq.~\refeq{eq:radiance_parallel_final}
        \item broadcast Eq.~\refeq{eq:radiance_parallel_pixels}
    \end{itemize}
    
    \subsubsection{Full Render}
    \begin{minted}{c++}
template<radiance_func_t Radiance>
bool render_image(const u32 width, const u32 height,
        u32 *image, const camera_t &cam,
        const vec4f_t &origin,
        const gaussians_t &gaussians,
        const bool &running)
    \end{minted}

    \begin{minted}{c++}
template<radiance_func_t Radiance>
bool render_image(const u32 width, const u32 height,
        u32 *image, const camera_t &cam,
        const vec4f_t &origin, const tiles_t &tiles,
        const bool &running, const u64 tc)
    \end{minted}

    \begin{minted}{c++}
template<simd_f32_func_t Exp, simd_f32_func_t Erf>
bool simd_render_image(const u32 width, const u32 height,
        u32 *image, const camera_t &cam,
        const vec4f_t &origin,
        const gaussians_t &gaussians,
        const bool &running)
    \end{minted}

    \begin{minted}{c++}
template<simd_f32_func_t Exp, simd_f32_func_t Erf>
bool simd_render_image(const u32 width, const u32 height,
        u32 *image, const camera_t &cam,
        const vec4f_t &origin, const tiles_t &tiles,
        const bool &running, const u64 tc)
    \end{minted}

    \begin{itemize}
        \item parallel radiance or transmittance â†’  radiance function as template parameter
        \item parallel â†’  exp and erf approximations as template parameters
        \item 2 versions each with and without tiling
        \item thread pool used in tiling versions
    \end{itemize}

    \section{Renderer}
    \label{sec:renderer}

    \begin{itemize}
        \item renders and displays images via custom vulkan renderer (relevant?)
        \item load \mintinline{sh}{.obj} files â†’  vertices as gaussians
        \item command line flags (table?)
        \item multiple rendering modes listed in Table~\ref{tab:exec_modes}
        \item write images to file / headless mode
    \end{itemize}

    \begin{table}[b]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|}
            \hline
            Mode & Features\\
            1    & Sequential Baseline\\
            2    & Parallel \gls{transmittance}\\
            3    & Parallel \gls{radiance}\\
            4    & Parallel Pixels\\
            5    & Sequential + Tiling\\
            6    & Parallel \gls{transmittance} + Tiling\\
            7    & Parallel \gls{radiance} + Tiling\\
            8    & Parallel Pixels + Tiling\\
            st   & Single Threaded\\
            mt   & Multi Threaded\\
            \hline
        \end{tabular}
        \caption{Modes}
        \label{tab:exec_modes}
    \end{table}
    
    \chapter{Experiments}
    \label{ch:experiments}
    
    All experiments were performed on a Ryzen 9 7950X CPU, which has 16 cores with hyperthreading and AVX512 support.
    A list of all vector instruction set extensions the CPU provides can be found in Listing~\ref{lst:extensions} and
    were determined using the shell script given in Listing~\ref{lst:exts_script}.
    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{sh}
#!/usr/bin/env sh
gcc -march=native -dM -E - < /dev/null \
    | grep -E "SSE|AVX" | sort
        \end{minted}
        \caption{Shell script to determine supported vector extensions.}
        \label{lst:exts_script}
    \end{listing}

    \section{Approximations}
    \begin{itemize}
        \item averaged cycle count (pmu-tools)
        \item accuracy over relevant intervals
    \end{itemize}

    %@TODO: Do I need this section? Do I have relevant experiments w.r.t the transmittance? \section{Transmittance}

    \section{Full Render}
    Aside from testing the approximations on their own, I have also performed end-to-end tests
    to evaluate the performance improvements of the implemented optimizations in a - close to - real world scenario.
    The tests will be performed on two separate models and evaluate metrics concerning the usage and efficiency of
    \gls{simd} instructions (\ref{sec:utah_teapot}) and the runtime of the program (\ref{sec:dense_cube})

    \subsection{Utah/Newell Teapot}
    \label{sec:utah_teapot}
    %@TODO: Rerun HPCTOOLKIT tests using Utah Newell Teapot instead of Dense Cube
    I use the \href{https://graphics.stanford.edu/courses/cs148-10-summer/as3/code/as3/teapot.obj}{Newell Teapot Model}
    \footnote{https://graphics.stanford.edu/courses/cs148-10-summer/as3/code/as3/teapot.obj} to measure various metrics
    related to the efficiency of the \gls{simd} implementation of the rendering functions (\ref{sec:rendering_functions}).
    Additionally, I will measure L1 instruction and data cache loads and load misses.

    \begin{figure}[t]
        \centering
        \includegraphics[scale=.2]{images/teapot.png}
        \caption{Newell Teapot rendered using renderer from Section~\ref{sec:renderer}}
        \label{fig:teapot_render}
    \end{figure}
    
    For this purpose I will use a tool called HPCToolkit developed by \citeauthor{hpc_toolkit} and presented in
    \citetitle{hpc_toolkit} \cite{hpc_toolkit}.

    The tool is able to measure many different metrics such as the Linux kernels perf events or the events supported by
    PAPI (Performance Application Programming Interface) developed by \citeauthor{papi} at the university of Tennessee,
    Knoxville, and first published in \citetitle{papi} \cite{papi}.

    \begin{itemize}
        \item how does it measure â†’  links instrumentation at runtime (\mintinline{sh}{hpcrun})
        \item how to evaluate the output (metrics and views (top-down, bottom-up, flat))
        \item derived metrics
    \end{itemize}

    The derived metrics used were vector instruction waste and vector instruction efficiency defined as:
    \begin{align}
        \text{waste}(\text{cycles}, \text{instructions}) = 2 \cdot \text{cycles} - \text{instructions} \label{eq:vec_waste}\\
        \text{efficiency}(\text{cycles}, \text{instructions}) = 100 \cdot \frac{\text{instructions}}{2\cdot\text{cycles}}
        \label{eq:vec_efficiency}
    \end{align}
    which should be based on the exclusive\footnote{An exclusive metric refers to the quantity of the metric measured
    for that scope alone, disregarding nested scopes such as function calls. Reference: HPCTOOLKIT User Manual page 18
    \href{https://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf}{https://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf}}
    \mintinline{c}{PAPI_TOT_CYC} (total cycles) and \mintinline{c}{PAPI_VEC_INS}
    (vector instructions) metrics for the given scopes.

    %@TODO: Verify this:
    % HPCToolkit does not include inlined functions in exclusive metrics, which is why \gls{simd}
    % instructions and CPU cycles generated by the T-SIMD library would not count towards the exclusive \mintinline{c}{PAPI_VEC_INS} and \mintinline{c}{PAPI_TOT_CYC}
    % metrics.

%     \begin{listing}[H]
%         \begin{minted}[linenos, frame=lines]{c++}
% f32 func(f32 in)
% {
%     // inlined broadcast should count for scope
%     simd::Vec<simd::Float> x = simd::set1(in);
%     // exp should not count for scope
%     x = simd::exp(x);
%     // inlined mul and broadcast should count for scope
%     x = x * simd::set1(2.f);
%     return x;
% }
%         \end{minted}
%         \caption{Example for functions calls that should and should not count towards exclusive metrics.}
%         \label{lst:scopes_example}
%     \end{listing}
% 
%     As such the inclusive metrics are used for calculating the derived metrics. To get the exclusive derived metrics including the inlined T-SIMD functions I subtract
%     the inclusive derived metrics of the functions I want to disregard. For the example in Listing~\ref{lst:scopes_example}
%     this would be:
%     \[ \text{metric}_e = \text{metric}_i - \text{metric}_{\text{simd::exp}} \]
%     where $\text{metric}_i$ is the inclusive metric and $\text{matric}_{\text{simd::exp}}$ is the metric for the \mintinline{c++}{simd::exp} call.

    %Therefore the derived metrics are calculated using the inclusive metrics. To obtain the final value for the derived metrics the
    %value of the derived metrics of the scopes of non T-SIMD functions are subtracted from the value calculated using the inclusive metrics.

    \subsection{Dense Cube}
    \label{sec:dense_cube}
    Runtime averaged over 10 frames
    \begin{figure}[t]
        \centering
        \includegraphics[scale=.2]{images/cube.png}
        \caption{Dense cube rendered using renderer from Section~\ref{sec:renderer}}
        \label{fig:cube_render}
    \end{figure}
    
    \chapter{Results and Discussion}

    The results of the experiments discussed in Chapter~\ref{ch:experiments} show

    \section{Runtime}

    \subsection{Approximations}
    \begin{itemize}
        \item exp: \mintinline{c++}{approx::simd_fast_exp} is slightly faster than \mintinline{c++}{approx::vcl_exp},
            but a lot less accurate â†’  \mintinline{c++}{approx::vcl_exp} as default
        \item erf: \mintinline{c++}{approx::simd_abramowitz_stegun_erf} is significantly faster and accurate than the
            runner-up \mintinline{c++}{approx::simd_spline_erf_mirror} â†’  \mintinline{c++}{approx::simd_abramowitz_stegun_erf}
            as default.
        \item svml: slowest error function â†’  beat by both splines which are ill-suited to simd parallelization
    \end{itemize}

    \subsection{Full Render}
    On average the runtime is reduced by $19\%$ when compiling with clang instead of GCC.
    Rendering finishes about $9.4$ times faster when using tiling.

    Figure~\ref{fig:runtimes_st_simd_speedup} shows
    the speedups gained from employing the \gls{simd} routines described in Chapter~\ref{ch:optimizations}
    and Chapter~\ref{ch:implementation}. Showing a speedup of up to $25$ times when
    parallelization over the images pixels.
    \begin{figure}[t]
        \centering
        \input{plots/simd_speedup}
        \caption{Single Threaded Speedups from using SIMD}
        \label{fig:runtimes_st_simd_speedup}
    \end{figure}

    While the SVML routines provide much higher accuracy using them is not worth the
    performance trade-off as seen in Figure~\ref{fig:runtimes_st_svml_v_no}.
    \begin{figure}[t]
        \centering
        \input{plots/runtimes_svml}
        \caption{Single Threaded Runtimes using SVML and no SVML with Clang}
        \label{fig:runtimes_st_svml_v_no}
    \end{figure}

    \begin{figure}[t]
        \centering
        \input{plots/speedup}
        \caption{Speedup of Modes 6-8 relative to Mode 5}
        \label{fig:runtimes_st_v_mt}
    \end{figure}

    \section{Waste and Efficiency}

    \chapter{Conclusion and Future Work}
    %@TODO: Short recap + what can still be done: anisotropic Gaussians, loading models learned via Gaussian splatting, more efficient tiling method using SIMD, performance left on the table TBD (see results)
    \section{Conclusion}
    \begin{itemize}
        \item SIMD + Tiling lead to massive performance improvements
    \end{itemize}

    \section{Future Work}
    \begin{itemize}
        \item Memory/Cache Bottleneck?
        \item Anisotropic Gaussians
        \item Loading Scenes learned via Gaussian splatting
        \item More efficient tiling (SIMD, Threads)
        \item Combining Splatting and Ray Tracing
        \item GPU Implementation
    \end{itemize}

    \appendix
    \chapter{Tables}
    \label{ch:tables}
    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c | c | c | c|}
            \hline
            Mode & GCC               & Clang             & SVML\\\hline
            1 st & $\sim1673.50$ sec & $\sim1367.64$ sec & - \\
            2 st & $\sim96.78$ sec   & $\sim80.93$ sec   & $\sim152.43$ sec\\
            3 st & $\sim69.46$ sec   & $\sim54.19$ sec   & $\sim135.71$ sec\\
            4 st & $\sim67.39$ sec   & $\sim54.47$ sec   & $\sim137.09$ sec\\
            5 st & $\sim210.41$ sec  & $\sim171.01$ sec  & - \\
            6 st & $\sim14.84$ sec   & $\sim11.73$ sec   & $\sim23.29$ sec\\
            7 st & $\sim9.32$ sec    & $\sim7.38$ sec    & $\sim18.59$ sec\\
            8 st & $\sim8.79$ sec    & $\sim7.01$ sec    & $\sim18.99$ sec\\\hline\hline

            5 mt & $\sim8.30$ sec    & $\sim7.24$ sec    & - \\
            6 mt & $\sim0.77$ sec    & $\sim0.66$ sec    & $\sim1.19$ sec\\
            7 mt & $\sim0.60$ sec    & $\sim0.49$ sec    & $\sim1.00$ sec\\
            8 mt & $\sim0.56$ sec    & $\sim0.48$ sec    & $\sim0.97$ sec\\
            \hline
        \end{tabular}
        \caption{Runtime of rendering the dense cube based on compiler, SVML usage, and mode using AVX512. Averaged over 10 Frames.}
        \label{tab:perf_dense_cube_avx512}
    \end{table}

    \chapter{Figures}
    %@TODO: Remove figures for cycles -> boxplot instead
    \begin{figure}[H]
        \centering
        \input{plots/znver4_simd_erf_timing}
        \caption{Cycles to calculate $n$ values of $\erf$.}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_simd_exp_timing}
        \caption{Cycles to calculate $n$ values of $\exp$.}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_erf_approx}
        \caption{$\erf$ Approximations}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_erf_err}
        \caption{$\erf$ Errors}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_exp_approx}
        \caption{$\exp$ Approximations}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_exp_err}
        \caption{$\exp$ Errors}
    \end{figure}

    \chapter{Listings}
    \label{ch:listings}
    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
#define __AVX__ 1
#define __AVX2__ 1
#define __AVX512BF16__ 1
#define __AVX512BITALG__ 1
#define __AVX512BW__ 1
#define __AVX512CD__ 1
#define __AVX512DQ__ 1
#define __AVX512F__ 1
#define __AVX512IFMA__ 1
#define __AVX512VBMI__ 1
#define __AVX512VBMI2__ 1
#define __AVX512VL__ 1
#define __AVX512VNNI__ 1
#define __AVX512VPOPCNTDQ__ 1
#define __MMX_WITH_SSE__ 1
#define __SSE__ 1
#define __SSE2__ 1
#define __SSE2_MATH__ 1
#define __SSE3__ 1
#define __SSE4_1__ 1
#define __SSE4_2__ 1
#define __SSE4A__ 1
#define __SSE_MATH__ 1
#define __SSSE3__ 1
        \end{minted}
        \caption{Ryzen 9 7950X (\mintinline{sh}{-march=znver4}) Vector Extensions determined by \mintinline{sh}{gcc}.}
        \label{lst:extensions}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
typedef uint8_t  u8;
typedef uint16_t u16;
typedef uint32_t u32;
typedef uint64_t u64;

typedef int8_t  i8;
typedef int16_t i16;
typedef int32_t i32;
typedef int64_t i64;

typedef float f32;
typedef double f64;
        \end{minted}
        \caption{Primitive Type Aliases}
        \label{lst:type_aliases}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{julia}
function cubic_spline_interpolation(x, y)
    h = [x[i] - x[i-1] for i in eachindex(x)[2:end]]
    b = [ 3 * ((y[i+1] - y[i])/h[i] - (y[i] - y[i-1])/h[i-1])
        for i in eachindex(y)[2:end-1]]
    a_0 = y[2:end]
    A = zeros(length(h) - 1, length(h) - 1)
    for i in eachindex(h)[1:end-1]
        if i-1 > 0
            A[i, i-1] = h[i]
        end
        A[i, i] = 2*(h[i] + h[i+1])
        if i+1 <= length(h)-1
            A[i, i+1] = h[i+1]
        end
    end
    Ai = inv(A)
    a_2 = zeros(length(h) + 1)
    a_2[2:end-1] = Ai * b
    a_3 = [(a_2[i + 1] - a_2[i])/(3 * h[i])
        for i in eachindex(h)]
    a_1 = [(y[i+1] - y[i])/h[i] + a_2[i+1]*h[i]
        - a_3[i]*h[i]^2 for i in eachindex(h)]
    p = s -> a_0 + a_1 .* (s .- x[2:end]) + a_2[2:end]
        .* (s .- x[2:end]).^2 + a_3 .* (s .- x[2:end]).^3
    function get_index(t)
        t = t < x[1] ? x[1] : t
        t = t > x[end] ? x[end] : t
        return argmax([x[i] <= t <= x[i+1]
            for i in eachindex(x)[1:end-1]])
    end
    return (t -> p(t)[get_index(t)]), [a_0, a_1, a_2, a_3]
end
        \end{minted}
        \caption{Julia function to generate cubic spline interpolation}
        \label{lst:jl_cubic_spline}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{julia}
function generate_c(f=erf, name="erf", x=-6.0:.5:6.0,
            lower=-1.0, upper=1.0)
    _, a = cubic_spline_interpolation(x, f.(x))
    factors = map(x -> string.(Float32.(x)) .* "f",
            Vector{Float32}.(a))
    xs = "(x - " .* string.(x[2:end]) .* "f)"
    polynomials = "((" .* factors[4] .* " * " .* xs
            .* " + " .* factors[3][2:end] .* ") * " .* xs
    polynomials = polynomials .* " + " .* factors[2]
            .* ") * " .* xs .* " + " .* factors[1]
    returns = "return " .* polynomials .* ";"
    fun = "/// generated using spline \
interpolation with supports $(x)
f32 spline_"*name*"(const f32 x)
{
    if (x <= $(Float32(x[2]))f) return $(lower)f;\n"
    for i in eachindex(returns)[1:end-1]
        fun = fun .* "    else if (x < $(x[i+2])f) { "
            .* returns[i] .* " }\n"
    end
    fun = fun .* "    return $(upper)f;
}"
    println(fun)
end
        \end{minted}
        \caption{Julia functions to generate sequential code for cubic spline interpolation.}
        \label{lst:jl_code_gen}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{julia}
function generate_simd_c(f=erf, name="erf", x=-6.0:.5:6.0,
            lower=-1.0, upper=1.0)
    _, a = cubic_spline_interpolation(x, f.(x))
    factors = map(x -> "simd::set1(" .* string.(x) .* "f)",
            Vector{Float32}.(a))
    xs = "(x - " .* "simd::set1(" .* string.(x[2:end])
            .* "f))"
    polynomials = "((" .* factors[4] .* " * " .* xs .* " + "
            .* factors[3][2:end] .* ") * " .* xs
    polynomials = polynomials .* " + " .* factors[2]
            .* ") * " .* xs .* " + " .* factors[1]
    fun = "/// generated using spline interpolation \
with supports $(x)
simd::Vec<simd::Float> simd_spline_"*name*
"(const simd::Vec<simd::Float> x)
{
    simd::Vec<simd::Float> value = simd::set1($(upper));
    simd::ifelse(x <= simd::set1($(Float32(x[2]))f), \
simd::set1($(lower)), value);\n"
    for i in eachindex(polynomials)[1:end-1]
        fun = fun .* "    value = simd::ifelse(simd::\
bit_and(simd::set1($(x[i+1])f) <= x, x <= simd::set1\
($(x[i+2])f)), " .* polynomials[i] .* ", value);\n"
    end
    fun = fun .* "    return value;
}"
    println(fun)
end
        \end{minted}
        \caption{Julia function to generate \gls{simd} code for cubic spline interpolation.}
        \label{lst:jl_code_gen_simd}
    \end{listing}

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
template<size_t SIMD_WIDTH>
inline simd::Vec<simd::Float, SIMD_WIDTH>
vcl_exp(simd::Vec<simd::Float, SIMD_WIDTH> x);

template<>
inline simd::Vec<simd::Float, 32>
vcl_exp(simd::Vec<simd::Float, 32> x)
{
    return static_cast<__m256>(
            vcl::exp(vcl::Vec8f(static_cast<__m256>(x))));
}

#ifdef __AVX512F__
template<>
inline simd::Vec<simd::Float, 64>
vcl_exp(simd::Vec<simd::Float, 64> x)
{
    return static_cast<__m512>(
            vcl::exp(vcl::Vec16f(static_cast<__m512>(x))));
}
#endif
        \end{minted}
        \caption{Template wrapper for VCL exponential function.}
        \label{lst:vcl_exp}
    \end{listing}

 %@TODO: fill out interfaces
    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
struct vec4f_t {
    f32 x, y, z, w;
};
struct simd_vec4f_t { simd::Vec<simd::Float> x, y, z, w; };
struct gaussian_t {
    vec4f_t albedo, mu;
    f32 sigma, magnitude;
};
struct simd_gaussian_t {
    simd_vec4f_t albedo, mu;
    simd::Vec<simd::Float> sigma, magnitude;
};
struct gaussian_vec_t {
    // all pointers aligned to SIMD_WIDTH
    struct { f32 *r, *g, *b, *a; } albedo;
    struct { f32 *x, *y, *z, *w; } mu;
    f32 *sigma;
    f32 *magnitude;
    u64 size = 0;
};
struct gaussians_t {
    std::vector<gaussian_t> gaussians;
    gaussian_vec_t *soa_gaussians;
};
struct tiles_t {
    const std::vector<gaussians_t> tiles;
    const f32 tw, th;
    const u64 w, h;
};
        \end{minted}
        \caption{Types used for the implementation of the rendering functions.}
        \label{lst:types_vrt_full}
    \end{listing}

    \backmatter
    \printglossaries
    \printbibliography[heading=bibintoc]
\end{document}
