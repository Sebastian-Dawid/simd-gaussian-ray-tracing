\documentclass[a4paper, 11pt]{memoir}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\input{smart-thesis/style}
\input{smart-thesis/common-packages}
\input{smart-thesis/common-macros}

\usepackage{lipsum}
\usepackage[table]{xcolor}
\usepackage{minted}
\usepackage{float}

% pgfplots preamble
\usetikzlibrary{arrows.meta}
\usetikzlibrary{backgrounds}
\usepgfplotslibrary{patchplots}
\usepgfplotslibrary{fillbetween}
\pgfplotsset{%
    layers/standard/.define layer set={%
        background,axis background,axis grid,axis ticks,axis lines,axis tick labels,pre main,main,axis descriptions,axis foreground%
    }{
        grid style={/pgfplots/on layer=axis grid},%
        tick style={/pgfplots/on layer=axis ticks},%
        axis line style={/pgfplots/on layer=axis lines},%
        label style={/pgfplots/on layer=axis descriptions},%
        legend style={/pgfplots/on layer=axis descriptions},%
        title style={/pgfplots/on layer=axis descriptions},%
        colorbar style={/pgfplots/on layer=axis descriptions},%
        ticklabel style={/pgfplots/on layer=axis tick labels},%
        axis background@ style={/pgfplots/on layer=axis background},%
        3d box foreground style={/pgfplots/on layer=axis foreground},%
    },
}

\addbibresource{main.bib}

\makeglossaries
\input{glossary}

\thesistype{Bachelor Thesis}
\discipline{Computer Science}
\title{Parallel Gaussian Ray Tracing on the CPU}
\author{Sebastian Dawid}
\institution{Bielefeld University,Technical Faculty,Visual AI for Extended Reality Group}
\supervisors{Prof.\@~Dr.\@~Helge Rhodin,Prof.\@~Dr.-Ing.\@~Ralf M\"oller}

\newcommand*{\erf}{\text{erf}}

\makepagestyle{abs}

% Display the page number in the footer.
\makeevenfoot{abs}{\thepage}{}{}
\makeoddfoot{abs}{}{}{\thepage}

\begin{document}
    \frontmatter
    \smarttitle
    \newpage
    \tableofcontents*

    \clearpage
    \thispagestyle{abs}
    \abstractintoc
    \begin{abstract}
        \lipsum[1]
    \end{abstract}

    \mainmatter
    \chapter{Introduction}

    \chapter{Background}
    \section{Gaussian Ray Tracing}
    \label{sec:int_grt}
    In their 2015 Paper \citetitle{Rhodin:2015} \cite{Rhodin:2015} Rhodin \etal describe a volumetric image formation
    model based on a parametric density representation $D(\mathbf{x})$ given as the sum of scaled isotropic Guassians
    $\mathcal{G} = \{ G_q \}_q$. The density $D$ is then given as:
    \begin{align}
        D(\mathbf{x}) = \sum_{G_q \in \mathcal{G}} G_q(\mathbf{x})
        \label{eq:density}\\
        G_q(\mathbf{x}) = c_q \cdot \exp{\left( - \frac{\Vert\mathbf{x} - \mu_q\Vert_2^2}{2\sigma_q^2} \right)}
        \label{eq:gaussian}
    \end{align}
    where $c_q$ describes the magnitude, $\mu_q$ the center and $\sigma_q$ the standard deviation of the Gaussian $G_q$.
    Additionally an \gls{albedo} attribute $\mathbf{a}_q$ is defined for each Gaussian to denote its color. This leads
    to the scene representation $\gamma = \{ c_q, \mu_q, \sigma_q, \mathbf{a}_q \}$. The $\gamma$ is omitted from
    $G_q(\mathbf{x})$ for readability and since the parameters are given implicitly via $q$.

    To get the final color of a pixel they determine the amount of light that reaches the camera from each point
    along a ray. For this purpose they determine the \gls{transmittance} $T$ of a point at distance $s$ along a ray from
    a camera position $\mathbf{o}$ in direction $\mathbf{n}$ as:
    \begin{equation}
        T(\mathbf{o}, \mathbf{n}, s, \gamma) = \exp{\left( - \int_0^s D(\mathbf{o} + t\mathbf{n}) dt \right)}
        \label{eq:transmittance}
    \end{equation}

    For the Gaussian density representation the density along a ray $\mathbf{x} = \mathbf{o} + s\mathbf{n}$ through a
    sum of 3D Gaussians is a sum of 1D Gaussians, where the 1D Gaussians are given by inserting the ray into the
    3D Gaussians $G_q$ \eqref{eq:gaussian}. This results in the form
    $\bar{c} \exp{\left( - \frac{(x - \bar{\mu})^2}{2\bar{\sigma}^2} \right)}$ for the 1D scaled Gaussians, with
    $\bar{\mu} = (\mu - \mathbf{o})^T\mathbf{n}$, $\bar{\sigma} = \sigma$ and
    $\bar{c} = c \cdot \exp{\left( - \frac{(\mu - \mathbf{o})^T(\mu - \mathbf{o}) - \bar{\mu}^2}{2\bar{\sigma}^2} \right)}$.

    The \gls{transmittance} can be expressed analytically using the error function
    $\erf{(x)} = \frac{2}{\sqrt{\pi}}\int_0^s \exp{(-t^2)} dt$ and gaussian form of the density, as follows:
    \begin{equation}
        \begin{aligned}
            T(\mathbf{o}, \mathbf{n}, s, \gamma) &= \exp{\left( -\int_0^s
                \sum_q G_q(\mathbf{o} + t\mathbf{n} ) dt \right)}\\
            &= \exp{\left( \sum_q \frac{\bar{\sigma}_q \bar{c}_q}{\sqrt{\frac{2}{\pi}}}
            \left( \erf{\left( \frac{-\bar{\mu}_q}{\sqrt{2}\bar{\sigma}_q} \right)}
            - \erf{\left( \frac{s - \bar{\mu}_q}{\sqrt{2}\bar{\sigma}_q} \right)} \right) \right)}\\
        \end{aligned}
        \label{eq:transmittance_analytical}
    \end{equation}

    Assuming the elements in the scene emit an equal amount of \gls{radiance}, a ray is shot through each pixel of a virtual
    \gls{pinhole_camera}. The \gls{radiance} can be computed as the product of \gls{transmittance} $T$ \eqref{eq:transmittance},
    density $D$ \eqref{eq:density}, \gls{albedo} $\mathbf{a}$ and the ambient \gls{radiance} $L_e$ integrated along a ray
    $\mathbf{x} = \mathbf{o} + s\mathbf{n}$. They assume the ambient \gls{radiance} is fixed as $L_e = 1$. As such they
    disregard it in the following equation:
    \begin{equation}
        L(\mathbf{o}, \mathbf{n}, \gamma) = \int_0^\infty T(\mathbf{o}, \mathbf{n}, s, \gamma)
            \sum_q G_q(\mathbf{o} + s\mathbf{n})\mathbf{a}_q ds
    \end{equation}
    
    This integral may be approximated with sufficient accuracy by sampling around the mean of each Gaussian $G_q$
    a compact interval $S_q = \{ \bar{\mu}_q + k\lambda_q | k \in K \subset \Z \}$. For their purposes it was sufficient
    to choose $\lambda_q \sim \bar{\sigma}_q$ as the step length:
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_q \mathbf{a}_q \sum_{s \in S_q}
            \lambda_q T(\mathbf{o}, \mathbf{n}, s, \gamma)G_q(\mathbf{o} + s\mathbf{n})
        \label{eq:radiance}
    \end{equation}

    Rhodin \etal describe that local sampling with $\lambda_q = \bar{\sigma}_q$ and
    $K = \{ -4, -3, \dots, 0 \}$ delivers a good enough approximation.

    \section{Gaussian Splatting}
    %@TODO: describe image formation model (splatting) as laid out in the following papers
    \cite{kerbl3Dgaussians}
    \cite{volume_splatting}

    \section{Ray Tracing vs. Splatting}
    %@TODO: What differentiates the two methods? Advantages? Disadvantages?

    \chapter{Optimizations}
    %@TODO: Write introductory paragraph on optimizations
    \begin{itemize}
        \item CPU ray tracing is slow
        \item Some machines have stronger CPUs than GPUs (e.g. integrated GPU only)
    \end{itemize}
    \section{Vectorization}
    \label{sec:vectorization}
    %@TODO: rename to vectorization -> SIMD in Chpter 4
    One method of optimizing a problem is vectorization, which describes the process of
    solving a problem on a vector of data instead of processing the pieces of data in the
    vector sequentially.

    \subsection{Notation}
    To properly rewrite the method as it is laid out in \cite{Rhodin:2015} it is
    necessary to introduce some notation to work with vectorization and ensure it is
    cleanly separated from conventional vector spaces. Vectors used for parallelization
    will be referred to as p-vectors (parallel vectors) from here on. This notation is
    defined in Table~\ref{tab:notation}:
    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|}
            \hline
            Notation & Definition \\
            \hline
            $x^W$ & p-vector containing $W$ elements\\
            $x^W_i$ & $i$-th element of p-vector $x$\\
            $\mathbf{x}^W$ & p-vector containing $W$ vectors $\mathbf{x} \in \R^n$\\
            $\langle \mathbf{x}^W, \mathbf{y}^W \rangle$ & elementwise inner product of the vectors in $\mathbf{x}$ and $\mathbf{y}$\\
            $[ x ]^W$ & $x$ broadcast to a p-vector of $W$ elements\\
            $\odot$ & elementwise multiplication\\
            $\frac{x^W}{y^W}$ & elementwise division\\
            $f^W$ & function that produces a p-vector containing $W$ elements\\\hline
        \end{tabular}
        \caption{Vectorization Notation}
        \label{tab:notation}
    \end{table}

    \subsection{Approaches}
    There are multiple ways to parallelize gaussian ray tracing using vectorization. To generate the final image it is necessary to iterate
    over two quantities: (1) the gaussians in the scene and (2) the pixels of the image. Therefore these are the opportunities to parallelize.
    Parallelism over the pixels is possible by expanding the rendering equations given in \cite{Rhodin:2015} to operate on multiple rays at the
    same time. Parallelism over the gaussians is possible in two ways:
    \begin{enumerate}
        \item The summands of the sum in the \gls{transmittance} equation~\eqref{eq:transmittance_analytical} do not depend on each
            other therefore it is possible to divide the summands into equal groups and process these in parallel
            and add these results up in the end to get the final result.
        \item The same principle can be applied to the outer sum in the \gls{radiance} equation~\eqref{eq:radiance}: the summands are
            not dependent on each other therefore they can be divided into equal groups that can
            be processed in parallel and added up in the end to get the final \gls{radiance}.
    \end{enumerate}
    The inner sum of the \gls{radiance} equation~\eqref{eq:radiance} is not interesting in terms of parallelism since the number
    of summands it has is fixed and therefore potential runtime improvements would not scale with the number of gaussians in the scene or number
    of pixels in the image.

    To enable parallelization over the gaussians it is necessary to split the set of
    gaussians $\mathcal{G}$ into $W$ disjoint subsets of equal magnitude. Since the magnitude of $\mathcal{G}$ is not guaranteed to be divisible by $W$
    a number of subsets equal to the remainder $|\mathcal{G}| \mod W$ will be augmented with an additional zero element.

    The most sensible division of $\mathcal{G}$ is to assign some order to its elements, such that $g_i, i=1,\dots,|\mathcal{G}|$ describes the $i$-th element of
    $\mathcal{G}$. With this $\mathcal{G}$ can be divided into subsets:
    \begin{equation}
        \mathcal{G}_j := \{ g_i \in \mathcal{G} \,|\, i \mod W \equiv j \}, j=0,\dots,W-1
    \end{equation}

    \paragraph{Parallel \gls{transmittance}:}
    \label{par:parallel_transmittance}
    Given this division I can define a \gls{transmittance} function that operates on the elements of the subsets $\mathcal{G}_j$ in parallel.
    \begin{equation}
        \begin{aligned}
            T^W(\mathbf{o}, \mathbf{n}, s, \gamma) &= \exp^W\left( \sum_{m = 0}^{\left\lceil \frac{|\mathcal{G}|}{W} \right\rceil - 1}
            \begin{pmatrix}
                \frac{\bar{\sigma}_{mW}\bar{c}_{mW}}{\sqrt{\frac{2}{\pi}}} \\ \vdots \\\frac{\bar{\sigma}_{(m+1)W-1}\bar{c}_{(m+1)W-1}}{\sqrt{\frac{2}{\pi}}} 
            \end{pmatrix} \right.\\&\left.\odot \begin{pmatrix}
                \erf{\left( \frac{- \bar{\mu}_{mW}}{\sqrt{2}\bar{\sigma}_{mW}} \right) - \erf{\left( \frac{s - \bar{\mu}_{mW}}{\sqrt{2}\bar{\sigma}_{mW}} \right)}} \\
                \vdots \\
               \erf{\left( \frac{- \bar{\mu}_{(m+1)W - 1}}{\sqrt{2}\bar{\sigma}_{(m+1)W - 1}} \right) - \erf{\left( \frac{s - \bar{\mu}_{(m+1)W - 1}}{\sqrt{2}\bar{\sigma}_{(m+1)W - 1}} \right)}} 
            \end{pmatrix}\right)
        \end{aligned}
        \label{eq:transmittance_parallel}
    \end{equation}

    Now I replace the call to the \gls{transmittance} function in the \gls{radiance} equation~\eqref{eq:radiance} with the sum
    over the results of the parallel \gls{transmittance} equation~\eqref{eq:transmittance_parallel}:
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_{q} \mathbf{a}_q \sum_{s \in S_q} \lambda_qG_q(\mathbf{o} + s\mathbf{n})\sum_{i = 1}^W T^W_i(\mathbf{o}, \mathbf{n}, s, \gamma)
    \end{equation}

    Since the subsets $\mathcal{G}_j$ are disjoint by definition this version of the \gls{radiance} equation yields the same results as the original (Eq.~\eqref{eq:radiance})

    \paragraph{Parallel \gls{radiance}:}
    \label{par:parallel_radiance}
    The analytical solution to the \gls{transmittance} integral from Eq.~\eqref{eq:transmittance_analytical} can be broadcast
    to operate on p-vectors of points on rays parameterized by origins $\mathbf{o}^W$, directions $\mathbf{n}^W$
    and distances $s^W$, as follows:
    \begin{equation}
        \begin{aligned}
            T^W(\mathbf{o}^W, \mathbf{n}^W, s^W, \gamma) = \exp^W\Bigg(& \sum_q \frac{(\bar{\sigma}_q)^W
            (\bar{c}_q)^W}{\left[ \sqrt{\frac{2}{\pi}} \right]^W} \\
            \odot \Bigg(& \erf^W{\left( \frac{-(\bar{\mu}_q)^W}{[ \sqrt{2} ]^W \odot (\bar{\sigma}_q)^W} \right)}\\
            &- \erf^W{\left( \frac{s^W - (\bar{\mu}_q)^W}{[ \sqrt{2} ]^W \odot (\bar{\sigma}_q)^W} \right)} \Bigg) \Bigg) 
        \end{aligned}
        \label{eq:transmittance_broadcast}
    \end{equation}
    with
    \begin{align*}
        \bar{\mu}^W &= \left\langle [ \mu ]^W - \mathbf{o}, \mathbf{n}^W \right\rangle, \bar{\sigma}^W = \left[ \sigma \right]^W\\
        \bar{c}^W &= [c]^W \odot \exp^W{\left( - \frac{\left\langle [\mu]^W - \mathbf{o}^W, [\mu]^W - \mathbf{o}^W \right\rangle
    - \left(\bar{\mu}^W\right)^2}{[2]^W \odot \left(\bar{\sigma}^W\right)^2} \right)}
    \end{align*}

    This version of the \gls{transmittance} equation \eqref{eq:transmittance_analytical} allows me to rewrite the \gls{radiance} equation to operate on
    multiple gaussians at the same time in the same way as in \ref{par:parallel_transmittance}:
    \begin{equation}
        \begin{aligned}
            \hat{L}^W(\mathbf{o}, \mathbf{n}, \gamma) &= \sum_{m = 0}^{\left\lceil \frac{|\mathcal{G}|}{W} \right\rceil - 1} \left( \begin{pmatrix}
                \mathbf{a}_{mW}\\ \vdots \\ \mathbf{a}_{(m+1)W - 1}
            \end{pmatrix} \right.\\
            &\odot \sum_{s^W \in S_m} \left( T^W([\mathbf{o}]^W, [\mathbf{n}]^W, s^w, \gamma)\right.\\
            &\odot \left.\left.\begin{pmatrix}
                G_{mW}(\mathbf{o} + s^W_1\mathbf{n})\\ \vdots\\ G_{(m+1)W - 1}(\mathbf{o} + s^W_W\mathbf{n})
            \end{pmatrix}\right)\right)
        \end{aligned}
        \label{eq:radiance_parallel_gaussians}
    \end{equation}
    with
    \[ S_m = \left\{\left. \begin{pmatrix}
        \bar{\mu}_{mW}\\ \vdots\\ \bar{\mu}_{(m+1)W - 1}
    \end{pmatrix} + [k]^W \odot \begin{pmatrix}
        \lambda_{mW}\\ \vdots\\ \lambda_{(m+1)W - 1}
    \end{pmatrix} \,\right|\, k \in K \subset \Z \right\} \]
    and $\bar{\mu}$, $\bar{\sigma}$ and $\bar{c}$ defined the same as in Section~\ref{sec:int_grt}.

    Finally we can collect these results into the final \gls{radiance}:
    \begin{equation}
        \hat{L}(\mathbf{o}, \mathbf{n}, \gamma) = \sum_{i = 1}^W \hat{L}^W_i(\mathbf{o}, \mathbf{n}, \gamma)
        \label{eq:radiance_parallel_final}
    \end{equation}

    \paragraph{Parallel Pixels:}
    \label{par:parallel_pixels}
    Given the broadcast version of the \gls{transmittance} equation \eqref{eq:transmittance_broadcast} from before I can broadcast the \gls{radiance} equation to
    calculate the \gls{radiance} for multiple pixels at once:
    \begin{equation}
        \begin{aligned}
            \hat{L}^W(\mathbf{o}^W, \mathbf{n}^W, \gamma) &= \sum_q [ \mathbf{a}_q ]^W \sum_{s \in S_q} \Big(
            [ \lambda_q ]^W \odot T^W(\mathbf{o}^W, \mathbf{n}^W, [ s ]^W, \gamma)\\
            &\odot G_q^W(\mathbf{o}^W + [ s ]^W \odot \mathbf{n}^W) \Big)
        \end{aligned}
        \label{eq:radiance_parallel_pixels}
    \end{equation}

    Note that both the \gls{transmittance} and \gls{radiance} equations still operate on the Gaussians one by one as the
    sums iterate over every Gaussian $G_q \in \mathcal{G}$ individually.

    \section{Tiling}
    %@TODO: describe the tiling method used. note similarities to splatting.
    In most scenes most pixels will be unaffected by most gaussians, therefore it is not sensible to perform
    the per pixel calculations for all gaussians instead of just the gaussians that could affect the pixel.
    Reducing the number of gaussians considered when dealing with any individual pixel is a good way to reduce
    time wasted performing unnecessary computations.

    \begin{figure}[H]
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tikzpicture}
                \draw[gray, thin] (0, 0) grid[xstep=4, ystep=2.25] (16, 9);

                \fill[purple] (12, 5.5) circle (0.1);
                \draw[purple, thick] (12, 5.5) circle (3);

                \fill[red] (2.7, 1.8) circle (0.1);
                \draw[red, thick] (2.7, 1.8) circle (1.7);
                
                \fill[green] (4, 3) circle (0.1);
                \draw[green, thick] (4, 3) circle (1.5);
                
                \fill[blue] (5, 2) circle (0.1);
                \draw[blue, thick] (5, 2) circle (1);
            \end{tikzpicture}
        }
        \caption{Image of four gaussians split into 16 tiles.}
        \label{fig:tiling}
    \end{figure}

    One way of achieving this goal is to split the final image into $N \times N$ tiles (\eg Figure~\ref{fig:tiling}) and determining which gaussians
    are relevant to which tiles through spacial proximity. Since it is not clear where in the image any one gaussian is
    going to end up before generating the image it is necessary to estimate location and size of the gaussians before
    generating the final image through ray tracing.

    To estimate size and position of the gaussians in the final image the guassians are projected to the image plane using
    scaled orthographic projection. Let $V \in \R^{4 \times 4}$ be the view matrix of the camera. Then the projected guassian
    $\bar{g} = (\bar{\mu}, \bar{\sigma})$ to a gaussian $g = (\mu, \sigma)$ is calculated as:

    \begin{align}
        \mu_p &= V \cdot (\mu, 1)^T\\
        \bar{\mu} &= \frac{1}{\mu_{p3}} \cdot (\mu_{p1}, \mu_{p2})^T\\
        \bar{\sigma} &= \frac{\sigma}{\mu_{p3}}
    \end{align}

    Note that this method of projection is a simplified version of the projection used in \cite{kerbl3Dgaussians}, since I only consider isotropic gaussians.
    Gaussians with $\bar{\sigma} < 10^{-5}$ are discared since they are unlikely to contribute to the final image significantly. Also note that after the projection
    the image coordinates are mapped to $[-1, 1] \times [-1, 1]$.

    Let $tw, th \in \R$ be the width and height of a tile and let $\mathbf{c} \in \R^2$ denote the center of the tile. The vector $\mathbf{p} = (|\mathbf{c}_1 - \bar{mu}_1|, |\mathbf{c}_2 - \bar{\mu}_2|)^T$
    describes the absolute distances along the $x$ and $y$ Axes between the center of the tile $\mathbf{c}$ and mean of the projected gaussian $\bar{\mu}$.
    A gaussian counts as relevant to a tile if
    \begin{equation}
        \mathbf{p}_1 < |\mathbf{c}_1| + \frac{tw}{2} + 2\bar{\sigma}
    \end{equation}
    and 
    \begin{equation}
        \mathbf{p}_2 < |\mathbf{c}_2| + \frac{th}{2} + 2\bar{\sigma}
    \end{equation}
    meaning that the gaussian has to be in a $2\bar{\sigma}$ range from the borders of the tile to be considered relevant.
    The $|\mathbf{c}_i|, i=1,2$ terms are added to account for stronger perspective distortion towards the edges of the image.

    \chapter{Approximations}
    %@TODO: Explain why it is necessary to approximate functions / implement own approximations
    \section{Cubic Spline Interpolation}
    %@TODO: Describe cubic spline interpolation

    \section{Fast Exponential Function}
    \cite{fast_exp}
    \begin{equation}
        \exp{(x)} = 2^{\frac{x}{\ln{(2)}}}
    \end{equation}
    
    \section{Polynomial Approximations of the Error Function}
    \subsection{Taylor Series}
    \subsection{Abramowitz and Stegun}
    \citeauthor{AbraSteg72} give one such approximation in Section 7.1.27 of their \enquote{\citetitle{AbraSteg72}}\cite{AbraSteg72} as:
    \begin{equation}
        \erf{(x)} = 1 - \frac{1}{(1 + ax + bx^2 + cx^3 + dx^4)^4}
    \end{equation}
    with
    \begin{align*}
        a &= 0.278393,\,
        b = 0.230289\\
        c &= 0.000972,\,
        d = 0.078108
    \end{align*}
    
    \chapter{Implementation}
    %@TODO: details TBD
    The source code of the implementation is available at: \href{https://github.com/Sebastian-Dawid/simd-gaussian-ray-tracing}{https://github.com/Sebastian-Dawid/simd-gaussian-ray-tracing}

    The Implementation is split into two parts: (1) a library that implements the rendering functions and approximations and (2) a renderer that uses the library to generate images.

    \section{SIMD}
    \subsection{Math to SIMD}
    \begin{itemize}
        \item most conversion are 1 to 1
        \item p-vectors of vectors are implemented as vectors of p-vectors
    \end{itemize}
    \subsection{T-SIMD}
    A simple way to enable cross compilation for different \gls{simd} extentions is the T-SIMD library described in \enquote{\citetitle{own_moeller_16_2}} \cite{own_moeller_16_2} by \citeauthor{own_moeller_16_2}.

    \section{Library}
    \subsection{Approximations}
    \subsubsection{SVML}
    Intels SVML Library provides implementations for both the exponential and error functions. Since the library is closed source I have no way of knowing how these approximations were implemented.
    \subsubsection{VCL}
    The approximation is based on the IEEE-754 single precision floating point format and the taylor series of the exponential function.
    \subsection{Rendering Functions}
    \section{Renderer}
    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|}
            \hline
            Mode & Features\\
            1    & Sequential Baseline\\
            2    & Parallel \gls{transmittance}\\
            3    & Parallel \gls{radiance}\\
            4    & Parallel Pixels\\
            5    & Sequential + Tiling\\
            6    & Parallel \gls{transmittance} + Tiling\\
            7    & Parallel \gls{radiance} + Tiling\\
            8    & Parallel Pixels + Tiling\\
            st   & Single Threaded\\
            mt   & Multi Threaded\\
            \hline
        \end{tabular}
        \caption{Modes}
        \label{tab:exec_modes}
    \end{table}
    
    \chapter{Experiments}
    \section{Approximations}
    \section{Transmittance}
    \section{Full Render}
    Aside from testing singular functions or sections of the program, I have also performed a number of end-to-end tests
    to evaluate the performance improvements of the implemented optimizations in a, close to, real world scenario.
    The tests will be performed on two separate models and evaluate different aspects of the implementation,
    which I will describe in the following sections:
    \subsection{Utah/Newell Teapot}
    \href{https://graphics.stanford.edu/courses/cs148-10-summer/as3/code/as3/teapot.obj}{Model}\footnote{https://graphics.stanford.edu/courses/cs148-10-summer/as3/code/as3/teapot.obj}
    \subsection{Dense Cube}
    
    \chapter{Results and Discussion}
    \section{Runtime}
 
    %@TODO: Redo runtime plots to include mode 1 (rerun tests)
    \begin{figure}[H]
        \centering
        \input{plots/runtimes}
        \caption{Single Threaded Runtimes using GCC and Clang}
        \label{fig:runtimes_st_gcc_v_clang}
    \end{figure}

    \begin{figure}[H]
        \centering
        \input{plots/runtimes_svml}
        \caption{Single Threaded Runtimes using SVML and no SVML with Clang}
        \label{fig:runtimes_st_svml_v_no}
    \end{figure}

    \begin{figure}[H]
        \centering
        \input{plots/speedup}
        \caption{Speedup of Modes 6-8 relative to Mode 5}
        \label{fig:runtimes_st_v_mt}
    \end{figure}

    \section{HPCTOOLKIT}
    \cite{hpc_toolkit}

    The derived metrics used were vector instruction waste and vector instruction efficiency defined as:
    \begin{align}
        \text{waste}(\text{cycles}, \text{instructions}) = 2 \cdot \text{cycles} - \text{instructions} \label{eq:vec_waste}\\
        \text{efficiency}(\text{cycles}, \text{instructions}) = 100 \cdot \frac{\text{instructions}}{2\cdot\text{cycles}} \label{eq:vec_efficiency}
    \end{align}
    which should be based on the exclusive\footnote{An exclusive metric refers to the quantity of the metric measured for that scope
    alone, disregarding nested scopes such as function calls. Reference: HPCTOOLKIT User Manual page 18
    \href{https://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf}{https://hpctoolkit.org/manual/HPCToolkit-users-manual.pdf}}
    \mintinline{c}{PAPI_TOT_CYC} (total cycles) and \mintinline{c}{PAPI_VEC_INS}
    (vector instructions) metrics for the given scopes. HPCToolkit does not include inlined functions in exclusive metrics, which is why \gls{simd}
    instructions and CPU cycles generated by the T-SIMD library would not count towards the exclusive \mintinline{c}{PAPI_VEC_INS} and \mintinline{c}{PAPI_TOT_CYC}
    metrics.

    \begin{listing}[H]
        \begin{minted}[linenos, frame=lines]{c++}
f32 func(f32 in)
{
    // inlined broadcast should count for scope
    simd::Vec<simd::Float> x = simd::set1(in);
    // exp should not count for scope
    x = simd::exp(x);
    // inlined mul and broadcast should count for scope
    x = x * simd::set1(2.f);
    return x;
}
        \end{minted}
        \caption{Example for functions calls that should and should not count towards exclusive metrics.}
        \label{lst:scopes_example}
    \end{listing}

    As such the inclusive metrics are used for calculating the derived metrics. To get the exclusive derived metrics including the inlined T-SIMD functions I subtract
    the inclusive derived metrics of the functions I want to disregard. For the example in Listing~\ref{lst:scopes_example}
    this would be:
    \[ \text{metric}_e = \text{metric}_i - \text{metric}_{\text{simd::exp}} \]

    %Therefore the derived metrics are calculated using the inclusive metrics. To obtain the final value for the derived metrics the
    %value of the derived metrics of the scopes of non T-SIMD functions are subtracted from the value calculated using the inclusive metrics.

    \chapter{Conclusion and Future Work}
    %@TODO: Short recap + what can still be done: anisotropic gaussians, loading models learned via gaussian splatting, more efficient tiling method using SIMD, performance left on the table TBD (see results)
    \section{Conclusion}
    \begin{itemize}
        \item SIMD + Tiling lead to massive performance improvements
    \end{itemize}

    \section{Future Work}
    \begin{itemize}
        \item Memory/Cache Bottleneck?
        \item Anisotropic Gaussians
        \item Loading Scenes learned via gaussians splatting
        \item More efficient tiling (SIMD, Threads)
        \item Combining Splatting and Ray Tracing
        \item GPU Implementation
    \end{itemize}

    \appendix
    \chapter{Tables}
    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c | c | c|}
            \hline
            Mode & GCC              & Clang\\\hline
            1 st & $\sim1656.4$ sec & $\sim1459.4$ sec\\
            2 st & $\sim102.09$ sec & $\sim87.09$ sec\\
            3 st & $\sim67.63$ sec  & $\sim68.19$ sec\\
            4 st & $\sim71.46$ sec  & $\sim55.33$ sec\\
            5 st & $\sim168.76$ sec & $\sim143.95$ sec\\
            6 st & $\sim1.72$ sec   & $\sim1.21$ sec\\
            7 st & $\sim0.84$ sec   & $\sim0.72$ sec\\
            8 st & $\sim0.77$ sec   & $\sim0.59$ sec\\\hline\hline
            5 mt & $\sim0.582$ sec  & $\sim0.502$ sec\\
            6 mt & $\sim0.085$ sec  & $\sim0.068$ sec\\
            7 mt & $\sim0.057$ sec  & $\sim0.051$ sec\\
            8 mt & $\sim0.052$ sec  & $\sim0.046$ sec\\
            \hline
        \end{tabular}
        \caption{Runtime of rendering the dense cube based on compiler and mode using AVX512.}
        \label{tab:perf_dense_cube_avx512}
    \end{table}

    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            Mode & \gls{radiance}             & \gls{transmittance}          & $\erf$                      & $\exp$\\\hline
            1 st & & & &\\
            2 st & & & &\\
            3 st & & & &\\
            4 st & & & &\\
            5 st & & & &\\
            6 st & & & &\\
            7 st & $3.8 \cdot 10^7$ ($0.9\%$) & $1.15 \cdot 10^9$ ($28.4\%$) & $1.14 \cdot 10^8$ ($2.8\%$) & $4.0 \cdot 10^6$ ($0.1\%$)\\
            8 st & & & &\\\hline\hline
            5 mt & & & &\\
            6 mt & & & &\\
            7 mt & & $3.96 \cdot 10^8$ ($4.4\%$) & &\\
            8 mt & & & &\\
            \hline
        \end{tabular}
        \caption{Clang: Vector Instruction Waste (\% of total) by Mode}
        \label{tab:clang_vec_waste}
    \end{table}
    \begin{table}[H]
        \centering
        \rowcolors{1}{white}{lightgray}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            Mode & \gls{radiance} & \gls{transmittance} & $\erf$   & $\exp$\\\hline
            1 st & & & &\\
            2 st & & & &\\
            3 st & & & &\\
            4 st & & & &\\
            5 st & & & &\\
            6 st & & & &\\
            7 st & $5.0\%$        & $53.78\%$           & $25.0\%$ & - \\
            8 st & & & &\\\hline\hline
            5 mt & & & &\\
            6 mt & & & &\\
            7 mt & & $31.25\%$ & &\\
            8 mt & & & &\\
            \hline
        \end{tabular}
        \caption{Clang: Vector Instruction Efficiency by Mode}
        \label{tab:clang_vec_efficiency}
    \end{table}

    \chapter{Figures}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_simd_erf_timing}
        \caption{Cycles to calculate $n$ values of $\erf$.}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_simd_exp_timing}
        \caption{Cycles to calculate $n$ values of $\exp$.}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_erf_approx}
        \caption{$\erf$ Approximations}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_erf_err}
        \caption{$\erf$ Errors}
    \end{figure}
    
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_exp_approx}
        \caption{$\exp$ Approximations}
    \end{figure}
    \begin{figure}[H]
        \centering
        \input{plots/znver4_cmp_exp_err}
        \caption{$\exp$ Errors}
    \end{figure}

    \backmatter
    \printglossaries
    \printbibliography[heading=bibintoc]
\end{document}
